<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Reinforcement Learning</title>
<meta name="author" content="Bibek Panthi" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="../css/stylesheet.css" />
<link rel="stylesheet" type="text/css" href="../css/braindump.css" />
<script src="../js/counters.js" type="text/javascript"></script>
<script src="../js/URI.js" type="text/javascript"></script>
<script src="../js/pages.js" type="text/javascript"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="H" href="./index.html"> HOME </a>
</div><div id="preamble" class="status">
<p class="date">Date: <span class="timestamp-wrapper"><span class="timestamp">[2022-11-13 Sun]</span></span></p>
</div>
<div id="content" class="content">
<h1 class="title">Reinforcement Learning</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#CS%20285%3A%20Deep%20Reinforcement%20Learning">1. CS 285: Deep Reinforcement Learning</a></li>
<li><a href="#CS%20234%3A%20Reinforcement%20Learning%20%20-%20Emma%20Brunskill">2. CS 234: Reinforcement Learning  - Emma Brunskill</a></li>
<li><a href="#Books">3. Books</a></li>
<li><a href="#Sutton%20%26%20Barto">4. Sutton &amp; Barto</a></li>
<li><a href="#Papers">5. Papers</a>
<ul>
<li><a href="#Imperfect%20Information%20Game%2C%20Collaboration%20and%20Communication">5.1. Imperfect Information Game, Collaboration and Communication</a></li>
<li><a href="#Learning%20Decision%20Trees%20With%20RL">5.2. Learning Decision Trees With RL</a></li>
<li><a href="#Intention%20Conditioned%20Value%20Function">5.3. Intention Conditioned Value Function</a></li>
<li><a href="#%5B%5Bid%3A99C9FD6E-E6C3-4B61-A808-7246CEB5F189%5D%5BSolving%20Offline%20Reinforcement%20Learning%20with%20Decision%20Tree%20Regression%5D%5D">5.4. Solving Offline Reinforcement Learning with Decision Tree Regression</a></li>
<li><a href="#%5B%5Bid%3AC46B63F1-4324-4AC5-A956-CA49D593AFA3%5D%5BGoal-Conditioned%20Supervised%20Learning%5D%5D">5.5. Goal-Conditioned Supervised Learning</a></li>
<li><a href="#%5B%5Bid%3AA626137F-5AE8-4341-86CD-82F8191D7947%5D%5BReward%20Conditioned%20Policies%5D%5D">5.6. Reward Conditioned Policies</a></li>
<li><a href="#%5B%5Bid%3A0228C204-726C-4C08-BD67-E449FF659DAB%5D%5BThe%20Surprising%20Effectiveness%20of%20PPO%20in%20Cooperative%20Multi-Agent%20Games%5D%5D">5.7. The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games</a></li>
</ul>
</li>
<li><a href="#Websites">6. Websites</a></li>
<li><a href="#Spinning%20Up%20by%20OpenAI">7. Spinning Up by OpenAI</a>
<ul>
<li><a href="#Mathematical%20Background">7.1. Mathematical Background</a></li>
<li><a href="#General%20Knowledge%20of%20Deep%20Learning">7.2. General Knowledge of Deep Learning</a>
<ul>
<li><a href="#Standard%20Architectures">7.2.1. Standard Architectures</a></li>
<li><a href="#Regularizers">7.2.2. Regularizers</a></li>
<li><a href="#Normalization">7.2.3. Normalization</a></li>
<li><a href="#Optimizers">7.2.4. Optimizers</a></li>
<li><a href="#Reparameterization%20trick">7.2.5. Reparameterization trick</a></li>
</ul>
</li>
<li><a href="#Suggested%20Main%20Concepts">7.3. Suggested Main Concepts</a></li>
<li><a href="#Write%20Your%20own%20Code%20%28in%20the%20beginning%29">7.4. Write Your own Code (in the beginning)</a></li>
<li><a href="#Taxonomy%20of%20RL%20Algorithms">7.5. Taxonomy of RL Algorithms</a></li>
</ul>
</li>
<li><a href="#Debugging%20RL%20Algorithms">8. Debugging RL Algorithms</a></li>
<li><a href="#Podcast">9. Podcast</a></li>
<li><a href="#Concepts">10. Concepts</a>
<ul>
<li><a href="#Averagers">10.1. Averagers</a></li>
<li><a href="#True%20Gradient%20Descent">10.2. True Gradient Descent</a></li>
<li><a href="#DQN">10.3. DQN</a>
<ul>
<li><a href="#Replay%20Buffer">10.3.1. Replay Buffer</a></li>
<li><a href="#Stable%20Target">10.3.2. Stable Target</a></li>
<li><a href="#Prioritized%20Replay%20Buffer">10.3.3. Prioritized Replay Buffer</a></li>
<li><a href="#Double%20DQN">10.3.4. Double DQN</a></li>
<li><a href="#Duelling%20DQN">10.3.5. Duelling DQN</a></li>
<li><a href="#Maximization%20Bias">10.3.6. Maximization Bias</a></li>
</ul>
</li>
<li><a href="#Actions%20are%20Infinite">10.4. Actions are Infinite</a></li>
</ul>
</li>
<li><a href="#People">11. People</a>
<ul>
<li><a href="#Dibya%20Gosh">11.1. Dibya Gosh</a></li>
</ul>
</li>
<li><a href="#MARL">12. MARL</a></li>
</ul>
</div>
</div>
<div id="outline-container-CS%20285%3A%20Deep%20Reinforcement%20Learning" class="outline-2">
<h2 id="CS%20285%3A%20Deep%20Reinforcement%20Learning"><span class="section-number-2">1.</span> CS 285: Deep Reinforcement Learning</h2>
<div class="outline-text-2" id="text-1">
<p>
<a href="https://rail.eecs.berkeley.edu/deeprlcourse/">https://rail.eecs.berkeley.edu/deeprlcourse/</a>
</p>
</div>
</div>
<div id="outline-container-CS%20234%3A%20Reinforcement%20Learning%20%20-%20Emma%20Brunskill" class="outline-2">
<h2 id="CS%20234%3A%20Reinforcement%20Learning%20%20-%20Emma%20Brunskill"><span class="section-number-2">2.</span> CS 234: Reinforcement Learning  - Emma Brunskill</h2>
<div class="outline-text-2" id="text-2">
<p>
<a href="https://www.youtube.com/playlist?list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u">https://www.youtube.com/playlist?list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u</a>
</p>
</div>
</div>
<div id="outline-container-Books" class="outline-2">
<h2 id="Books"><span class="section-number-2">3.</span> Books</h2>
<div class="outline-text-2" id="text-3">
<p>
From <a href="https://qr.ae/prnRfl">https://qr.ae/prnRfl</a>
</p>
<blockquote>
<p>
In my opinion, the main RL problems are related to:
</p>

<p>
Information representation: from POMDP to predictive state representation to TD-networks to deep-learning.
Inverse RL: how to learn the reward
Algorithms
</p>
<ul class="org-ul">
<li>Off-policy</li>
<li>Large scale: linear and nonlinear approximations of the value function</li>
<li>Policy search vs. Q-learning based</li>
</ul>
<p>
Beyond MDP, contextual MDPs
Optimal exploration
Hierarchical RL
Multitask RL and life long learning
Multiagent RL
Efficient model based RL
</p>

<p>
I can suggest good papers for each of these problems, but there are few books. This is a list of most of the books I am aware of:
</p>

<ul class="org-ul">
<li>Sutton &amp; Barto Book: Reinforcement Learning: An Introduction. The classical intuitive intro to the field. I love it.</li>

<li>The chapter by Bertsekas: Page on mit.edu</li>

<li>Algorithms for Reinforcement Learning: Csaba Szepesvari. Nice compendium of ready to be implemented algorithms.</li>

<li>Approximators. Busoniu, Lucian; Robert Babuska ; Bart De Schutter ; Damien Ernst (2010). This is a very practical book that explains some state-of-the-art algorithms (i.e., useful for real world problems) like fitted-Q-iteration and its variations.</li>

<li>Reinforcement Learning: State-of-the-Art. Vol. 12 of Adaptation, Learning and Optimization. Wiering, M., van Otterlo, M. (Eds.), 2012. Springer, Berlin. In Sutton's words "This book is a valuable resource for students wanting to</li>
</ul>
<p>
go beyond the older textbooks and for researchers wanting to easily catch up with
recent developments".
</p>

<ul class="org-ul">
<li>Optimal Adaptive Control and Differential Games by Reinforcement Learning Principles : Draguna Vrabie, Kyriakos G. Vamvoudakis , Frank L. Lewis. I am not familiar with this one, but I have seen it recommended.</li>

<li>Markov Decision Processes in Artificial Intelligence, Sigaud O. &amp; Buffet O. editors, ISTE Ld., Wiley and Sons Inc, 2010.</li>
</ul>

<p>
I definitely suggest the books by Sutton and Barto as an excellent intro, the chapter by Bertsekas for getting a solid theoretical background and the book by Busoniu et al. for practical algorithms that can solve some non-toy problems. I also find useful the book by Szepesvari as a quick reference for understanding an comparing algorithms.
</p>

<p>
There are also several good specialized monographs and surveys on the topic, some of these are:
</p>

<ul class="org-ul">
<li>"From Bandits to Monte-Carlo Tree Search: The Optimistic Principle Applied to Optimization and Planning" by Remi Munos (New trends on Machine Learning). This monograph covers important nonconvex optimistic optimization methods that can be applied for policy search. Available at <a href="https://hal.archives-ouvertes.fr/hal-00747575v5/document">https://hal.archives-ouvertes.fr/hal-00747575v5/document</a></li>

<li>"Reinforcement Learning in Robotics: A Survey" by J. Kober, J. A. Bagnell and J. Peters. Available at Page on ias.tu-darmstadt.de</li>

<li>"A Tutorial on Linear Function Approximators for Dynamic Programming and Reinforcement Learning" by A. Geramifard, T. J. Walsh, S. Tllex, G. Chowdhary, N. Roy and J. P. How (Foundations and Trends in Machine Learning). Available at Page on mit.edu</li>

<li>"A Survey on Policy Search for Robotic" by Newmann and Peters (Foundations and Trends in Machine Learning). Available at Page on ausy.tu-darmstadt.de</li>
</ul>
</blockquote>

<ul class="org-ul">
<li><a href="reinforcement_learning_problem_is_the_ai_problem.html#ID-63DC47AA-7705-432D-A185-EF5CEE5E6149">Reinforcement Learning Problem is the AI Problem</a></li>
<li id="<a href="https://stable-baselines.readthedocs.io/en/master/">Stable Baseline</a>">Collection of RL Algorithms</li>
<li><a href="https://medium.datadriveninvestor.com/which-reinforcement-learning-rl-algorithm-to-use-where-when-and-in-what-scenario-e3e7617fb0b1">A collection of psuedocode of RL Algorithms with notes on which to use when</a></li>
<li><a href="https://jonathan-hui.medium.com/rl-reinforcement-learning-algorithms-comparison-76df90f180cf">An excellent collection of RL algorithms and their comparision</a></li>
</ul>
</div>
</div>
<div id="outline-container-Sutton%20%26%20Barto" class="outline-2">
<h2 id="Sutton%20%26%20Barto"><span class="section-number-2">4.</span> Sutton &amp; Barto</h2>
<div class="outline-text-2" id="text-4">
<ul class="org-ul">
<li><p>
321 : \(\widehat{\nabla{J(\theta)}}\) is a stochastic estimate whose expectation approximates the gradient of the performance measure with respect to its argument.
</p>
<blockquote>
<p>
“Stochastic Estimate” triggered these thoughts:
</p>
<ul class="org-ul">
<li>We can not know the gradient of the performance, because it may be difficult to measure through out all states, or due to other reasons</li>
<li>In these cases, we can instead use “Stochastic Estimate” of the desired thing. This estimate has to approximate the true thing in its Expectation. ie. the Expectation (on average value) must be the desired thing.</li>
<li>Thus when we can’ t compute the thing we require, it may suffice to compute the thing which in ‘expectation’ is the desired thing.</li>
</ul>
</blockquote></li>

<li><p>
329  <b>Monte Carlo Method have high variance.</b>
</p>
<blockquote>
<p>
As a Monte Carlo Method REINFORCE may be of high variance and thus produce slow learning.
</p>
</blockquote></li>
</ul>
</div>
</div>
<div id="outline-container-Papers" class="outline-2">
<h2 id="Papers"><span class="section-number-2">5.</span> Papers</h2>
<div class="outline-text-2" id="text-5">
</div>
<div id="outline-container-Imperfect%20Information%20Game%2C%20Collaboration%20and%20Communication" class="outline-3">
<h3 id="Imperfect%20Information%20Game%2C%20Collaboration%20and%20Communication"><span class="section-number-3">5.1.</span> Imperfect Information Game, Collaboration and Communication</h3>
<div class="outline-text-3" id="text-5-1">
<ul class="org-ul">
<li>Learning Multi-agent Implicit Communication Through Actions: A Case Study in Contract Bridge, a Collaborative <a href="imperfect_information_game.html#ID-21C0EDEC-B6E7-48FB-8D25-24EE3BBBE8D6">Imperfect-Information Game</a>. [<a href="papers/rl/Learning MultiAgent Implicit Communication Through Actions - Bridge - Imperfect Information Collaborative Game - 1810.04444v1.pdf">file</a>]</li>
</ul>
</div>
</div>
<div id="outline-container-Learning%20Decision%20Trees%20With%20RL" class="outline-3">
<h3 id="Learning%20Decision%20Trees%20With%20RL"><span class="section-number-3">5.2.</span> Learning Decision Trees With RL</h3>
<div class="outline-text-3" id="text-5-2">
<p>
<a href="papers/rl/Learning Decision Trees with RL - Xiong.pdf">papers/rl/Learning Decision Trees with RL - Xiong.pdf</a>
Trains a RNN network using RL, to decide which feature to split the decision tree next. Performs better than greedy strategy.
</p>

<p>
Because greedy strategies look on immediate infromation gain, where RL can be trained for longterm payoff.
</p>
</div>
</div>
<div id="outline-container-Intention%20Conditioned%20Value%20Function" class="outline-3">
<h3 id="Intention%20Conditioned%20Value%20Function"><span class="section-number-3">5.3.</span> Intention Conditioned Value Function</h3>
<div class="outline-text-3" id="text-5-3">
<ul class="org-ul">
<li><a href="papers/RL from Passive Data via Latent Intentions - 2304.04782; Intention Conditioned Value Function (ICVF).pdf">papers/RL from Passive Data via Latent Intentions - 2304.04782; Intention Conditioned Value Function (ICVF).pdf</a></li>
<li><a href="papers/The Successor Representation - Peter Dayan.pdf">papers/The Successor Representation - Peter Dayan.pdf</a></li>
<li><a href="papers/Learning One Representation to Optimize All Rewards - 2103.07945.pdf">papers/Learning One Representation to Optimize All Rewards - 2103.07945.pdf</a></li>
<li><a href="icvf.html#ID-32650E63-BC19-4764-B0EB-4EBF2F5CAAF7">ICVF</a></li>
</ul>
</div>
</div>
<div id="outline-container-%5B%5Bid%3A99C9FD6E-E6C3-4B61-A808-7246CEB5F189%5D%5BSolving%20Offline%20Reinforcement%20Learning%20with%20Decision%20Tree%20Regression%5D%5D" class="outline-3">
<h3 id="%5B%5Bid%3A99C9FD6E-E6C3-4B61-A808-7246CEB5F189%5D%5BSolving%20Offline%20Reinforcement%20Learning%20with%20Decision%20Tree%20Regression%5D%5D"><span class="section-number-3">5.4.</span> <a href="supervised_learning_for_rl.html#ID-99C9FD6E-E6C3-4B61-A808-7246CEB5F189">Solving Offline Reinforcement Learning with Decision Tree Regression</a></h3>
</div>
<div id="outline-container-%5B%5Bid%3AC46B63F1-4324-4AC5-A956-CA49D593AFA3%5D%5BGoal-Conditioned%20Supervised%20Learning%5D%5D" class="outline-3">
<h3 id="%5B%5Bid%3AC46B63F1-4324-4AC5-A956-CA49D593AFA3%5D%5BGoal-Conditioned%20Supervised%20Learning%5D%5D"><span class="section-number-3">5.5.</span> <a href="goal_conditioned_supervised_learning.html#ID-C46B63F1-4324-4AC5-A956-CA49D593AFA3">Goal-Conditioned Supervised Learning</a></h3>
</div>
<div id="outline-container-%5B%5Bid%3AA626137F-5AE8-4341-86CD-82F8191D7947%5D%5BReward%20Conditioned%20Policies%5D%5D" class="outline-3">
<h3 id="%5B%5Bid%3AA626137F-5AE8-4341-86CD-82F8191D7947%5D%5BReward%20Conditioned%20Policies%5D%5D"><span class="section-number-3">5.6.</span> <a href="reward_conditioned_policies.html#ID-A626137F-5AE8-4341-86CD-82F8191D7947">Reward Conditioned Policies</a></h3>
</div>
<div id="outline-container-%5B%5Bid%3A0228C204-726C-4C08-BD67-E449FF659DAB%5D%5BThe%20Surprising%20Effectiveness%20of%20PPO%20in%20Cooperative%20Multi-Agent%20Games%5D%5D" class="outline-3">
<h3 id="%5B%5Bid%3A0228C204-726C-4C08-BD67-E449FF659DAB%5D%5BThe%20Surprising%20Effectiveness%20of%20PPO%20in%20Cooperative%20Multi-Agent%20Games%5D%5D"><span class="section-number-3">5.7.</span> <a href="the_surprising_effectiveness_of_ppo_in_cooperative_multi_agent_games.html#ID-0228C204-726C-4C08-BD67-E449FF659DAB">The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games</a></h3>
</div>
</div>
<div id="outline-container-Websites" class="outline-2">
<h2 id="Websites"><span class="section-number-2">6.</span> Websites</h2>
<div class="outline-text-2" id="text-6">
<ul class="org-ul">
<li>A (Long) Peek into Reinforcement Learning : <a href="https://lilianweng.github.io/posts/2018-02-19-rl-overview/">https://lilianweng.github.io/posts/2018-02-19-rl-overview/</a></li>
</ul>
</div>
</div>
<div id="outline-container-Spinning%20Up%20by%20OpenAI" class="outline-2">
<h2 id="Spinning%20Up%20by%20OpenAI"><span class="section-number-2">7.</span> Spinning Up by OpenAI</h2>
<div class="outline-text-2" id="text-7">
<p>
<a href="https://spinningup.openai.com/en/latest/user/introduction.html">https://spinningup.openai.com/en/latest/user/introduction.html</a>
</p>
</div>
<div id="outline-container-Mathematical%20Background" class="outline-3">
<h3 id="Mathematical%20Background"><span class="section-number-3">7.1.</span> Mathematical Background</h3>
<div class="outline-text-3" id="text-7-1">
<ul class="org-ul">
<li>Probability and Statistics
<ul class="org-ul">
<li>Random Variables</li>
<li>Bayes' Theorem</li>
<li>Chain Rule of Probability</li>
<li>Expected Values</li>
<li>Standard Deviations</li>
<li>Importance Sampling</li>
</ul></li>
<li>Multivariate Calculus
<ul class="org-ul">
<li>Gradients</li>
<li>Taylor Series Expansion</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-General%20Knowledge%20of%20Deep%20Learning" class="outline-3">
<h3 id="General%20Knowledge%20of%20Deep%20Learning"><span class="section-number-3">7.2.</span> General Knowledge of Deep Learning</h3>
<div class="outline-text-3" id="text-7-2">
<p>
[ Links to resources on all these are in the page <a href="https://spinningup.openai.com/en/latest/spinningup/spinningup.html">Spinning Up as a Deep RL Researcher</a> : (<a href="papers/RL/Spinning Up as a Deep RL Researcher — Spinning Up documentation.html">saved html</a>) ]
</p>
</div>
<div id="outline-container-Standard%20Architectures" class="outline-4">
<h4 id="Standard%20Architectures"><span class="section-number-4">7.2.1.</span> Standard Architectures</h4>
<div class="outline-text-4" id="text-7-2-1">
<ul class="org-ul">
<li>MLP</li>
<li>Vanilla RNN</li>
<li>LSTM</li>
<li>GRU</li>
<li>conv layers</li>
<li>resnets</li>
<li>attention mechanisms</li>
</ul>
</div>
</div>
<div id="outline-container-Regularizers" class="outline-4">
<h4 id="Regularizers"><span class="section-number-4">7.2.2.</span> Regularizers</h4>
<div class="outline-text-4" id="text-7-2-2">
<ul class="org-ul">
<li>Weight decay</li>
<li>Dropout</li>
</ul>
</div>
</div>
<div id="outline-container-Normalization" class="outline-4">
<h4 id="Normalization"><span class="section-number-4">7.2.3.</span> Normalization</h4>
<div class="outline-text-4" id="text-7-2-3">
<ul class="org-ul">
<li>batch norm</li>
<li>layer norm</li>
<li>weight norm</li>
</ul>
</div>
</div>
<div id="outline-container-Optimizers" class="outline-4">
<h4 id="Optimizers"><span class="section-number-4">7.2.4.</span> Optimizers</h4>
<div class="outline-text-4" id="text-7-2-4">
<ul class="org-ul">
<li>SGD</li>
<li>Momentums SGD</li>
<li>Adam</li>
<li>others</li>
</ul>
</div>
</div>
<div id="outline-container-Reparameterization%20trick" class="outline-4">
<h4 id="Reparameterization%20trick"><span class="section-number-4">7.2.5.</span> Reparameterization trick</h4>
<div class="outline-text-4" id="text-7-2-5">
<p>
<a href="https://arxiv.org/abs/1312.6114">https://arxiv.org/abs/1312.6114</a> : <a href="papers/ML Base/Auto-Encoding Variational Bayes - 1312.6114.pdf">papers/ML Base/Auto-Encoding Variational Bayes - 1312.6114.pdf</a>
</p>
</div>
</div>
</div>
<div id="outline-container-Suggested%20Main%20Concepts" class="outline-3">
<h3 id="Suggested%20Main%20Concepts"><span class="section-number-3">7.3.</span> Suggested Main Concepts</h3>
<div class="outline-text-3" id="text-7-3">
<ul class="org-ul">
<li>Montonic Improvement Theory : <a href="papers/rl/Optimizing Expectations Graphs - John Schulman.pdf">papers/rl/Optimizing Expectations Graphs - John Schulman.pdf</a></li>
</ul>
</div>
</div>
<div id="outline-container-Write%20Your%20own%20Code%20%28in%20the%20beginning%29" class="outline-3">
<h3 id="Write%20Your%20own%20Code%20%28in%20the%20beginning%29"><span class="section-number-3">7.4.</span> Write Your own Code (in the beginning)</h3>
<div class="outline-text-3" id="text-7-4">
<ul class="org-ul">
<li></li>


<li>Which algorithms?
You should probably start with vanilla policy gradient (also called REINFORCE), DQN, A2C (the synchronous version of A3C), PPO (the variant with the clipped objective), and DDPG, approximately in that order. The simplest versions of all of these can be written in just a few hundred lines of code (ballpark 250-300), and some of them even less (for example, a no-frills version of VPG can be written in about 80 lines). Write single-threaded code before you try writing parallelized versions of these algorithms. (Do try to parallelize at least one.)</li>
</ul>
</div>
</div>
<div id="outline-container-Taxonomy%20of%20RL%20Algorithms" class="outline-3">
<h3 id="Taxonomy%20of%20RL%20Algorithms"><span class="section-number-3">7.5.</span> Taxonomy of RL Algorithms</h3>
<div class="outline-text-3" id="text-7-5">

<div id="figure-1" class="figure">
<p><img src="data/reinforcement_learning/a_taxonomy_of_rl_algorithms-20230727194644.png" alt="a_taxonomy_of_rl_algorithms-20230727194644.png" />
</p>
<p><span class="figure-number">Figure 1: </span>A Taxonomy of RL Algorithms</p>
</div>
</div>
</div>
</div>
<div id="outline-container-Debugging%20RL%20Algorithms" class="outline-2">
<h2 id="Debugging%20RL%20Algorithms"><span class="section-number-2">8.</span> Debugging RL Algorithms</h2>
<div class="outline-text-2" id="text-8">
<p>
<a href="https://andyljones.com/posts/rl-debugging.html">https://andyljones.com/posts/rl-debugging.html</a> talks about:
</p>
<ul class="org-ul">
<li>Probe Environments</li>
<li>Probe Agents</li>
<li>Logs execessively</li>
<li>Use really large batch size</li>
<li>Rescale your rewards</li>
</ul>
</div>
</div>
<div id="outline-container-Podcast" class="outline-2">
<h2 id="Podcast"><span class="section-number-2">9.</span> Podcast</h2>
<div class="outline-text-2" id="text-9">
<p>
TalkRL: <a href="https://podcasts.apple.com/us/podcast/talkrl-the-reinforcement-learning-podcast/id1478198107">https://podcasts.apple.com/us/podcast/talkrl-the-reinforcement-learning-podcast/id1478198107</a>
</p>
</div>
</div>
<div id="outline-container-Concepts" class="outline-2">
<h2 id="Concepts"><span class="section-number-2">10.</span> Concepts</h2>
<div class="outline-text-2" id="text-10">
</div>
<div id="outline-container-Averagers" class="outline-3">
<h3 id="Averagers"><span class="section-number-3">10.1.</span> Averagers</h3>
<div class="outline-text-3" id="text-10-1">
<p>
<a href="https://youtu.be/gOV8-bC1_KU?t=608">https://youtu.be/gOV8-bC1_KU?t=608</a>
</p>

<p>
<a href="file:///Users/bpanthi977/Desktop/RLAlgsInMDPs.pdf">file:///Users/bpanthi977/Desktop/RLAlgsInMDPs.pdf</a>
</p>
</div>
</div>
<div id="outline-container-True%20Gradient%20Descent" class="outline-3">
<h3 id="True%20Gradient%20Descent"><span class="section-number-3">10.2.</span> True Gradient Descent</h3>
<div class="outline-text-3" id="text-10-2">
<ul class="org-ul">
<li>Is DQN True Gradient Descent? No. They are approximations to it.   <a href="https://youtu.be/gOV8-bC1_KU?t=3253">CS234 - Lecture 6 (t=3253)</a></li>
<li>GTD (Gradient Temporal Difference) are true gradient descent</li>
<li>The first section of chapter on Function Approximation (Sutton &amp; Barto) has few points on this.</li>
</ul>
</div>
</div>
<div id="outline-container-DQN" class="outline-3">
<h3 id="DQN"><span class="section-number-3">10.3.</span> DQN</h3>
<div class="outline-text-3" id="text-10-3">
<ul class="org-ul">
<li>Huber Loss on Bellman Error <a href="https://youtu.be/gOV8-bC1_KU?t=4756">https://youtu.be/gOV8-bC1_KU?t=4756</a></li>
</ul>
</div>
<div id="outline-container-Replay%20Buffer" class="outline-4">
<h4 id="Replay%20Buffer"><span class="section-number-4">10.3.1.</span> Replay Buffer</h4>
</div>
<div id="outline-container-Stable%20Target" class="outline-4">
<h4 id="Stable%20Target"><span class="section-number-4">10.3.2.</span> Stable Target</h4>
</div>
<div id="outline-container-Prioritized%20Replay%20Buffer" class="outline-4">
<h4 id="Prioritized%20Replay%20Buffer"><span class="section-number-4">10.3.3.</span> Prioritized Replay Buffer</h4>
</div>
<div id="outline-container-Double%20DQN" class="outline-4">
<h4 id="Double%20DQN"><span class="section-number-4">10.3.4.</span> Double DQN</h4>
</div>
<div id="outline-container-Duelling%20DQN" class="outline-4">
<h4 id="Duelling%20DQN"><span class="section-number-4">10.3.5.</span> Duelling DQN</h4>
</div>
<div id="outline-container-Maximization%20Bias" class="outline-4">
<h4 id="Maximization%20Bias"><span class="section-number-4">10.3.6.</span> Maximization Bias</h4>
<div class="outline-text-4" id="text-10-3-6">
<ul class="org-ul">
<li>Double DQN</li>
<li>Stable Target</li>
</ul>
<p>
prevent maximization bias
</p>
</div>
</div>
</div>
<div id="outline-container-Actions%20are%20Infinite" class="outline-3">
<h3 id="Actions%20are%20Infinite"><span class="section-number-3">10.4.</span> Actions are Infinite</h3>
<div class="outline-text-3" id="text-10-4">
<p>
From <a href="assumptions_of_decision_making_models_in_agi.html#ID-46E03179-87C5-4566-9D04-C68B7712B600">Assumptions of Decision Making Models in AGI</a>:
</p>
<blockquote>
<p>
It is unreasonable to assume that at any state, all possible actions are listed.
</p>

<ul class="org-ul">
<li><p>
Actions in small scale may be discrete or a finite collection of distributions, but at the level where planning happens set of all possible actions is infinite.
</p>

<p>
Such actions can in principle to thought to be recursively composed of a set of basic operations/actions. But the decision making happens not at those basic actions but at level of composed actions.
</p></li>

<li>It is a task in itself to know what actions can be taken and what actions should we evaluate.</li>

<li><p>
Thus decision making involves composing short timestep actions to get longer term action over which planning can be done.
</p>

<p>
i.e. decision making is often not about <i>selection</i> but <i>selective composition</i>. [<a href="file:///Users/bpanthi977/Documents/synced/Notes/papers/Assumptions of Decision Making Models in AGI.pdf#page=2">Page 2</a>]
</p></li>
</ul>
</blockquote>

<p>
So, one thing to explore would be Hierarchical Reinforcement Learning.
</p>
</div>
</div>
</div>
<div id="outline-container-People" class="outline-2">
<h2 id="People"><span class="section-number-2">11.</span> People</h2>
<div class="outline-text-2" id="text-11">
</div>
<div id="outline-container-Dibya%20Gosh" class="outline-3">
<h3 id="Dibya%20Gosh"><span class="section-number-3">11.1.</span> Dibya Gosh</h3>
<div class="outline-text-3" id="text-11-1">
<p>
<a href="https://scholar.google.com/citations?user=znnl0kwAAAAJ">https://scholar.google.com/citations?user=znnl0kwAAAAJ</a>
and his blog article: <a href="https://dibyaghosh.com/blog/probability/highdimensionalgeometry.html">Trouble in High-Dimensional Land</a>
</p>
</div>
</div>
</div>
<div id="outline-container-MARL" class="outline-2">
<h2 id="MARL"><span class="section-number-2">12.</span> MARL</h2>
<div class="outline-text-2" id="text-12">
<ul class="org-ul">
<li><a href="multi_agent_reinforcement_learning.html#ID-48C31D08-B4C5-434C-A1AC-1C7A94306D1C">MARL</a></li>
<li><a href="https://github.com/thu-ml/tianshou">https://github.com/thu-ml/tianshou</a></li>
<li><a href="https://pettingzoo.farama.org/api/aec/">https://pettingzoo.farama.org/api/aec/</a></li>
</ul>

<hr />
<h3>Backlinks</h3>

<ul class="org-ul">
<li><a href="rl_can_be_used_to_train_non_differentiable_models.html#ID-125D38D9-A3F2-47E0-A4A8-B8084C7EF72D">RL can be used to train Non-Differentiable Models</a></li>
<li><a href="procgen_benchmark.html#ID-A62AC955-8AC8-4D6A-A899-5D9A8A042CEF">Procgen Benchmark</a></li>
<li><a href="genetic_algorithm.html#ID-73B816D0-ABEF-40AF-BB38-E680F26CA7EB">Genetic Algorithm</a></li>
<li><a href="sergey_levine.html#ID-65DED394-F17D-4C6A-A86D-FE48A5EED74A">Sergey Levine</a></li>
<li><a href="supervised_learning_for_rl.html#ID-FFD963C6-CED1-4D80-9DC0-ABCC1AB9695A">Supervised Learning for RL</a></li>
<li><a href="assumptions_of_decision_making_models_in_agi.html#ID-46E03179-87C5-4566-9D04-C68B7712B600">Assumptions of Decision Making Models in AGI</a></li>
</ul>
</div>
</div>
</div>
<div id="postamble" class="status">
<hr/>You can send your feedback, queries <a href="mailto:bpanthi977@gmail.com?subject=Feedback: Reinforcement Learning">here</a><span id="visits"></span><span id="claps"></span><div id="claps-message"></div>
</div>
</body>
</html>
