<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Reward Conditioned Policies</title>
<meta name="author" content="Bibek Panthi" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="../css/stylesheet.css" />
<link rel="stylesheet" type="text/css" href="../css/braindump.css" />
<script src="../js/counters.js" type="text/javascript"></script>
<script src="../js/URI.js" type="text/javascript"></script>
<script src="../js/pages.js" type="text/javascript"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="./index.html"> UP </a>
 |
 <a accesskey="H" href="../index.html"> HOME </a>
</div><div id="preamble" class="status">
<p class="date">Date: <span class="timestamp-wrapper"><span class="timestamp">&lt;2024-09-14 Sat&gt;</span></span></p>
</div>
<div id="content" class="content">
<h1 class="title">Reward Conditioned Policies</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#Any%20trajectory%20is%20optimal%20trajectory%20when%20conditioned%20on%20matching%20the%20reward">1. Any trajectory is optimal trajectory when conditioned on matching the reward</a></li>
<li><a href="#%5B%5Bpdf%3A~%2FDocuments%2Fsynced%2FNotes%2Fpapers%2FReward%20Conditioned%20Policies%20-%201912.13465v1.pdf%3A%3A9%2B%2B0.63%3B%3Bannot-9-4%5D%5BExploration%20is%20a%20challenge%20with%20RCP%5D%5D">2. Exploration is a challenge with RCP</a></li>
</ul>
</div>
</div>
<p>
Paper: <a href="papers/Reward Conditioned Policies - 1912.13465v1.pdf">Reward Conditioned Policies - 1912.13465v1.pdf</a>
Authors: <a href="aviral_kumar.html#ID-482D9EC3-12D2-49F8-93E4-9236B8BEA7D3">Aviral Kumar</a>, <a href="sergey_levine.html#ID-65DED394-F17D-4C6A-A86D-FE48A5EED74A">Sergey Levine</a>
</p>
<div id="outline-container-Any%20trajectory%20is%20optimal%20trajectory%20when%20conditioned%20on%20matching%20the%20reward" class="outline-2">
<h2 id="Any%20trajectory%20is%20optimal%20trajectory%20when%20conditioned%20on%20matching%20the%20reward"><span class="section-number-2">1.</span> Any trajectory is optimal trajectory when conditioned on matching the reward</h2>
<div class="outline-text-2" id="text-1">
<blockquote>
<p>
Non-expert trajectories collected from suboptimal policies can be viewed as optimal supervision, not for maximizing the reward, but for matching the reward of the given trajectory. (<a href="file:///Users/bpanthi977/Documents/synced/Notes/papers/Reward Conditioned Policies - 1912.13465v1.pdf#page=1">pg. 1</a>)
</p>

<p>
By then conditioning the policy on the numerical value of the reward, we can obtain a policy that generalizes to larger returns.
</p>
</blockquote>
</div>
</div>
<div id="outline-container-%5B%5Bpdf%3A~%2FDocuments%2Fsynced%2FNotes%2Fpapers%2FReward%20Conditioned%20Policies%20-%201912.13465v1.pdf%3A%3A9%2B%2B0.63%3B%3Bannot-9-4%5D%5BExploration%20is%20a%20challenge%20with%20RCP%5D%5D" class="outline-2">
<h2 id="%5B%5Bpdf%3A~%2FDocuments%2Fsynced%2FNotes%2Fpapers%2FReward%20Conditioned%20Policies%20-%201912.13465v1.pdf%3A%3A9%2B%2B0.63%3B%3Bannot-9-4%5D%5BExploration%20is%20a%20challenge%20with%20RCP%5D%5D"><span class="section-number-2">2.</span> <a href="file:///Users/bpanthi977/Documents/synced/Notes/papers/Reward Conditioned Policies - 1912.13465v1.pdf#page=9">Exploration is a challenge with RCP</a></h2>
<div class="outline-text-2" id="text-2">
<blockquote>
<p>
We expect that exploration is likely to be one of the major challenges with reward-conditioned policies: the methods we presented rely on general- ization and random chance to acquire trajectories that improve in performance over those previously seen in the dataset. Sometimes the reward-conditioned policies might generalize successfully, and sometimes they might not.
</p>
</blockquote>


<hr />
<h3>References</h3>

<ul class="org-ul">
<li><a href="https://arxiv.org/abs/1912.13465">https://arxiv.org/abs/1912.13465</a></li>
</ul>
<hr />
<h3>Backlinks</h3>

<ul class="org-ul">
<li><a href="supervised_learning_for_rl.html#ID-FFD963C6-CED1-4D80-9DC0-ABCC1AB9695A">Supervised Learning for RL</a></li>
<li><a href="reinforcement_learning.html#ID-B010228E-5555-4D07-8E63-B54E476A249E">Reinforcement Learning</a></li>
</ul>
</div>
</div>
</div>
<div id="postamble" class="status">
<hr/>You can send your feedback, queries <a href="mailto:bpanthi977@gmail.com?subject=Feedback: Reward Conditioned Policies">here</a><span id="visits"></span><span id="claps"></span><div id="claps-message"></div><a href="https://bpanthi977.github.io/braindump/data/rss.xml"><img src="https://bpanthi977.github.io/braindump/data/rss.png" /></a>
</div>
</body>
</html>
