<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Transformer Architecture</title>
<meta name="author" content="Bibek Panthi" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="../css/stylesheet.css" />
<link rel="stylesheet" type="text/css" href="../css/braindump.css" />
<script src="../js/counters.js" type="text/javascript"></script>
<script src="../js/URI.js" type="text/javascript"></script>
<script src="../js/pages.js" type="text/javascript"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="H" href="./index.html"> HOME </a>
</div><div id="preamble" class="status">
<p class="date">Date: <span class="timestamp-wrapper"><span class="timestamp">[2023-03-17 Fri]</span></span></p>
</div>
<div id="content" class="content">
<h1 class="title">Transformer Architecture</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#Idenitfying%20parts%20to%20attend%20to%20is%20similar%20to%20_Search%20problem_">1. Idenitfying parts to attend to is similar to <span class="underline">Search problem</span></a></li>
<li><a href="#Self-Attention%20in%20Sequence%20Modelling">2. Self-Attention in Sequence Modelling</a></li>
</ul>
</div>
</div>
<p>
See <a href="mit_6_s191_introduction_to_deep_learning.html#ID-B81B32C3-D182-4BDC-9235-8079D7C250CB">RNN and Transformers (MIT 6.S191 2022)</a> for link to video lecture.
</p>

<p>
Transformer architecture
</p>
<ul class="org-ul">
<li>Identify parts to attend to</li>
<li>and Extract features with high attention</li>
</ul>

<p>
Attention has been used in: 
</p>
<ul class="org-ul">
<li>AlphaFold2: Uses Self-Attention</li>
<li>BERT, GPT-3</li>
<li>Vision Transformers in Computer Vision</li>
</ul>
<div id="outline-container-Idenitfying%20parts%20to%20attend%20to%20is%20similar%20to%20_Search%20problem_" class="outline-2">
<h2 id="Idenitfying%20parts%20to%20attend%20to%20is%20similar%20to%20_Search%20problem_"><span class="section-number-2">1.</span> Idenitfying parts to attend to is similar to <span class="underline">Search problem</span></h2>
<div class="outline-text-2" id="text-1">
<ul class="org-ul">
<li>Enter a Query (\(Q\)) for search</li>
<li>Extract key information \(K_i\) for each search result</li>
<li>Compute how similar is the key to the query: Attention Mask</li>
<li>Extract required information from the search i.e. Value \(V\)</li>
</ul>


<div id="figure-1" class="figure">
<p><img src="data/mit_6_s191_introduction_to_deep_learning/attention_as_search-20230316105659.png" alt="attention_as_search-20230316105659.png" />
</p>
<p><span class="figure-number">Figure 1: </span>Attention as Search</p>
</div>
</div>
</div>
<div id="outline-container-Self-Attention%20in%20Sequence%20Modelling" class="outline-2">
<h2 id="Self-Attention%20in%20Sequence%20Modelling"><span class="section-number-2">2.</span> Self-Attention in Sequence Modelling</h2>
<div class="outline-text-2" id="text-2">
<p>
Goal: Identify and attend to most important features in input
</p>

<ol class="org-ol">
<li><p>
We want to elimintate recurrence because that what gave rise to the limitations. So, we need to <b>encode position information</b>
</p>

<div id="figure-2" class="figure">
<p><img src="data/mit_6_s191_introduction_to_deep_learning/position_aware_encoding-20230316113706.png" alt="position_aware_encoding-20230316113706.png" />
</p>
<p><span class="figure-number">Figure 2: </span>Position-Aware Encoding (@ 0:48:32)</p>
</div></li>

<li>Extract, <b>query, key, value</b> for search 
<ul class="org-ul">
<li>Multiply the positional encoding with three matrices to get query, key and value encoding for each word</li>
</ul></li>

<li>Compute <b>attention weighting</b> (A matix of post-softmax attention scores)
<ul class="org-ul">
<li><p>
Compute pairwise similarity between each query and key =&gt; Dot Product (0:51:01)
</p>

<p>
Attention Score = \(\frac {Q . K^T} {scaling}\) 
</p></li>
<li>Apply softmax to the attention score to get value in \([0, 1]\)</li>
</ul></li>
<li>Extract <b>features with high attention</b>: Multiply attention weighting with Value.</li>
</ol>


<div id="figure-3" class="figure">
<p><img src="data/mit_6_s191_introduction_to_deep_learning/self_attention_head-20230316114501.png" alt="self_attention_head-20230316114501.png" />
</p>
<p><span class="figure-number">Figure 3: </span>Self-Attention Head</p>
</div>



<hr />
<h3>Backlinks</h3>

<ul class="org-ul">
<li><a href="ai_deception.html#ID-3F32B30B-EB59-4A84-919B-E0C4F6E91186">AI Deception</a></li>
<li><a href="gelu_gaussian_error_linear_units.html#ID-F50D443F-311E-4089-A49F-643F416322D5">GELU (Gaussian Error Linear Units)</a></li>
<li><a href="Recurrent neural network.html#ID-BCFBAB21-B79D-448A-A339-7F2228AFB121">Limitations of RNN</a></li>
</ul>
</div>
</div>
</div>
<div id="postamble" class="status">
<hr/>You can send your feedback, queries <a href="mailto:bpanthi977@gmail.com?subject=Feedback: Transformer Architecture">here</a><span id="visits"></span><span id="claps"></span><div id="claps-message"></div>
</div>
</body>
</html>
