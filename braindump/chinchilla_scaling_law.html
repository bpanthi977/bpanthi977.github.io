<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Chinchilla Scaling Law</title>
<meta name="author" content="Bibek Panthi" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="../css/stylesheet.css" />
<link rel="stylesheet" type="text/css" href="../css/braindump.css" />
<script src="../js/counters.js" type="text/javascript"></script>
<script src="../js/URI.js" type="text/javascript"></script>
<script src="../js/pages.js" type="text/javascript"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="./index.html"> UP </a>
 |
 <a accesskey="H" href="../index.html"> HOME </a>
</div><div id="preamble" class="status">
<p class="date">Date: <span class="timestamp-wrapper"><span class="timestamp">&lt;2024-10-01 Tue&gt;</span></span></p>
</div>
<div id="content" class="content">
<h1 class="title">Chinchilla Scaling Law</h1>
<p>
Chinchilla model (70B parameters) was trained with 1.4T token. ie. a 1:20 parameters:tokens ratio. And it outperformed other models under same compute budget.
</p>

<p>
Compute optimal (i.e. most accuracy under a fixed FLOPs budget) can be got with propertionally increasing the tokens with the parameters size. 
</p>


<p>
From the paper: <a href="https://arxiv.org/abs/2203.15556">Training Compute-Optimal Large Language Models</a>
</p>
<blockquote>
<p>
We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for computeoptimal training, the model size and the number of training tokens should be scaled equally
</p>
</blockquote>

<p>
Compute Optimal doesn't imply convergence. Or Optimality in terms of model size.
</p>

<p>
<a href="https://x.com/karpathy/status/1781033433336262691">https://x.com/karpathy/status/1781033433336262691</a>
</p>


<p>
Regarding LLaMA3 (8B parameters, 70B Tokens), Andrej Karpathy <a href="https://x.com/karpathy/status/1781028605709234613">says</a>:
</p>

<blockquote>
<p>
Scaling laws. Very notably, 15T is a very very large dataset to train with for a model as "small" as 8B parameters, and this is not normally done and is new and very welcome. The Chinchilla "compute optimal" point for an 8B model would be train it for ~200B tokens. (if you were only interested to get the most "bang-for-the-buck" w.r.t. model performance at that size). So this is training ~75X beyond that point, which is unusual but personally, I think extremely welcome. Because we all get a very capable model that is very small, easy to work with and inference. Meta mentions that even at this point, the model doesn't seem to be "converging" in a standard sense. In other words, the LLMs we work with all the time are significantly undertrained by a factor of maybe 100-1000X or more, nowhere near their point of convergence. Actually, I really hope people carry forward the trend and start training  and releasing even more long-trained, even smaller models.
</p>
</blockquote>

<hr />
<h3>Backlinks</h3>

<ul class="org-ul">
<li><a href="evaluation_of_pre_training_llms_on_leadership_class_supercomputers.html#ID-EAFE31A1-2F4A-4F91-B4ED-B1E743C99B8C">Evaluation of Pre-training LLMs on Supercomputers</a></li>
</ul>
</div>
<div id="postamble" class="status">
<hr/>You can send your feedback, queries <a href="mailto:bpanthi977@gmail.com?subject=Feedback: Chinchilla Scaling Law">here</a><span id="visits"></span><span id="claps"></span><div id="claps-message"></div><a href="https://bpanthi977.github.io/braindump/data/rss.xml"><img src="https://bpanthi977.github.io/braindump/data/rss.png" /></a>
</div>
</body>
</html>
