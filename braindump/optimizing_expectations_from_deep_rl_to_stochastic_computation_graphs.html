<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Optimizing Expectations - From Deep RL to Stochastic Computation Graphs</title>
<meta name="author" content="Bibek Panthi" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="../css/stylesheet.css" />
<link rel="stylesheet" type="text/css" href="../css/braindump.css" />
<script src="../js/counters.js" type="text/javascript"></script>
<script src="../js/URI.js" type="text/javascript"></script>
<script src="../js/pages.js" type="text/javascript"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="H" href="./index.html"> HOME </a>
</div><div id="preamble" class="status">
<p class="date">Date: <span class="timestamp-wrapper"><span class="timestamp">&lt;2024-10-07 Mon&gt;</span></span></p>
</div>
<div id="content" class="content">
<h1 class="title">Optimizing Expectations - From Deep RL to Stochastic Computation Graphs</h1>
<p>
PhD thesis by John Schulman
</p>

<ul class="org-ul">
<li>RL as a special case of optimizing over Stochastic Computation Graphs [<a href="books/Optimizing Expectations - From RL to Stochastic Computation Graphs.pdf#page=3">Page 3</a>]</li>

<li>Category of RL Algorithms
What to Learn?
<ul class="org-ul">
<li>Policy: Policy Optimization
<ul class="org-ul">
<li>DFO (Derivative Free Optimization) [e.g. Evolutionary Algorithm, HyperNEAT, cross-entropy, covariance matrix adaptation, &#x2026;] [pg. 4]</li>
<li>Policy Gradietn Methods</li>
</ul></li>

<li>Approximate Dynamic Progamming: Value functions</li>
<li>Learn the Dynamics</li>
<li>Combination of above three</li>
</ul></li>

<li>Policy Optimization
<ul class="org-ul">
<li>Score function estimator, pathwise derivative estimator [<a href="books/Optimizing Expectations - From RL to Stochastic Computation Graphs.pdf#page=13">Page 13</a>]</li>
</ul></li>

<li><p>
Vanilla Policy Gradient has problems (<a href="books/Optimizing Expectations - From RL to Stochastic Computation Graphs.pdf#page=24">Page 24</a>)
</p>
<ul class="org-ul">
<li>Not sample efficient</li>

<li>Hard to choose stepsize as traning  progresses</li>

<li>Prematurely converges to a nearly deterministic policy with suboptimal behaviours (adding entroy bouns usually fails)</li>
</ul>

<p>
Two techniques to fix this:
</p>
<ul class="org-ul">
<li>Step in Natural gradient direction isntead of gradient direction</li>

<li>Choose stepsize in optimial way ensuring montonic improvemnet</li>
</ul></li>

<li>We can compute gradient of a cost function from any stochastic computation graph, by following an algorithm. It is same as computing the derivative of an equivalent Surrogate Loss function defined on that graph.

<ul class="org-ul">
<li>This derivative computation is itself a stochastic computation graph, and thus higher order derivatives can also be computed.</li>

<li></li>
</ul></li>
</ul>
</div>
<div id="postamble" class="status">
<hr/>You can send your feedback, queries <a href="mailto:bpanthi977@gmail.com?subject=Feedback: Optimizing Expectations - From Deep RL to Stochastic Computation Graphs">here</a><span id="visits"></span><span id="claps"></span><div id="claps-message"></div>
</div>
</body>
</html>
