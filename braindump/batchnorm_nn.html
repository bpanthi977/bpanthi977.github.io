<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>BatchNorm NN</title>
<meta name="author" content="Bibek Panthi" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="../css/stylesheet.css" />
<link rel="stylesheet" type="text/css" href="../css/braindump.css" />
<script src="../js/counters.js" type="text/javascript"></script>
<script src="../js/URI.js" type="text/javascript"></script>
<script src="../js/pages.js" type="text/javascript"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="./index.html"> UP </a>
 |
 <a accesskey="H" href="../index.html"> HOME </a>
</div><div id="preamble" class="status">
<p class="date">Date: <span class="timestamp-wrapper"><span class="timestamp">[2023-05-19 Fri]</span></span></p>
</div>
<div id="content" class="content">
<h1 class="title">BatchNorm NN</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#Why%20it%20works%3F">1. Why it works?</a></li>
<li><a href="#Use%20before%20activation">2. Use before activation</a></li>
<li><a href="#ID-1E5921A1-EF19-47B9-B1AE-66FEBE51461B">3. BatchNorm affects Gradient Penalty in WGAN</a></li>
</ul>
</div>
</div>
<ul class="org-ul">
<li>Improves train loss but degrades test loss</li>
</ul>

<p>
Its easy for Neural Networks when then data is normalized.  The most common form of data normalization is
</p>
<ul class="org-ul">
<li>centering the data on zero</li>
<li>and giving the data a unit standard deviation</li>
</ul>

<p>
BatchNormalization does this not for the input only but for the output of transformations in the Neural Network.
</p>
<div id="outline-container-Why%20it%20works%3F" class="outline-2">
<h2 id="Why%20it%20works%3F"><span class="section-number-2">1.</span> Why it works?</h2>
<div class="outline-text-2" id="text-1">
<ul class="org-ul">
<li><span class="underline">May not be internal covariate shift</span>: Although the original paper stated that batch normalization operates by "reducing internal covariate shift", no one really knows for sure why batch normalization helps.</li>

<li><span class="underline">Helps with gradient propagation</span>: In practice, the main effect of batch normalization appears to be that it helps with gradient propagation much like residual connections and thus allows for deeper networks.</li>
</ul>

<p>
(pg. 256 <a href="deep_learning_with_python_francois_chollet.html#ID-857CAF91-5AFC-4F9E-8BD6-0A1804942B7B">Deep Learning with Python - François Chollet</a>)
</p>
</div>
</div>
<div id="outline-container-Use%20before%20activation" class="outline-2">
<h2 id="Use%20before%20activation"><span class="section-number-2">2.</span> Use before activation</h2>
<div class="outline-text-2" id="text-2">
<p>
This is also recommended by <a href="deep_learning_with_python_francois_chollet.html#ID-857CAF91-5AFC-4F9E-8BD6-0A1804942B7B">Deep Learning with Python - François Chollet</a> (pg. 256). Although he mentions that this is still debatable.
</p>

<blockquote>
<p>
The intuitive reason for this approach is that batch normalization will center your inputs on zero, while your relu activation uses zero as a pivot for keeping or dropping activated channels: doing normalization before the activation maximizes the utilization of the relu. That said, this ordering best practice is not exactly critical, so if you do convolution, then activation, and then batch normalization, your model will still train, and you won't necessarily see worse results.
</p>
</blockquote>
</div>
</div>
<div id="outline-container-ID-1E5921A1-EF19-47B9-B1AE-66FEBE51461B" class="outline-2">
<h2 id="ID-1E5921A1-EF19-47B9-B1AE-66FEBE51461B"><span class="section-number-2">3.</span> BatchNorm affects Gradient Penalty in WGAN</h2>
<div class="outline-text-2" id="text-3">
<p>
<a href="batchnorm_nn.html#ID-AFD3AD13-EF16-4307-82D9-988A7E098851">Batch normalization</a> is avoided for the critic (discriminator). Batch normalization creates correlations between samples in the same batch. It impacts the effectiveness of the gradient penalty which is confirmed by experiments.
</p>

<hr />
<h3>References</h3>

<ul class="org-ul">
<li><a href="https://jonathan-hui.medium.com/gan-wasserstein-gan-wgan-gp-6a1a2aa1b490">https://jonathan-hui.medium.com/gan-wasserstein-gan-wgan-gp-6a1a2aa1b490</a></li>
</ul>
</div>
</div>
</div>
<div id="postamble" class="status">
<hr/>You can send your feedback, queries <a href="mailto:bpanthi977@gmail.com?subject=Feedback: BatchNorm NN">here</a><span id="visits"></span><span id="claps"></span><div id="claps-message"></div><a href="https://bpanthi977.github.io/braindump/data/rss.xml"><img src="https://bpanthi977.github.io/braindump/data/rss.png" /></a>
</div>
</body>
</html>
