<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Evaluation of Pre-training LLMs on Supercomputers</title>
<meta name="author" content="Bibek Panthi" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="../css/stylesheet.css" />
<link rel="stylesheet" type="text/css" href="../css/braindump.css" />
<script src="../js/counters.js" type="text/javascript"></script>
<script src="../js/URI.js" type="text/javascript"></script>
<script src="../js/pages.js" type="text/javascript"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="./index.html"> UP </a>
 |
 <a accesskey="H" href="../index.html"> HOME </a>
</div><div id="preamble" class="status">
<p class="date">Date: <span class="timestamp-wrapper"><span class="timestamp">&lt;2024-10-01 Tue&gt;</span></span></p>
</div>
<div id="content" class="content">
<h1 class="title">Evaluation of Pre-training LLMs on Supercomputers</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#Contributions%2FScope">1. Contributions/Scope</a></li>
<li><a href="#Setup">2. Setup</a></li>
<li><a href="#Parallelism%20Approach%20for%20Distributed%20Training%20of%20LLMs">3. Parallelism Approach for Distributed Training of LLMs</a>
<ul>
<li><a href="#Big%20LLMs%20require%20distributed%20training">3.1. Big LLMs require distributed training</a></li>
<li><a href="#Approaches%20to%20Parallelism">3.2. Approaches to Parallelism</a></li>
<li><a href="#Data%20Parallel">3.3. Data Parallel</a></li>
<li><a href="#Pipeline%20Parallel">3.4. Pipeline Parallel</a></li>
<li><a href="#Tensor%20Parallel">3.5. Tensor Parallel</a></li>
</ul>
</li>
<li><a href="#Experimental%20Setup%20%28%26%20Memory%2C%20Bandwidth%20Analysis%29">4. Experimental Setup (&amp; Memory, Bandwidth Analysis)</a>
<ul>
<li><a href="#Experimental%20Setup%20%28%26%20Memory%2C%20Bandwidth%20Analysis%29--FSDP%20%28Fully%20Sharded%20Data%20Parallel%29">4.1. FSDP (Fully Sharded Data Parallel)</a></li>
<li><a href="#DeepSpeed-Megatron">4.2. DeepSpeed-Megatron</a></li>
</ul>
</li>
<li><a href="#Runtime%20and%20Energy%20Analysis">5. Runtime and Energy Analysis</a>
<ul>
<li><a href="#Compute%20Time%20~%20120%20P%5E2%20%2F%20R">5.1. Compute Time ~ 120 P<sup>2</sup> / R</a></li>
<li><a href="#Energy%20%3D%20t%20%5Ctimes%20R_watt">5.2. Energy = t &times; R<sub>watt</sub></a></li>
</ul>
</li>
<li><a href="#Evaluation">6. Evaluation</a>
<ul>
<li><a href="#Scaling%20Analysis">6.1. Scaling Analysis</a></li>
<li><a href="#Memory%20%26%20Bandwidth">6.2. Memory &amp; Bandwidth</a></li>
<li><a href="#Power">6.3. Power</a></li>
<li><a href="#Train%20time%20and%20Energy%20Projections">6.4. Train time and Energy Projections</a></li>
</ul>
</li>
<li><a href="#Conclusions%20%28%26%20Limitations%29">7. Conclusions (&amp; Limitations)</a>
<ul>
<li><a href="#Software%20improvements%20might%20change%20the%20projections">7.1. Software improvements might change the projections</a></li>
<li><a href="#Large%20per%20node%20memory%20and%20communication%20bandwidth%20required">7.2. Large per node memory and communication bandwidth required</a></li>
<li><a href="#Current%20Memory%20Bandwidth%20is%20limiting%20for%20Crusher">7.3. Current Memory Bandwidth is limiting for Crusher</a></li>
<li><a href="#Frontier%20is%20promising%20for%20GPT3%20size%20model%20training">7.4. Frontier is promising for GPT3 size model training</a></li>
</ul>
</li>
<li><a href="#Thank%20you%21">8. Thank you!</a></li>
<li><a href="#Misc">9. Misc</a>
<ul>
<li><a href="#DL%20scales%20with%20model%20and%20data">9.1. DL scales with model and data</a></li>
<li><a href="#Nickel">9.2. Nickel</a></li>
<li><a href="#Mistakes">9.3. Mistakes</a></li>
<li><a href="#Model%20Parallel%20%20%3D%20PP%20or%20TP">9.4. Model Parallel = PP or TP</a></li>
<li><a href="#FSDP%20%3D%20ZeRO%20Stage%203">9.5. FSDP = ZeRO Stage 3</a></li>
</ul>
</li>
</ul>
</div>
</div>
<p>
Summary and Presentation on the paper:
</p>

<p>
Evaluation of Pre-training Large Language Models on Leadership-class Supercomputers
</p>

<p>
(Links: [<a href="https://www.osti.gov/pages/biblio/1994640">web</a>][<a href="papers/Evaluation of Pre-training LLMs on Leadership Class Supercomputers.pdf">file</a>][<a href="papers/Evaluation of Pre-training LLMs on Leadership Class Supercomputers.pdf#page=nil">pdf</a>])
</p>
<div id="outline-container-Contributions%2FScope" class="outline-2">
<h2 id="Contributions%2FScope"><span class="section-number-2">1.</span> Contributions/Scope</h2>
<div class="outline-text-2" id="text-1">
<p>
[<a href="papers/Evaluation of Pre-training LLMs on Leadership Class Supercomputers.pdf#page=2">Page 2</a>]
</p>

<p>
Evaluates the Time, Memory and Energy cost of traning LLMs on SuperComputers using different distributed training methods.
</p>

<ul class="org-ul">
<li><b>Performance analysis of Training LLMs</b>: Give theoretical equations for
<ul class="org-ul">
<li>Memory</li>
<li>Bandwidth</li>
<li>Time</li>
<li>Energy</li>
</ul></li>

<li><b>Projection of Cost</b> : Find train time and energy costs and project them for full training [<a href="papers/Evaluation of Pre-training LLMs on Leadership Class Supercomputers.pdf#page=15">Table 2</a>]</li>
</ul>

<blockquote>
<p>
We conduct a first-of-its-kind performance analysis to understand the time and energy cost of pre-training LLMs on the Department of Energy (DOE)’s leadership-class supercomputers [<a href="papers/Evaluation of Pre-training LLMs on Leadership Class Supercomputers.pdf#page=1">pg. 1</a>]
</p>
</blockquote>

<ul class="org-ul">
<li><b>Baseline and Insights</b>
<ul class="org-ul">
<li>Drawing insights on future platforms for training LLMs.</li>
<li>Establish a performance baseline of two frameworks on two different systems [<a href="papers/Evaluation of Pre-training LLMs on Leadership Class Supercomputers.pdf#page=2">Page 2</a>][<a href="papers/Evaluation of Pre-training LLMs on Leadership Class Supercomputers.pdf#page=11">Fig 6</a>]</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-Setup" class="outline-2">
<h2 id="Setup"><span class="section-number-2">2.</span> Setup</h2>
<div class="outline-text-2" id="text-2">
<p>
2 SuperComputers:
</p>
<ul class="org-ul">
<li>Crusher Supercomputer: AMD M1250X [<a href="papers/Evaluation of Pre-training LLMs on Leadership Class Supercomputers.pdf#page=6">Fig 3</a>]</li>
<li>Summit Supercomputer: NVIDIA V100 [<a href="papers/Evaluation of Pre-training LLMs on Leadership Class Supercomputers.pdf#page=7">Fig 4</a>]</li>
</ul>

<p>
(Crusher has <a href="https://docs.olcf.ornl.gov/systems/crusher_quick_start_guide.html">identical hardware</a> &amp; similar software as Frontier system - World's fastest Supercomputer)
</p>

<p>
2 Training frameworks (both PyTorch based Distributed Traning Methods) [<a href="papers/Evaluation of Pre-training LLMs on Leadership Class Supercomputers.pdf#page=5">pg. 5</a>]
</p>
<ul class="org-ul">
<li>Fully Sharded Data Parallel (FSDP)</li>
<li>DeepSpeed-Megatron [<a href="papers/Evaluation of Pre-training LLMs on Leadership Class Supercomputers.pdf#page=5">pg. 5</a>]</li>
</ul>

<p>
Multiple Models:
</p>
<ul class="org-ul">
<li>175B GPT</li>
<li>1T GPT</li>
<li>1.4B, 16B</li>
</ul>
</div>
</div>
<div id="outline-container-Parallelism%20Approach%20for%20Distributed%20Training%20of%20LLMs" class="outline-2">
<h2 id="Parallelism%20Approach%20for%20Distributed%20Training%20of%20LLMs"><span class="section-number-2">3.</span> Parallelism Approach for Distributed Training of LLMs</h2>
<div class="outline-text-2" id="text-3">
</div>
<div id="outline-container-Big%20LLMs%20require%20distributed%20training" class="outline-3">
<h3 id="Big%20LLMs%20require%20distributed%20training"><span class="section-number-3">3.1.</span> Big LLMs require distributed training</h3>
<div class="outline-text-3" id="text-3-1">
<p>
Big LLMs have high computation and memory requirements and they don't always fit in a single GPUs. Even if they do, scaling up the training requires multiple GPUs. This means a distributed approach to training is required.
</p>
</div>
</div>
<div id="outline-container-Approaches%20to%20Parallelism" class="outline-3">
<h3 id="Approaches%20to%20Parallelism"><span class="section-number-3">3.2.</span> Approaches to Parallelism</h3>
<div class="outline-text-3" id="text-3-2">
<p>
[<a href="papers/Evaluation of Pre-training LLMs on Leadership Class Supercomputers.pdf#page=5">pg. 5</a>][<a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/features/parallelisms.html">docs.nvidia.com</a>]
</p>

<ul class="org-ul">
<li>Data Parallelism:
<ul class="org-ul">
<li>Distributed Data Parallel</li>
<li>Zero Redundancy Optimizer (<a href="https://arxiv.org/abs/1910.02054">ZeRO</a>) [<a href="https://medium.com/@Shrishml/breaking-the-gmemory-barrier-how-zero-revolutionizes-large-language-model-training-8e00d2e2f30a">Article</a>]
<ul class="org-ul">
<li>Stage 1: Optimizer states (os)</li>
<li>Stage 2: Gradients (os + g)</li>
<li>Stage 3: Parameters (os + g + p)</li>
</ul></li>
<li>FSDP \(\approx\) Stage 3 of ZeRO</li>
</ul></li>
<li>Pipeline Parallelism: Gpipe</li>
<li>Tensor Parallelism: Megatron</li>
</ul>
</div>
</div>
<div id="outline-container-Data%20Parallel" class="outline-3">
<h3 id="Data%20Parallel"><span class="section-number-3">3.3.</span> Data Parallel</h3>
<div class="outline-text-3" id="text-3-3">
<p>
[<a href="papers/Evaluation of Pre-training LLMs on Leadership Class Supercomputers.pdf#page=5">Page 5</a>]
</p>

<p>
Each model replica processes different batch of data and then synchronize the backward pass.
</p>
</div>
<ol class="org-ol">
<li><a id="Distributed%20Data%20Parallel"></a>Distributed Data Parallel<br />
<div class="outline-text-4" id="text-3-3-1">
<p>
[<a href="https://dl.acm.org/doi/10.14778/3415478.3415530">Paper</a>]
</p>

<ul class="org-ul">
<li>Model is replicated across all devices</li>
<li>Gradients are communicated to keep the models in sync</li>
</ul>

<p>
Advanced Techniques:
</p>
<ul class="org-ul">
<li>Overlapping communcation with computation</li>
<li>Bucketing gradients</li>
<li>Skipping gradient synchronization</li>
</ul>

<p>
By proper tuning, near linear scaling upte 256 GPUs was obtained.
</p>

<p>
But, it requires model to fit in a single GPU.
</p>

\begin{equation*}
M = M_{total}
\end{equation*}

\begin{equation*}
C \sim 2 \times M_{p}
\end{equation*}
</div>
</li>
<li><a id="Experimental%20Setup%20%28%26%20Memory%2C%20Bandwidth%20Analysis%29--FSDP%20%28Fully%20Sharded%20Data%20Parallel%29"></a>FSDP (Fully Sharded Data Parallel)<br />
<div class="outline-text-4" id="text-3-3-2">
<p>
[<a href="https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/">PyTorch Docs</a>][<a href="https://arxiv.org/abs/2304.11277">FSDP Paper</a>]
</p>

<ul class="org-ul">
<li>Model is not replicated across all devices</li>
<li>Model Parameters, Optimizer States and Gradients are shared among devices</li>
<li>For computation
<ul class="org-ul">
<li>each unit is fully materialized (i.e. unshared)</li>
<li>Computation is done</li>
<li>then it is shared to free memory</li>
</ul></li>
</ul>
</div>
<ol class="org-ol">
<li><a id="FSDP%20Algorithm"></a>FSDP Algorithm<br />
<div class="outline-text-5" id="text-3-3-2-1">
<ul class="org-ul">
<li>Model is shared (across layers ) in units</li>
<li>For computation
<ul class="org-ul">
<li>each unit is fully materialized (i.e. unshared)</li>
<li>Computation is done</li>
<li>then it is shared to free memory</li>
</ul></li>
</ul>



<div id="figure-1" class="figure">
<p><img src="data/evaluation_of_pre_training_llms_on_leadership_class_supercomputers/fsdp_algorithm-20241001192644.png" alt="fsdp_algorithm-20241001192644.png" />
</p>
<p><span class="figure-number">Figure 1: </span>FSDP Algorithm (Source: <a href="https://arxiv.org/abs/2304.11277">FSDP Paper</a> - Page 3)</p>
</div>
</div>
</li>
<li><a id="Memory%20and%20Communication"></a>Memory and Communication<br />
<div class="outline-text-5" id="text-3-3-2-2">
\begin{equation*}
M = M_{total}
\end{equation*}

<p>
But communication is increased compared to DDP (by 1.5x [<a href="papers/Evaluation of Pre-training LLMs on Leadership Class Supercomputers.pdf#page=7">pg. 7</a>], See <a href="papers/ZeRO - Memory Optimization Toward Training Trillion Parameter Models - 1910.02054v3.pdf#page=14">ZeRO 7.2.2</a>).
</p>

\begin{equation*}
C \sim 3 \times M_p
\end{equation*}
</div>
</li>
</ol>
</li>
</ol>
</div>
<div id="outline-container-Pipeline%20Parallel" class="outline-3">
<h3 id="Pipeline%20Parallel"><span class="section-number-3">3.4.</span> Pipeline Parallel</h3>
<div class="outline-text-3" id="text-3-4">
<p>
[<a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/features/parallelisms.html#pipeline-parallelism">NVIDIA Docs</a>]
</p>

<ul class="org-ul">
<li>Different layers of models are stored in different devices</li>
<li>Computation proceeds on a device and the results are passed to next device to computed next layer</li>
<li>Micro Batching is done to hide pipeline bubble</li>
</ul>


<div id="figure-2" class="figure">
<p><img src="data/evaluation_of_pre_training_llms_on_leadership_class_supercomputers/pipeline_parallelism-20241007165347.png" alt="pipeline_parallelism-20241007165347.png" />
</p>
<p><span class="figure-number">Figure 2: </span>Pipeline Parallelism <a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/_images/pp.gif">Animation</a></p>
</div>
</div>
<ol class="org-ol">
<li><a id="GPipe%20-%20To%20reduce%20pipeline%20bubble"></a>GPipe - To reduce pipeline bubble<br />
<div class="outline-text-4" id="text-3-4-1">

<div id="figure-3" class="figure">
<p><img src="data/evaluation_of_pre_training_llms_on_leadership_class_supercomputers/pipeline_bubble-20241007170918.png" alt="pipeline_bubble-20241007170918.png" />
</p>
<p><span class="figure-number">Figure 3: </span>Pipeline Bubble [Source: <a href="https://siboehm.com/articles/22/pipeline-parallel-training">siboehm.com</a>]</p>
</div>
</div>
</li>
</ol>
</div>
<div id="outline-container-Tensor%20Parallel" class="outline-3">
<h3 id="Tensor%20Parallel"><span class="section-number-3">3.5.</span> Tensor Parallel</h3>
<div class="outline-text-3" id="text-3-5">
<p>
Parallel processing of a layer by tensor decomposition.
</p>

<ul class="org-ul">
<li>Multiplying each column with the input separately</li>
<li>Collect the results from GPUs and concatenate together</li>
</ul>


<div id="figure-4" class="figure">
<p><img src="data/evaluation_of_pre_training_llms_on_leadership_class_supercomputers/tensor_parallelism-20241001194051.png" alt="tensor_parallelism-20241001194051.png" />
</p>
<p><span class="figure-number">Figure 4: </span>Tensor Parallelism</p>
</div>
</div>
<ol class="org-ol">
<li><a id="TP%20Across%20Layers"></a>TP Across Layers<br />
<div class="outline-text-4" id="text-3-5-1">

<div id="figure-5" class="figure">
<p><img src="data/evaluation_of_pre_training_llms_on_leadership_class_supercomputers/tensor_parallelism_across_layers-20241009075736.png" alt="tensor_parallelism_across_layers-20241009075736.png" />
</p>
<p><span class="figure-number">Figure 5: </span>Tensor Parallelism Across Layers</p>
</div>

<p>
[<a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/features/parallelisms.html#tensor-parallelism">docs.nvidia.com</a>]
</p>
</div>
</li>
</ol>
</div>
</div>
<div id="outline-container-Experimental%20Setup%20%28%26%20Memory%2C%20Bandwidth%20Analysis%29" class="outline-2">
<h2 id="Experimental%20Setup%20%28%26%20Memory%2C%20Bandwidth%20Analysis%29"><span class="section-number-2">4.</span> Experimental Setup (&amp; Memory, Bandwidth Analysis)</h2>
<div class="outline-text-2" id="text-4">
<p>
2 supercomputers, 2 training frameworks [<a href="papers/Evaluation of Pre-training LLMs on Leadership Class Supercomputers.pdf#page=2">pg. 2</a>]
</p>

<p>
SuperComputers:
</p>
<ul class="org-ul">
<li>Crusher Supercomputer: AMD MI250X</li>
<li>Summit Supercomputer: NVIDIA V100</li>
</ul>

<p>
Traning frameworks (both PyTorch based Distributed Traning Methods) [<a href="papers/Evaluation of Pre-training LLMs on Leadership Class Supercomputers.pdf#page=5">pg. 5</a>]
</p>
<ul class="org-ul">
<li>Fully Sharded Data Parallel (FSDP)</li>
<li>DeepSpeed-Megatron [<a href="papers/Evaluation of Pre-training LLMs on Leadership Class Supercomputers.pdf#page=5">pg. 5</a>]</li>
</ul>

<p>
Memory Requirements (Assumed for this paper. It can vary with precision used for parameters, optimizer states):
</p>
<ul class="org-ul">
<li>Model Parameters (\(M_p\))</li>
<li>Optimizer States (\(M_o = 2 \times M_p\); for Adam; Momemntum and Variance)</li>
<li>Gradient (\(M_g = 2 \times M_p\) (??))</li>
<li>Residue/Overheads (\(M_e = M_p\))</li>
</ul>
</div>
<div id="outline-container-Experimental%20Setup%20%28%26%20Memory%2C%20Bandwidth%20Analysis%29--FSDP%20%28Fully%20Sharded%20Data%20Parallel%29" class="outline-3">
<h3 id="Experimental%20Setup%20%28%26%20Memory%2C%20Bandwidth%20Analysis%29--FSDP%20%28Fully%20Sharded%20Data%20Parallel%29"><span class="section-number-3">4.1.</span> FSDP (Fully Sharded Data Parallel)</h3>
<div class="outline-text-3" id="text-4-1">
<p>
[<a href="papers/Evaluation of Pre-training LLMs on Leadership Class Supercomputers.pdf#page=6">pg. 6</a>]
Used in the experiments with Crusher supercomputer.
</p>

\begin{equation*}
M = M_{total} / N_{devices}
\end{equation*}

\begin{equation*}
M \sim 6 \times M_{p} / N_{devices}
\end{equation*}

\begin{equation*}
C \sim 3 \times M_p
\end{equation*}
</div>
<ol class="org-ol">
<li><a id="FSDP%20in%20Crusher"></a>FSDP in Crusher<br />
<div class="outline-text-4" id="text-4-1-1">

<div id="figure-6" class="figure">
<p><img src="data/evaluation_of_pre_training_llms_on_leadership_class_supercomputers/fsdp-20241001190654.png" alt="fsdp-20241001190654.png" />
</p>
<p><span class="figure-number">Figure 6: </span>FSDP</p>
</div>
</div>
</li>
</ol>
</div>
<div id="outline-container-DeepSpeed-Megatron" class="outline-3">
<h3 id="DeepSpeed-Megatron"><span class="section-number-3">4.2.</span> DeepSpeed-Megatron</h3>
<div class="outline-text-3" id="text-4-2">
</div>
<ol class="org-ol">
<li><a id="Megatron"></a>Megatron<br />
<div class="outline-text-4" id="text-4-2-1">
<p>
<a href="https://github.com/NVIDIA/Megatron-LM">https://github.com/NVIDIA/Megatron-LM</a>
</p>

<ul class="org-ul">
<li>A PyTorch based library by NVIDIA</li>
<li>Providing GPU-optimized techniques for Deep Learning
<ul class="org-ul">
<li>Layers (attention mechanism, normalization layers)</li>
<li>Activation recomputation, distributed checkpointing</li>
<li>Model parallelism techniques (tensor, sequence, pipeline, context, and MoE expert parallelism)</li>
</ul></li>
</ul>

<p>
EleutherAI implemented an OpenSource version of DeepSpeed-Megatron. The authors ported it to AMD for benchmarking. [<a href="papers/Evaluation of Pre-training LLMs on Leadership Class Supercomputers.pdf#page=8">Page 8</a>]
</p>
</div>
</li>
<li><a id="Parallelisms"></a>Parallelisms<br />
<div class="outline-text-4" id="text-4-2-2">
<p>
[<a href="papers/Evaluation of Pre-training LLMs on Leadership Class Supercomputers.pdf#page=7">pg. 7</a>]
</p>

<p>
Used in the experiments with Summit supercomputer.
</p>

<p>
Uses Data Parallelism (DP), Pipeline Parallelism (PP) and Tensor Parallelism (TP).
</p>

<ul class="org-ul">
<li>Tensor Parallelism: is within a node</li>
<li>Pipeline Parallelism: is in 2 stages. Each on a node with half of model layers</li>
<li>Data Parallelism: ZeRO (stage 1, i.e. optimizer states) are sharded among the remaining level of parallelism (i.e. \(N_{devices}/(PP \times TP)\)). To avoid excessive communication.</li>
</ul>
</div>
</li>
<li><a id="DeepSpeed-Megatron--Memory%20and%20Communication"></a>Memory and Communication<br />
<div class="outline-text-4" id="text-4-2-3">
<p>
Thus memory requirement per device is:
</p>

\begin{equation*}
M = \frac {M_p} {PP \times TP} + \frac {M_o} {DP} + \frac {M_g} {PP \times TP} + M_e
\end{equation*}

\begin{equation*}
M = \frac { 2 \times (DP + PP \times TP + 1)} {N_{devices}} M_p
\end{equation*}

<p>
And Communcation volume is [<a href="papers/Evaluation of Pre-training LLMs on Leadership Class Supercomputers.pdf#page=8">Eqn 6</a>] (\(2 \times M_p\) is for Data Parallelism, PP is negligible, TP ??):
</p>

\begin{equation*}
C \sim \big( \frac {TP} {N_{devices}} + 1 \big) \times 2 \times M_p
\end{equation*}
</div>
</li>
<li><a id="Fit%20the%20model%20and%20then%20use%20Data%20Parallelism"></a>Fit the model and then use Data Parallelism<br />
<div class="outline-text-4" id="text-4-2-4">
<p>
[<a href="papers/Evaluation of Pre-training LLMs on Leadership Class Supercomputers.pdf#page=7">pg. 7</a>]
</p>
<blockquote>
<p>
Because the scaling efficiencies of pipeline and tensor parallelism are worse than data parallelism (that is, they require more communication), the best scaling strategy is to <b>find the minimum number of nodes that can fit the model in memory</b> with model parallelism, and to <b>apply the remaining resources to data parallelism</b>.
</p>
</blockquote>
</div>
</li>
<li><a id="DeepSpeed-Megatron%20in%20Summit"></a>DeepSpeed-Megatron in Summit<br />
<div class="outline-text-4" id="text-4-2-5">

<div id="figure-7" class="figure">
<p><img src="data/presentation/deepspeed_megatron_in_summit-20241009090854.png" alt="deepspeed_megatron_in_summit-20241009090854.png" />
</p>
<p><span class="figure-number">Figure 7: </span>DeepSpeed-Megatron on 4 Summit nodes with TP=6, PP=2 and DP (Zero Stage 1)=2</p>
</div>
</div>
</li>
</ol>
</div>
</div>
<div id="outline-container-Runtime%20and%20Energy%20Analysis" class="outline-2">
<h2 id="Runtime%20and%20Energy%20Analysis"><span class="section-number-2">5.</span> Runtime and Energy Analysis</h2>
<div class="outline-text-2" id="text-5">
</div>
<div id="outline-container-Compute%20Time%20~%20120%20P%5E2%20%2F%20R" class="outline-3">
<h3 id="Compute%20Time%20~%20120%20P%5E2%20%2F%20R"><span class="section-number-3">5.1.</span> Compute Time ~ 120 P<sup>2</sup> / R</h3>
<div class="outline-text-3" id="text-5-1">
\begin{equation*}
t = T_{FLOPS} / R_{FLOPS}
\end{equation*}
\begin{equation*}
t \sim 120 \times P^2 / R_{FLOPS}
\end{equation*}

<ul class="org-ul">
<li>\(T_{FLOPS}\) is total FLOPS required</li>
<li>\(R_{FLOPS}\) is the FLOPS per second the system can do</li>
</ul>
</div>
<ol class="org-ol">
<li><a id="Token%3AParameters%20%3D%2020%3A1%20is%20Optimal"></a>Token:Parameters = 20:1 is Optimal<br />
<div class="outline-text-4" id="text-5-1-1">
<p>
The <a href="chinchilla_scaling_law.html#ID-275452F6-0C9A-4BF9-A8EC-47CF8B85935A">Chinchilla Scaling Law</a>
</p>

<p>
It is recommended to maintain a 20 to 1 ratio of tokens vs. number of parameters to be compute optimal (i.e. obtain best accuracy for total compute) [<a href="papers/Evaluation of Pre-training LLMs on Leadership Class Supercomputers.pdf#page=2">pg. 2</a>]
</p>
</div>
</li>
<li><a id="Compute%20scales%20with%20square%20of%20Parameters"></a>Compute scales with square of Parameters<br />
<div class="outline-text-4" id="text-5-1-2">
<p>
Optimal Token counts = 20 x Parameters count
</p>

<p>
Compute &sim; 6 &times; Parameter counts &times; Token counts [<a href="papers/Evaluation of Pre-training LLMs on Leadership Class Supercomputers.pdf#page=4">Eqn 1</a>]
</p>

<p>
Compute ~ 120 &times; (Parameter counts)<sup>2</sup> [<a href="papers/Evaluation of Pre-training LLMs on Leadership Class Supercomputers.pdf#page=9">Eqn 7</a>]
</p>

<blockquote>
<p>
The total number of compute operations needed for the optimal training (20 tokens per parameter) [16] of LLMs is quadratically proportional to the number of model parameters (P) [<a href="papers/Evaluation of Pre-training LLMs on Leadership Class Supercomputers.pdf#page=8">pg. 8</a>]
</p>
</blockquote>
</div>
</li>
</ol>
</div>
<div id="outline-container-Energy%20%3D%20t%20%5Ctimes%20R_watt" class="outline-3">
<h3 id="Energy%20%3D%20t%20%5Ctimes%20R_watt"><span class="section-number-3">5.2.</span> Energy = t &times; R<sub>watt</sub></h3>
<div class="outline-text-3" id="text-5-2">
\begin{equation*}
E = t \times R_{watt}
\end{equation*}

<ul class="org-ul">
<li>\(R_{watt}\) is the avg power usage</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-Evaluation" class="outline-2">
<h2 id="Evaluation"><span class="section-number-2">6.</span> Evaluation</h2>
<div class="outline-text-2" id="text-6">
<p>
Now we the experiments and check against theoritical models (equations)
</p>
</div>
<div id="outline-container-Scaling%20Analysis" class="outline-3">
<h3 id="Scaling%20Analysis"><span class="section-number-3">6.1.</span> Scaling Analysis</h3>
<div class="outline-text-3" id="text-6-1">
<p>
Before doing extensive experiments the author find the optimal hyperparameters of the System.
</p>

<p>
Summit (DeepSpeed-Megatron)
</p>
<ul class="org-ul">
<li>PP=1, TP=12, DP=N / (PP &times; TP) gave best performance [<a href="papers/Evaluation of Pre-training LLMs on Leadership Class Supercomputers.pdf#page=12">Section 4.2.2</a>]</li>
</ul>

<p>
Crusher (FSDP)
</p>
<ul class="org-ul">
<li>Prefetching didn't affect much [<a href="papers/Evaluation of Pre-training LLMs on Leadership Class Supercomputers.pdf#page=11">Fig 6</a>]</li>
<li>Scaling Efficiency is better in Crusher (97%) [<a href="papers/Evaluation of Pre-training LLMs on Leadership Class Supercomputers.pdf#page=13">Figure 8</a>]</li>
</ul>
</div>
</div>
<div id="outline-container-Memory%20%26%20Bandwidth" class="outline-3">
<h3 id="Memory%20%26%20Bandwidth"><span class="section-number-3">6.2.</span> Memory &amp; Bandwidth</h3>
<div class="outline-text-3" id="text-6-2">
<ul class="org-ul">
<li>Measurements of memory usage and bandwidth requirement are consistent with theoritically derived formulas</li>
</ul>

<blockquote>
<p>
Compared to our projections (Eqs. 3 and 6), the measurements are consistent, as shown in Fig. 7. [<a href="papers/Evaluation of Pre-training LLMs on Leadership Class Supercomputers.pdf#page=12">pg. 12</a>]
</p>
</blockquote>
</div>
</div>
<div id="outline-container-Power" class="outline-3">
<h3 id="Power"><span class="section-number-3">6.3.</span> Power</h3>
<div class="outline-text-3" id="text-6-3">
<p>
175B Model
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-right">Summit</th>
<th scope="col" class="org-right">Crusher</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Avg. Power (Watt)</td>
<td class="org-right">84</td>
<td class="org-right">408</td>
</tr>

<tr>
<td class="org-left">Efficiency (T<sub>flops</sub>/Watt)</td>
<td class="org-right">0.165</td>
<td class="org-right">0.235</td>
</tr>
</tbody>
</table>

<p>
1T Model
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-right">Summit</th>
<th scope="col" class="org-right">Crusher</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Avg. Power (Watt)</td>
<td class="org-right">66</td>
<td class="org-right">151</td>
</tr>
</tbody>
</table>

<ul class="org-ul">
<li>For 1T Model power consumption is low because more time was spent in communication [<a href="papers/Evaluation of Pre-training LLMs on Leadership Class Supercomputers.pdf#page=13">Page 13</a>]</li>
</ul>
</div>
</div>
<div id="outline-container-Train%20time%20and%20Energy%20Projections" class="outline-3">
<h3 id="Train%20time%20and%20Energy%20Projections"><span class="section-number-3">6.4.</span> Train time and Energy Projections</h3>
<div class="outline-text-3" id="text-6-4">
<p>
It is listed in [<a href="papers/Evaluation of Pre-training LLMs on Leadership Class Supercomputers.pdf#page=15">Table 2</a>]
</p>

<ul class="org-ul">
<li>The values for Frontier are extrapolated from the values for Crusher.</li>
<li>For 1T model values for 1x Data and 20x data is shown.</li>
</ul>


<div id="figure-8" class="figure">
<p><img src="data/evaluation_of_pre_training_llms_on_leadership_class_supercomputers/table_2-20241002114119.png" alt="table_2-20241002114119.png" />
</p>
<p><span class="figure-number">Figure 8: </span>Table 2</p>
</div>
</div>
<ol class="org-ol">
<li><a id="Crusher%20is%20more%20energy%20efficient"></a>Crusher is more energy efficient<br />
<div class="outline-text-4" id="text-6-4-1">
<blockquote>
<p>
In general, MI250X-based Crusher is more energy-efficient for training LLMs, and the advantage grows rapidly for model sizes beyond GPT3. [<a href="papers/Evaluation of Pre-training LLMs on Leadership Class Supercomputers.pdf#page=16">pg. 16</a>]
</p>
</blockquote>
</div>
</li>
<li><a id="LLM%20workload%20is%204x%20more%20efficient%20that%20traditional%20workloads"></a>LLM workload is 4x more efficient that traditional workloads<br />
<div class="outline-text-4" id="text-6-4-2">
<p>
<a href="papers/Evaluation of Pre-training LLMs on Leadership Class Supercomputers.pdf#page=16">pg. 16</a>
</p>

<blockquote>
<p>
We observe that these values for training LLMs are over 4× more power efficient than executing traditional simulation workloads (∼ 52 GFLOPS/Watt).
</p>
</blockquote>
</div>
</li>
</ol>
</div>
</div>
<div id="outline-container-Conclusions%20%28%26%20Limitations%29" class="outline-2">
<h2 id="Conclusions%20%28%26%20Limitations%29"><span class="section-number-2">7.</span> Conclusions (&amp; Limitations)</h2>
<div class="outline-text-2" id="text-7">
</div>
<div id="outline-container-Software%20improvements%20might%20change%20the%20projections" class="outline-3">
<h3 id="Software%20improvements%20might%20change%20the%20projections"><span class="section-number-3">7.1.</span> Software improvements might change the projections</h3>
<div class="outline-text-3" id="text-7-1">
<ul class="org-ul">
<li>Message compression</li>
<li>Model Pruning</li>
<li>Autotuning of parallel strategies</li>
</ul>

<blockquote>
<p>
It’s important to note that the field is rapidly evolving, with ongoing advancements that can further reduce communication costs. For instance, techniques such as message compression, model pruning, and autotuning of parallel strategies [38] have shown promise in significantly mitigating the network bandwidth requirements for scaling up LLM training. [<a href="papers/Evaluation of Pre-training LLMs on Leadership Class Supercomputers.pdf#page=16">pg. 16</a>]
</p>
</blockquote>
</div>
</div>
<div id="outline-container-Large%20per%20node%20memory%20and%20communication%20bandwidth%20required" class="outline-3">
<h3 id="Large%20per%20node%20memory%20and%20communication%20bandwidth%20required"><span class="section-number-3">7.2.</span> Large per node memory and communication bandwidth required</h3>
<div class="outline-text-3" id="text-7-2">
<ul class="org-ul">
<li>Crusher: 128 GB per GPU (2 GCU in 1 die/GPU) [Source: Pg. 17]</li>
<li>Summit : 32 GB per GPU [Source: Internet]</li>
</ul>

<blockquote>
<p>
Therefore, for LLM training, larger amounts of per node device memory and communication bandwidth lead to obtaining better performance, but the specific requirements depend on the model size. [<a href="papers/Evaluation of Pre-training LLMs on Leadership Class Supercomputers.pdf#page=16">pg. 16</a>]
</p>
</blockquote>
</div>
</div>
<div id="outline-container-Current%20Memory%20Bandwidth%20is%20limiting%20for%20Crusher" class="outline-3">
<h3 id="Current%20Memory%20Bandwidth%20is%20limiting%20for%20Crusher"><span class="section-number-3">7.3.</span> Current Memory Bandwidth is limiting for Crusher</h3>
<div class="outline-text-3" id="text-7-3">
<ul class="org-ul">
<li>For achieveable performance: 94 GB/s</li>
<li>Currently available: 25 GB/s</li>
</ul>

<blockquote>
<p>
For theoretical peak and achievable performance, the minimum per-device communication bandwidth needed is 37 and 94 GB/s, respectively. The current 25 GB/s per-device on Crusher is not sufficient to support linear scaling for training GPT 1T model [<a href="papers/Evaluation of Pre-training LLMs on Leadership Class Supercomputers.pdf#page=17">pg. 17</a>]
</p>
</blockquote>
</div>
</div>
<div id="outline-container-Frontier%20is%20promising%20for%20GPT3%20size%20model%20training" class="outline-3">
<h3 id="Frontier%20is%20promising%20for%20GPT3%20size%20model%20training"><span class="section-number-3">7.4.</span> Frontier is promising for GPT3 size model training</h3>
<div class="outline-text-3" id="text-7-4">
<p>
Crusher is same as Frontier but with less nodes.
</p>

<blockquote>
<p>
Feasibility analysis and practical guide in building foundation models for sciences. Our results indicate that Frontier is a promising platform for GPT3-size model training [<a href="papers/Evaluation of Pre-training LLMs on Leadership Class Supercomputers.pdf#page=18">pg. 18</a>]
</p>
</blockquote>
</div>
</div>
</div>
<div id="outline-container-Thank%20you%21" class="outline-2">
<h2 id="Thank%20you%21"><span class="section-number-2">8.</span> Thank you!</h2>
</div>
<div id="outline-container-Misc" class="outline-2">
<h2 id="Misc"><span class="section-number-2">9.</span> Misc</h2>
<div class="outline-text-2" id="text-9">
</div>
<div id="outline-container-DL%20scales%20with%20model%20and%20data" class="outline-3">
<h3 id="DL%20scales%20with%20model%20and%20data"><span class="section-number-3">9.1.</span> DL scales with model and data</h3>
<div class="outline-text-3" id="text-9-1">
<p>
[<a href="papers/Evaluation of Pre-training LLMs on Leadership Class Supercomputers.pdf#page=4">pg. 4</a>]
Deep Learning scales with Model size and data
</p>

<p>
Total number of FLOPs required for each training epoch is
</p>

\begin{equation*}
T_{FLOPS} \sim 6 \times P \times D
\end{equation*}
</div>
</div>
<div id="outline-container-Nickel" class="outline-3">
<h3 id="Nickel"><span class="section-number-3">9.2.</span> Nickel</h3>
<div class="outline-text-3" id="text-9-2">
<p>
See NVIDIA Collective Communication Library (NCCL, prononunce "Nickel") <a href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/operations.html">documentation</a> for meaning of Allreduce, Allgather, ReduceScatter, &#x2026;
</p>

<blockquote>
<p>
Compared to DDP, where only an Allreduce during each backward propa- gation is required, FSDP requires an Allgather for the forward pass and both an Allgather and a ReduceScatter for the backward pass. [<a href="papers/Evaluation of Pre-training LLMs on Leadership Class Supercomputers.pdf#page=6">pg. 6</a>]
</p>
</blockquote>
</div>
</div>
<div id="outline-container-Mistakes" class="outline-3">
<h3 id="Mistakes"><span class="section-number-3">9.3.</span> Mistakes</h3>
<div class="outline-text-3" id="text-9-3">
<ul class="org-ul">
<li><a href="papers/Evaluation of Pre-training LLMs on Leadership Class Supercomputers.pdf#page=5">Page 5</a> ZeRO optimzation steps is wrongly ordered. See original ZeRO paper for correct order.</li>
</ul>
</div>
</div>
<div id="outline-container-Model%20Parallel%20%20%3D%20PP%20or%20TP" class="outline-3">
<h3 id="Model%20Parallel%20%20%3D%20PP%20or%20TP"><span class="section-number-3">9.4.</span> Model Parallel = PP or TP</h3>
<div class="outline-text-3" id="text-9-4">
<p>
Model Parallel = Pipeline parallel or Tensor Parallel i.e. splitting the model [<a href="https://huggingface.co/docs/transformers/v4.13.0/en/parallelism">https://huggingface.co/docs/transformers/v4.13.0/en/parallelism</a>]
</p>
</div>
</div>
<div id="outline-container-FSDP%20%3D%20ZeRO%20Stage%203" class="outline-3">
<h3 id="FSDP%20%3D%20ZeRO%20Stage%203"><span class="section-number-3">9.5.</span> FSDP = ZeRO Stage 3</h3>
<div class="outline-text-3" id="text-9-5">
<ul class="org-ul">
<li>FSDP and ZeRO stage 3 is the same [<a href="https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/scaling/JAX/data_parallel_fsdp.html#Parameter-Sharding">https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/scaling/JAX/data_parallel_fsdp.html#Parameter-Sharding</a>]</li>
</ul>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<hr/>You can send your feedback, queries <a href="mailto:bpanthi977@gmail.com?subject=Feedback: Evaluation of Pre-training LLMs on Supercomputers">here</a><span id="visits"></span><span id="claps"></span><div id="claps-message"></div>
</div>
</body>
</html>
