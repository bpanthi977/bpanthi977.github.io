<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Vanishing Gradient</title>
<meta name="author" content="Bibek Panthi" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="../css/stylesheet.css" />
<link rel="stylesheet" type="text/css" href="../css/braindump.css" />
<script src="../js/counters.js" type="text/javascript"></script>
<script src="../js/URI.js" type="text/javascript"></script>
<script src="../js/pages.js" type="text/javascript"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="H" href="./index.html"> HOME </a>
</div><div id="preamble" class="status">
<p class="date">Date: <span class="timestamp-wrapper"><span class="timestamp">[2023-03-18 Sat]</span></span></p>
</div>
<div id="content" class="content">
<h1 class="title">Vanishing Gradient</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#Solution%20to%20Vanishing%20Gradient%20Problem">1. Solution to Vanishing Gradient Problem</a>
<ul>
<li><a href="#Trick%201%3A%20Activation%20Function%20-%20ReLU%20%28has%20derivative%20%3D%201%20or%200%29">1.1. Trick 1: Activation Function - ReLU (has derivative = 1 or 0)</a></li>
<li><a href="#Trick%202%3A%20Initialized%20the%20weights%20to%20identity%20matrix%20and%20Bias%20to%20zero%20to%20prevent%20rapid%20shrinking">1.2. Trick 2: Initialized the weights to identity matrix and Bias to zero to prevent rapid shrinking</a></li>
<li><a href="#Trick%203%3A%20Gated%20Cells%20%28LSTM%2C%20GRU%2C%20etc%29-%20Best">1.3. Trick 3: Gated Cells (LSTM, GRU, etc)- Best</a></li>
</ul>
</li>
<li><a href="#Vanishing%20Gradients%20in%20Deep%20Neural%20Networks">2. Vanishing Gradients in Deep Neural Networks</a>
<ul>
<li><a href="#Residual%20Connection">2.1. Residual Connection</a></li>
</ul>
</li>
</ul>
</div>
</div>
<p>
If the gradients are &lt; 1, then as gradients are backpropagated the gradients decrease to near zero (vanishing gradient). Vanishing Gradient cause the model to focus on short term dependencies and ignore long term dependencies. 
</p>


<div id="figure-1" class="figure">
<p><img src="./data/Recurrent neural network/mpv-screenshot1ey2dT.png" alt="mpv-screenshot1ey2dT.png" />
</p>
</div>
<div id="outline-container-Solution%20to%20Vanishing%20Gradient%20Problem" class="outline-2">
<h2 id="Solution%20to%20Vanishing%20Gradient%20Problem"><span class="section-number-2">1.</span> Solution to Vanishing Gradient Problem</h2>
<div class="outline-text-2" id="text-1">
<p>
See <a href="recurrent_neural_network_6_s191_2020.html#ID-A0A5D7DB-A9A8-4227-9F9B-38762D4A25AF">Recurrent Neural Network - MIT 6.S191 2020</a>, <a href="mit_6_s191_introduction_to_deep_learning.html#ID-B81B32C3-D182-4BDC-9235-8079D7C250CB">RNN and Transformers (MIT 6.S191 2022)</a> for links to video lecture.
</p>
</div>
<div id="outline-container-Trick%201%3A%20Activation%20Function%20-%20ReLU%20%28has%20derivative%20%3D%201%20or%200%29" class="outline-3">
<h3 id="Trick%201%3A%20Activation%20Function%20-%20ReLU%20%28has%20derivative%20%3D%201%20or%200%29"><span class="section-number-3">1.1.</span> Trick 1: Activation Function - ReLU (has derivative = 1 or 0)</h3>
<div class="outline-text-3" id="text-1-1">

<div id="figure-2" class="figure">
<p><img src="./data/Recurrent neural network/mpv-screenshotSyWU5h.png" alt="mpv-screenshotSyWU5h.png" />
</p>
</div>
</div>
</div>
<div id="outline-container-Trick%202%3A%20Initialized%20the%20weights%20to%20identity%20matrix%20and%20Bias%20to%20zero%20to%20prevent%20rapid%20shrinking" class="outline-3">
<h3 id="Trick%202%3A%20Initialized%20the%20weights%20to%20identity%20matrix%20and%20Bias%20to%20zero%20to%20prevent%20rapid%20shrinking"><span class="section-number-3">1.2.</span> Trick 2: Initialized the weights to identity matrix and Bias to zero to prevent rapid shrinking</h3>
</div>
<div id="outline-container-Trick%203%3A%20Gated%20Cells%20%28LSTM%2C%20GRU%2C%20etc%29-%20Best" class="outline-3">
<h3 id="Trick%203%3A%20Gated%20Cells%20%28LSTM%2C%20GRU%2C%20etc%29-%20Best"><span class="section-number-3">1.3.</span> Trick 3: Gated Cells (LSTM, GRU, etc)- Best</h3>
<div class="outline-text-3" id="text-1-3">
<p>
Use a more complex recurrent unit with gates to control what information is passed through.
</p>
</div>
</div>
</div>
<div id="outline-container-Vanishing%20Gradients%20in%20Deep%20Neural%20Networks" class="outline-2">
<h2 id="Vanishing%20Gradients%20in%20Deep%20Neural%20Networks"><span class="section-number-2">2.</span> Vanishing Gradients in Deep Neural Networks</h2>
<div class="outline-text-2" id="text-2">
<p>
(pg. 252 <a href="deep_learning_with_python_francois_chollet.html#ID-857CAF91-5AFC-4F9E-8BD6-0A1804942B7B">Deep Learning with Python - François Chollet</a>)
In deep networks the noise at each layer can overwhelm the gradient information and backpropagation can stop working.
</p>

<ul class="org-ul">
<li>Each successive function in the chain introduces some amount of noise.</li>
<li>This noise starts overwhelming gradient information, If the function chain is too deep,</li>
<li>and backpropagation stops working.</li>
</ul>

<p>
Your model won’t train at all. This is the vanishing gradients problem.
</p>
</div>
<div id="outline-container-Residual%20Connection" class="outline-3">
<h3 id="Residual%20Connection"><span class="section-number-3">2.1.</span> Residual Connection</h3>
<div class="outline-text-3" id="text-2-1">
<p>
Residual connection acts as an information shortcut around destructive or noisy blocks (such as blocks that contain relu activations or dropout layers), enabling error gradient infor- mation from early layers to propagate noiselessly through a deep network.
</p>

<hr />
<h3>Backlinks</h3>

<ul class="org-ul">
<li><a href="recurrent_neural_network_6_s191_2020.html#ID-A0A5D7DB-A9A8-4227-9F9B-38762D4A25AF">Recurrent Neural Network - 6.S191 2020</a></li>
<li><a href="mit_6_s191_introduction_to_deep_learning.html#ID-B81B32C3-D182-4BDC-9235-8079D7C250CB">RNN and Transformers (MIT 6.S191 2022)</a></li>
<li><a href="Recurrent neural network.html#ID-62e97a52-1804-4948-91e4-bede2027d3d5">Recurrent neural network</a></li>
</ul>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<hr/>You can send your feedback, queries <a href="mailto:bpanthi977@gmail.com?subject=Feedback: Vanishing Gradient">here</a><span id="visits"></span><span id="claps"></span><div id="claps-message"></div>
</div>
</body>
</html>
