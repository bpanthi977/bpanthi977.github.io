<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>AI Deception</title>
<meta name="author" content="Bibek Panthi" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="../css/stylesheet.css" />
<link rel="stylesheet" type="text/css" href="../css/braindump.css" />
<script src="../js/counters.js" type="text/javascript"></script>
<script src="../js/URI.js" type="text/javascript"></script>
<script src="../js/pages.js" type="text/javascript"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="H" href="./index.html"> HOME </a>
</div><div id="preamble" class="status">
<p class="date">Date: <span class="timestamp-wrapper"><span class="timestamp">[2023-03-18 Sat]</span></span></p>
</div>
<div id="content" class="content">
<h1 class="title">AI Deception</h1>
<p>
<a href="https://youtu.be/bJLcIBixGj8?t=1183">The OTHER AI Alignment Problem: Mesa-Optimizers and Inner Alignment</a>: Sophisticated Agents will try to protect their goals from being modified because if they get new goals, they are unlikely to achieve their current goals. So, for a misaligned <a href="mesa_optimizer.html#ID-A0F7B38D-A67E-4C07-9C56-FFD601E592D5">mesa optimizer</a> the optimal behavior is deception.
  This (<a href="https://colab.research.google.com/drive/1286r553N8drh6-VeZjZA1vbUBY9Z1fps?usp=sharing">colab</a>) currently works because @karpathy  shows both eval mode (<a href="dropout.html#ID-E34661FC-DC0C-4131-994C-9B7B1EABF34F">dropout</a> off) and train mode (dropout on) to the model during training. Doesn't seem like it could be exploited yet. Or, can it? [<a href="https://twitter.com/bpanthi977/status/1636932181078720513">Tweet</a>]
</p>

<p>
<a href="https://twitter.com/karpathy/status/1635049541534879745">Tweet</a> by @karpathy:
</p>
<blockquote>
<p>
Dropout layers in a <a href="transformer_architecture.html#ID-F41C46C0-E4D5-4DEF-9D54-A9078268D4B4">Transformer</a> leak the phase bit (train/eval) - small example. So an LLM may be able to determine if it is being trained and if backward pass follows&#x2026;.
</p>
</blockquote>

<hr />
<h3>Backlinks</h3>

<ul class="org-ul">
<li><a href="artificial_intelligence.html#ID-2674491f-a645-4499-996e-af04db2be74d">Artificial Intelligence</a></li>
</ul>
</div>
<div id="postamble" class="status">
<hr/>You can send your feedback, queries <a href="mailto:bpanthi977@gmail.com?subject=Feedback: AI Deception">here</a><span id="visits"></span><span id="claps"></span><div id="claps-message"></div>
</div>
</body>
</html>
