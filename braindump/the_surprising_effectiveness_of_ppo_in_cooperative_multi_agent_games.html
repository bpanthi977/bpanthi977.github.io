<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games</title>
<meta name="author" content="Bibek Panthi" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="../css/stylesheet.css" />
<link rel="stylesheet" type="text/css" href="../css/braindump.css" />
<script src="../js/counters.js" type="text/javascript"></script>
<script src="../js/URI.js" type="text/javascript"></script>
<script src="../js/pages.js" type="text/javascript"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="./index.html"> UP </a>
 |
 <a accesskey="H" href="../index.html"> HOME </a>
</div><div id="preamble" class="status">
<p class="date">Date: <span class="timestamp-wrapper"><span class="timestamp">&lt;2024-12-15 Sun&gt;</span></span></p>
</div>
<div id="content" class="content">
<h1 class="title">The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games</h1>
<p>
[<a href="papers/The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games - 2103.01955v4.pdf#page=nil">pdf</a>][<a href="https://arxiv.org/abs/2103.01955">arXiv</a>]
</p>

<p>
by Chao Yu, &#x2026;, <a href="private/eugene_vinitsky.html#ID-C4AFE654-AF46-4C34-97C2-656AA111312B">Eugene Vinitsky</a>, et. al
</p>

<ul class="org-ul">
<li>Paper doesn't propose new algorithm, but provides empirical evidence on usefullness of <a href="proximal_policy_optimization.html#ID-57363C2D-5D41-4345-81D8-4EAE3D286126">PPO</a> in MARL</li>
<li>PPO is overlooked in MARL but people think it would be sample inefficient</li>
<li>But authors found that it was sample efficient as wells as produced good results</li>
<li>They use usual modifications to PPO like using GAE (<a href="generalized_advantage_estimation.html#ID-E9F7CD89-1DE6-494D-B466-2CF15BC97776">Generalized Advantage Estimation</a>) and use modification specific to MARL like <a href="papers/The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games - 2103.01955v4.pdf#page=14">Death Masking (pg. 14)</a></li>
<li>As limitation: the experiments were only done on envrionments with [<a href="papers/The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games - 2103.01955v4.pdf#page=10">Page 10</a>]
<ul class="org-ul">
<li>discrete action space</li>
<li>collaborative problems</li>
<li>homogeneous agents</li>
</ul></li>
</ul>

<hr />
<h3>Backlinks</h3>

<ul class="org-ul">
<li><a href="reinforcement_learning.html#ID-B010228E-5555-4D07-8E63-B54E476A249E">Reinforcement Learning</a></li>
</ul>
</div>
<div id="postamble" class="status">
<hr/>You can send your feedback, queries <a href="mailto:bpanthi977@gmail.com?subject=Feedback: The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games">here</a><span id="visits"></span><span id="claps"></span><div id="claps-message"></div><a href="https://bpanthi977.github.io/braindump/data/rss.xml"><img src="https://bpanthi977.github.io/braindump/data/rss.png" /></a>
</div>
</body>
</html>
