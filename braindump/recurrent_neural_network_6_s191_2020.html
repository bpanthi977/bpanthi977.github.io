<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Recurrent Neural Network - 6.S191 2020</title>
<meta name="author" content="Bibek Panthi" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="../css/stylesheet.css" />
<link rel="stylesheet" type="text/css" href="../css/braindump.css" />
<script src="../js/counters.js" type="text/javascript"></script>
<script src="../js/URI.js" type="text/javascript"></script>
<script src="../js/pages.js" type="text/javascript"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="./index.html"> UP </a>
 |
 <a accesskey="H" href="../index.html"> HOME </a>
</div><div id="preamble" class="status">
<p class="date">Date: <span class="timestamp-wrapper"><span class="timestamp">[2023-03-19 Sun]</span></span></p>
</div>
<div id="content" class="content">
<h1 class="title">Recurrent Neural Network - 6.S191 2020</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#Sequence%20Modelling">1. Sequence Modelling</a>
<ul>
<li><a href="#Using%20a%20Fixed%20Window%20won%27t%20work%20because%20long%20term%20dependencies%20wont%27%20work">1.1. Using a Fixed Window won't work because long term dependencies wont' work</a></li>
<li><a href="#Use%20Entire%20Sequence%20as%20Set%20of%20Counts%20-%20Bag%20of%20Words">1.2. Use Entire Sequence as Set of Counts - Bag of Words</a></li>
<li><a href="#Use%20a%20REALLY%20Big%20Fixed%20Window">1.3. Use a REALLY Big Fixed Window</a></li>
</ul>
</li>
<li><a href="#Sequence%20Modeling%3A%20Design%20Criteria">2. Sequence Modeling: Design Criteria</a></li>
<li><a href="#Recurrent%20Neural%20Networks%20for%20Sequence%20Modeling">3. Recurrent Neural Networks for Sequence Modeling</a>
<ul>
<li><a href="#RNN%20State%20Update%20and%20Output">3.1. RNN State Update and Output</a></li>
</ul>
</li>
<li><a href="#Backpropagation%20Through%20time">4. Backpropagation Through time</a>
<ul>
<li><a href="#The%20Problem%20of%20Long-Term%20Dependencies%3A%20Vanishing%20Gradient">4.1. The Problem of Long-Term Dependencies: Vanishing Gradient</a>
<ul>
<li><a href="#Trick%201%3A%20Activation%20Function%20-%20ReLU%20%28has%20derivative%20%3D%201%20or%200%29">4.1.1. Trick 1: Activation Function - ReLU (has derivative = 1 or 0)</a></li>
<li><a href="#Trick%202%3A%20Initialized%20the%20weights%20to%20identity%20matrix%20and%20Bias%20to%20zero">4.1.2. Trick 2: Initialized the weights to identity matrix and Bias to zero</a></li>
<li><a href="#Trick%203%3A%20Gated%20Cells%20%28LSTM%2C%20GRU%2C%20etc%29-%20Best">4.1.3. Trick 3: Gated Cells (LSTM, GRU, etc)- Best</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#--LSTMs">5. LSTMs</a>
<ul>
<li><a href="#Standard%20RNN">5.1. Standard RNN</a></li>
<li><a href="#LSTMs">5.2. LSTMs</a></li>
<li><a href="#Information%20is%20added%20or%20removed%20through%20structures%20called%20gates%20-%20sigmoid%20neural%20net%20layer%20and%20pointwise%20multiplication">5.3. Information is added or removed through structures called gates - sigmoid neural net layer and pointwise multiplication</a></li>
<li><a href="#Forget%20Store%20Update%20Output">5.4. Forget Store Update Output</a>
<ul>
<li><a href="#Forget">5.4.1. Forget</a></li>
<li><a href="#Store">5.4.2. Store</a></li>
<li><a href="#Update">5.4.3. Update</a></li>
<li><a href="#Output">5.4.4. Output</a></li>
</ul>
</li>
<li><a href="#Uninterrupted%20flow%20of%20gradient%20throught%20cell%20state">5.5. Uninterrupted flow of gradient throught cell state</a></li>
<li><a href="#LSTMs%3A%20Key%20Concepts">5.6. LSTMs: Key Concepts</a></li>
</ul>
</li>
</ul>
</div>
</div>
<p>
<p style="text-align:center; width:100%"><iframe width="560" height="315" src="https://www.youtube.com/embed/SEnXr6v2ifU" title="nil" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
</p>
<div id="outline-container-Sequence%20Modelling" class="outline-2">
<h2 id="Sequence%20Modelling"><span class="section-number-2">1.</span> Sequence Modelling</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-Using%20a%20Fixed%20Window%20won%27t%20work%20because%20long%20term%20dependencies%20wont%27%20work" class="outline-3">
<h3 id="Using%20a%20Fixed%20Window%20won%27t%20work%20because%20long%20term%20dependencies%20wont%27%20work"><span class="section-number-3">1.1.</span> Using a Fixed Window won't work because long term dependencies wont' work</h3>
<div class="outline-text-3" id="text-1-1">
</div>
</div>
<div id="outline-container-Use%20Entire%20Sequence%20as%20Set%20of%20Counts%20-%20Bag%20of%20Words" class="outline-3">
<h3 id="Use%20Entire%20Sequence%20as%20Set%20of%20Counts%20-%20Bag%20of%20Words"><span class="section-number-3">1.2.</span> Use Entire Sequence as Set of Counts - Bag of Words</h3>
<div class="outline-text-3" id="text-1-2">
<p>
But they don't preserve order 
</p>
</div>
</div>
<div id="outline-container-Use%20a%20REALLY%20Big%20Fixed%20Window" class="outline-3">
<h3 id="Use%20a%20REALLY%20Big%20Fixed%20Window"><span class="section-number-3">1.3.</span> Use a REALLY Big Fixed Window</h3>
<div class="outline-text-3" id="text-1-3">

<div id="figure-1" class="figure">
<p><img src="./data/Recurrent neural network/mpv-screenshot0JVeei.png" alt="mpv-screenshot0JVeei.png" />
</p>
</div>

<p>
Occurence of parameters in diffrent position requirer retraining - sortof 
</p>
</div>
</div>
</div>
<div id="outline-container-Sequence%20Modeling%3A%20Design%20Criteria" class="outline-2">
<h2 id="Sequence%20Modeling%3A%20Design%20Criteria"><span class="section-number-2">2.</span> Sequence Modeling: Design Criteria</h2>
<div class="outline-text-2" id="text-2">
<p>
To model sequences, we need to:
</p>

<ol class="org-ol">
<li>Handle variable-length sequences rq</li>
<li>Track long-term dependencies RNN</li>
<li>Maintain information about order</li>
<li>Share parameters across the sequence</li>
</ol>
</div>
</div>
<div id="outline-container-Recurrent%20Neural%20Networks%20for%20Sequence%20Modeling" class="outline-2">
<h2 id="Recurrent%20Neural%20Networks%20for%20Sequence%20Modeling"><span class="section-number-2">3.</span> Recurrent Neural Networks for Sequence Modeling</h2>
<div class="outline-text-2" id="text-3">

<div id="figure-2" class="figure">
<p><img src="./data/Recurrent neural network/mpv-screenshotN68cMG.png" alt="mpv-screenshotN68cMG.png" />
</p>
</div>

<p>
Recurrent because information is being passes within the cell through time 
</p>

<p>
RNN maintain a internal state \(h_t\) 
which is updated based on previous state and current input 
\(h_t = f_w(h_{t-1}, x_t)\) 
and here same set of functions and same set of parameters are used i.e. \(f_w\)  is remains same in each time step and it is what we learn 
</p>
</div>
<div id="outline-container-RNN%20State%20Update%20and%20Output" class="outline-3">
<h3 id="RNN%20State%20Update%20and%20Output"><span class="section-number-3">3.1.</span> RNN State Update and Output</h3>
<div class="outline-text-3" id="text-3-1">

<div id="figure-3" class="figure">
<p><img src="./data/Recurrent neural network/mpv-screenshot4gIooJ.png" alt="mpv-screenshot4gIooJ.png" />
</p>
</div>

<p>
There are three weight matrices \(W_{hh}\) , \(W_{xh}\) , \(W_{hy}\) 
</p>
</div>
</div>
</div>
<div id="outline-container-Backpropagation%20Through%20time" class="outline-2">
<h2 id="Backpropagation%20Through%20time"><span class="section-number-2">4.</span> Backpropagation Through time</h2>
<div class="outline-text-2" id="text-4">
</div>
<div id="outline-container-The%20Problem%20of%20Long-Term%20Dependencies%3A%20Vanishing%20Gradient" class="outline-3">
<h3 id="The%20Problem%20of%20Long-Term%20Dependencies%3A%20Vanishing%20Gradient"><span class="section-number-3">4.1.</span> The Problem of Long-Term Dependencies: Vanishing Gradient</h3>
<div class="outline-text-3" id="text-4-1">
<ul class="org-ul">
<li><a href="vanishing_gradient.html#ID-2880697B-C48B-40C2-B25D-CEA991940A8A">Vanishing Gradient</a></li>
</ul>

<div id="figure-4" class="figure">
<p><img src="./data/Recurrent neural network/mpv-screenshot1ey2dT.png" alt="mpv-screenshot1ey2dT.png" />
</p>
</div>
</div>
<div id="outline-container-Trick%201%3A%20Activation%20Function%20-%20ReLU%20%28has%20derivative%20%3D%201%20or%200%29" class="outline-4">
<h4 id="Trick%201%3A%20Activation%20Function%20-%20ReLU%20%28has%20derivative%20%3D%201%20or%200%29"><span class="section-number-4">4.1.1.</span> Trick 1: Activation Function - ReLU (has derivative = 1 or 0)</h4>
<div class="outline-text-4" id="text-4-1-1">

<div id="figure-5" class="figure">
<p><img src="./data/Recurrent neural network/mpv-screenshotSyWU5h.png" alt="mpv-screenshotSyWU5h.png" />
</p>
</div>
</div>
</div>
<div id="outline-container-Trick%202%3A%20Initialized%20the%20weights%20to%20identity%20matrix%20and%20Bias%20to%20zero" class="outline-4">
<h4 id="Trick%202%3A%20Initialized%20the%20weights%20to%20identity%20matrix%20and%20Bias%20to%20zero"><span class="section-number-4">4.1.2.</span> Trick 2: Initialized the weights to identity matrix and Bias to zero</h4>
<div class="outline-text-4" id="text-4-1-2">
</div>
</div>
<div id="outline-container-Trick%203%3A%20Gated%20Cells%20%28LSTM%2C%20GRU%2C%20etc%29-%20Best" class="outline-4">
<h4 id="Trick%203%3A%20Gated%20Cells%20%28LSTM%2C%20GRU%2C%20etc%29-%20Best"><span class="section-number-4">4.1.3.</span> Trick 3: Gated Cells (LSTM, GRU, etc)- Best</h4>
<div class="outline-text-4" id="text-4-1-3">
<p>
Use a more complex recurrent unit with gates to control what information is passed through.
</p>
</div>
</div>
</div>
</div>
<div id="outline-container---LSTMs" class="outline-2">
<h2 id="--LSTMs"><span class="section-number-2">5.</span> LSTMs</h2>
<div class="outline-text-2" id="text-5">
<p>
Links: <a href="lstm.html#ID-5A05C89D-F75E-4B28-B8A7-3A68D1B6C5CA">LSTM</a>.
</p>
</div>
<div id="outline-container-Standard%20RNN" class="outline-3">
<h3 id="Standard%20RNN"><span class="section-number-3">5.1.</span> Standard RNN</h3>
<div class="outline-text-3" id="text-5-1">

<div id="figure-6" class="figure">
<p><img src="./data/Recurrent neural network/mpv-screenshotkCtu5q.png" alt="mpv-screenshotkCtu5q.png" />
</p>
</div>
</div>
</div>
<div id="outline-container-LSTMs" class="outline-3">
<h3 id="LSTMs"><span class="section-number-3">5.2.</span> LSTMs</h3>
<div class="outline-text-3" id="text-5-2">

<div id="figure-7" class="figure">
<p><img src="./data/Recurrent neural network/mpv-screenshot4K4faK.png" alt="mpv-screenshot4K4faK.png" />
</p>
</div>
</div>
</div>
<div id="outline-container-Information%20is%20added%20or%20removed%20through%20structures%20called%20gates%20-%20sigmoid%20neural%20net%20layer%20and%20pointwise%20multiplication" class="outline-3">
<h3 id="Information%20is%20added%20or%20removed%20through%20structures%20called%20gates%20-%20sigmoid%20neural%20net%20layer%20and%20pointwise%20multiplication"><span class="section-number-3">5.3.</span> Information is added or removed through structures called gates - sigmoid neural net layer and pointwise multiplication</h3>
<div class="outline-text-3" id="text-5-3">
</div>
</div>
<div id="outline-container-Forget%20Store%20Update%20Output" class="outline-3">
<h3 id="Forget%20Store%20Update%20Output"><span class="section-number-3">5.4.</span> Forget Store Update Output</h3>
<div class="outline-text-3" id="text-5-4">
</div>
<div id="outline-container-Forget" class="outline-4">
<h4 id="Forget"><span class="section-number-4">5.4.1.</span> Forget</h4>
<div class="outline-text-4" id="text-5-4-1">
<p>
Decide what information is going to be thrown state depending on \(h_{t-1}\) and input \(x_t\) 
<img src="./data/Recurrent neural network/mpv-screenshotwa7mQC.png" alt="mpv-screenshotwa7mQC.png" />
</p>
</div>
</div>
<div id="outline-container-Store" class="outline-4">
<h4 id="Store"><span class="section-number-4">5.4.2.</span> Store</h4>
<div class="outline-text-4" id="text-5-4-2">
<p>
Decide what part of new information is important and store that to cell state 
<img src="./data/Recurrent neural network/mpv-screenshotsJIKH0.png" alt="mpv-screenshotsJIKH0.png" />
</p>
</div>
</div>
<div id="outline-container-Update" class="outline-4">
<h4 id="Update"><span class="section-number-4">5.4.3.</span> Update</h4>
<div class="outline-text-4" id="text-5-4-3">
<p>
Use the relevant part of prior information and current state to selectively update the cell state values
<img src="./data/Recurrent neural network/mpv-screenshotrQkDTz.png" alt="mpv-screenshotrQkDTz.png" />
</p>
</div>
</div>
<div id="outline-container-Output" class="outline-4">
<h4 id="Output"><span class="section-number-4">5.4.4.</span> Output</h4>
<div class="outline-text-4" id="text-5-4-4">
<p>
What info stored in cell state is used to compute the hidden state to carry over to next time step
</p>


<div id="figure-8" class="figure">
<p><img src="./data/Recurrent neural network/mpv-screenshotblSzjU.png" alt="mpv-screenshotblSzjU.png" />
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-Uninterrupted%20flow%20of%20gradient%20throught%20cell%20state" class="outline-3">
<h3 id="Uninterrupted%20flow%20of%20gradient%20throught%20cell%20state"><span class="section-number-3">5.5.</span> Uninterrupted flow of gradient throught cell state</h3>
<div class="outline-text-3" id="text-5-5">

<div id="figure-9" class="figure">
<p><img src="./data/Recurrent neural network/mpv-screenshota0DSl0.png" alt="mpv-screenshota0DSl0.png" />
</p>
</div>
</div>
</div>
<div id="outline-container-LSTMs%3A%20Key%20Concepts" class="outline-3">
<h3 id="LSTMs%3A%20Key%20Concepts"><span class="section-number-3">5.6.</span> LSTMs: Key Concepts</h3>
<div class="outline-text-3" id="text-5-6">
<ol class="org-ol">
<li>Maintain a separate cell state from what is outputted</li>
<li>Use gates to control the flow of information
<ul class="org-ul">
<li>Forget gate gets rid of irrelevant information</li>
<li>Store relevant information from current input</li>
<li>Selectively update cell state</li>
<li>Output gate returns a filtered version of the cell state</li>
</ul></li>
<li>Backpropagation through time with uninterrupted gradient flow</li>
</ol>

<hr />
<h3>Backlinks</h3>

<ul class="org-ul">
<li><a href="vanishing_gradient.html#ID-2880697B-C48B-40C2-B25D-CEA991940A8A">Vanishing Gradient</a></li>
<li><a href="lstm.html#ID-5A05C89D-F75E-4B28-B8A7-3A68D1B6C5CA">LSTM</a></li>
<li><a href="Recurrent neural network.html#ID-62e97a52-1804-4948-91e4-bede2027d3d5">Recurrent neural network</a></li>
</ul>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<hr/>You can send your feedback, queries <a href="mailto:bpanthi977@gmail.com?subject=Feedback: Recurrent Neural Network - 6.S191 2020">here</a><span id="visits"></span><span id="claps"></span><div id="claps-message"></div><a href="https://bpanthi977.github.io/braindump/data/rss.xml"><img src="https://bpanthi977.github.io/braindump/data/rss.png" /></a>
</div>
</body>
</html>
