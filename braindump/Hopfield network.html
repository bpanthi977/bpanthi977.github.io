<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Hopfield network</title>
<meta name="author" content="Bibek Panthi" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="../css/stylesheet.css" />
<link rel="stylesheet" type="text/css" href="../css/braindump.css" />
<script src="../js/counters.js" type="text/javascript"></script>
<script src="../js/URI.js" type="text/javascript"></script>
<script src="../js/pages.js" type="text/javascript"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="H" href="./index.html"> HOME </a>
</div><div id="preamble" class="status">
<p class="date">Date: <span class="timestamp-wrapper"><span class="timestamp">[2020-11-15 Sun]</span></span></p>
</div>
<div id="content" class="content">
<h1 class="title">Hopfield network</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#Hopfield%20network">1. Hopfield network</a>
<ul>
<li><a href="#Contents">1.1. Contents</a></li>
<li><a href="#Origins">1.2. Origins</a></li>
<li><a href="#Structure">1.3. Structure</a></li>
<li><a href="#Updating">1.4. Updating</a>
<ul>
<li><a href="#Neurons%20%22attract%20or%20repel%20each%20other%22%20in%20state%20space">1.4.1. Neurons "attract or repel each other" in state space</a></li>
</ul>
</li>
<li><a href="#Working%20principles%20of%20discrete%20and%20continuous%20Hopfield%20networks">1.5. Working principles of discrete and continuous Hopfield networks</a></li>
<li><a href="#Energy">1.6. Energy</a></li>
<li><a href="#Hopfield%20network%20in%20optimization">1.7. Hopfield network in optimization</a></li>
<li><a href="#Initialization%20and%20running">1.8. Initialization and running</a></li>
<li><a href="#Training">1.9. Training</a>
<ul>
<li><a href="#Learning%20rules">1.9.1. Learning rules</a></li>
<li><a href="#%5B%5Bid%3A73d5bbb3-ded9-4f7e-846b-da4a069a9ab7%5D%5BHebbian%5D%5D%20learning%20rule%20for%20Hopfield%20networks">1.9.2. Hebbian learning rule for Hopfield networks</a></li>
<li><a href="#The%20Storkey%20learning%20rule">1.9.3. The Storkey learning rule</a></li>
</ul>
</li>
<li><a href="#Spurious%20patterns">1.10. Spurious patterns</a></li>
<li><a href="#Capacity">1.11. Capacity</a></li>
<li><a href="#Human%20memory">1.12. Human memory</a></li>
</ul>
</li>
</ul>
</div>
</div>
<p>
:FILE: <a href="./data/Hopfield network/Hopfield network - Wikipedia (2020-11-15 4_13_16 PM).html">Saved file</a>
</p>
<div id="outline-container-Hopfield%20network" class="outline-2">
<h2 id="Hopfield%20network"><span class="section-number-2">1.</span> Hopfield network</h2>
<div class="outline-text-2" id="text-1">
<p>
A Hopfield network (or Ising model of a neural network or Ising–Lenz–Little model) is a form of recurrentartificial neural network popularized by John Hopfield in 1982, but described earlier by Little in 1974 based on Ernst Ising's work with Wilhelm Lenz.[1][2]
</p>

<p>
Hopfield networks also provide a model for understanding human memory [3][4]
</p>
</div>
<div id="outline-container-Contents" class="outline-3">
<h3 id="Contents"><span class="section-number-3">1.1.</span> Contents</h3>
</div>
<div id="outline-container-Origins" class="outline-3">
<h3 id="Origins"><span class="section-number-3">1.2.</span> Origins</h3>
</div>
<div id="outline-container-Structure" class="outline-3">
<h3 id="Structure"><span class="section-number-3">1.3.</span> Structure</h3>
</div>
<div id="outline-container-Updating" class="outline-3">
<h3 id="Updating"><span class="section-number-3">1.4.</span> Updating</h3>
<div class="outline-text-3" id="text-1-4">
</div>
<div id="outline-container-Neurons%20%22attract%20or%20repel%20each%20other%22%20in%20state%20space" class="outline-4">
<h4 id="Neurons%20%22attract%20or%20repel%20each%20other%22%20in%20state%20space"><span class="section-number-4">1.4.1.</span> Neurons "attract or repel each other" in state space</h4>
</div>
</div>
<div id="outline-container-Working%20principles%20of%20discrete%20and%20continuous%20Hopfield%20networks" class="outline-3">
<h3 id="Working%20principles%20of%20discrete%20and%20continuous%20Hopfield%20networks"><span class="section-number-3">1.5.</span> Working principles of discrete and continuous Hopfield networks</h3>
</div>
<div id="outline-container-Energy" class="outline-3">
<h3 id="Energy"><span class="section-number-3">1.6.</span> Energy</h3>
</div>
<div id="outline-container-Hopfield%20network%20in%20optimization" class="outline-3">
<h3 id="Hopfield%20network%20in%20optimization"><span class="section-number-3">1.7.</span> Hopfield network in optimization</h3>
</div>
<div id="outline-container-Initialization%20and%20running" class="outline-3">
<h3 id="Initialization%20and%20running"><span class="section-number-3">1.8.</span> Initialization and running</h3>
</div>
<div id="outline-container-Training" class="outline-3">
<h3 id="Training"><span class="section-number-3">1.9.</span> Training</h3>
<div class="outline-text-3" id="text-1-9">
</div>
<div id="outline-container-Learning%20rules" class="outline-4">
<h4 id="Learning%20rules"><span class="section-number-4">1.9.1.</span> Learning rules</h4>
</div>
<div id="outline-container-%5B%5Bid%3A73d5bbb3-ded9-4f7e-846b-da4a069a9ab7%5D%5BHebbian%5D%5D%20learning%20rule%20for%20Hopfield%20networks" class="outline-4">
<h4 id="%5B%5Bid%3A73d5bbb3-ded9-4f7e-846b-da4a069a9ab7%5D%5BHebbian%5D%5D%20learning%20rule%20for%20Hopfield%20networks"><span class="section-number-4">1.9.2.</span> <a href="hebbian_theory.html#ID-73d5bbb3-ded9-4f7e-846b-da4a069a9ab7">Hebbian</a> learning rule for Hopfield networks</h4>
</div>
<div id="outline-container-The%20Storkey%20learning%20rule" class="outline-4">
<h4 id="The%20Storkey%20learning%20rule"><span class="section-number-4">1.9.3.</span> The Storkey learning rule</h4>
</div>
</div>
<div id="outline-container-Spurious%20patterns" class="outline-3">
<h3 id="Spurious%20patterns"><span class="section-number-3">1.10.</span> Spurious patterns</h3>
</div>
<div id="outline-container-Capacity" class="outline-3">
<h3 id="Capacity"><span class="section-number-3">1.11.</span> Capacity</h3>
<div class="outline-text-3" id="text-1-11">
<p>
The Network capacity of the Hopfield network model is determined by neuron amounts and connections within a given network. 
</p>

<p>
Furthermore, it was shown that the recall accuracy between vectors and nodes was 0.138 (approximately 138 vectors can be recalled from storage for every 1000 nodes) (Hertz et al., 1991). Therefore, it is evident that many mistakes will occur if one tries to store a large number of vectors. When the Hopfield model does not recall the right pattern, it is possible that an intrusion has taken place, since semantically related items tend to confuse the individual, and recollection of the wrong pattern occurs.
</p>

<p>
Perfect recalls and high capacity, &gt;0.14, can be loaded in the network by Storkey learning method; ETAM
</p>

<p>
The storage capacity can be given as C ≅ n 2 log 2 ⁡ n {\displaystyle C&cong; {\frac {n}{2log _{2}n}}}  where n {\displaystyle n}  is the number of neurons in the net. Or approximately C ≊ 0.15 n {\displaystyle C\approxeq 0.15n} [18]
</p>
</div>
</div>
<div id="outline-container-Human%20memory" class="outline-3">
<h3 id="Human%20memory"><span class="section-number-3">1.12.</span> Human memory</h3>
<div class="outline-text-3" id="text-1-12">
<p>
The Hopfield model accounts for associativememory through the incorporation of memory vectors.
</p>

<p>
It is important to note that Hopfield's network model utilizes the same learning rule as Hebb's (1949) learning rule, which basically tried to show that learning occurs as a result of the strengthening of the weights by when activity is occurring.
</p>


<p>
Rizzuto and Kahana (2001) were able to show that the neural network model can account for repetition on recall accuracy by incorporating a probabilistic-learning algorithm. During the retrieval process, no learning occurs. As a result, the weights of the network remain fixed, showing that the model is able to switch from a learning stage to a recall stage. By adding contextual drift they were able to show the rapid forgetting that occurs in a Hopfield model during a cued-recall task. The entire network contributes to the change in the activation of any single node.
</p>

<hr />
<h3>References</h3>

<ul class="org-ul">
<li><a href="https://en.wikipedia.org/wiki/Hopfield_network">https://en.wikipedia.org/wiki/Hopfield_network</a></li>
</ul>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<hr/>You can send your feedback, queries <a href="mailto:bpanthi977@gmail.com?subject=Feedback: Hopfield network">here</a><span id="visits"></span><span id="claps"></span><div id="claps-message"></div>
</div>
</body>
</html>
