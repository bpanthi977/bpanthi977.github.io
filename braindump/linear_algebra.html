<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Linear Algebra via Exterior Products</title>
<meta name="author" content="Bibek Panthi" />
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<link rel="stylesheet" type="text/css" href="../css/stylesheet.css" />
<link rel="stylesheet" type="text/css" href="../css/braindump.css" />
<script src="../js/counters.js" type="text/javascript"></script>
<script src="../js/URI.js" type="text/javascript"></script>
<script src="../js/pages.js" type="text/javascript"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="./index.html"> UP </a>
 |
 <a accesskey="H" href="../index.html"> HOME </a>
</div><div id="preamble" class="status">
<p class="date">Date: <span class="timestamp-wrapper"><span class="timestamp">[2020-06-22 Mon]</span></span></p>
</div>
<div id="content" class="content">
<h1 class="title">Linear Algebra via Exterior Products</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#Homomorphism">1. Homomorphism</a></li>
<li><a href="#Vectors">2. Vectors</a>
<ul>
<li><a href="#Projectors%20and%20Subspaces">2.1. Projectors and Subspaces</a>
<ul>
<li><a href="#Properties">2.1.1. Properties</a></li>
</ul>
</li>
<li><a href="#Isomorphism">2.2. Isomorphism</a>
<ul>
<li><a href="#Canonical%20Isomorphism">2.2.1. Canonical Isomorphism</a></li>
</ul>
</li>
<li><a href="#Direct%20Sum%20of%20Vector%20Space">2.3. Direct Sum of Vector Space</a>
<ul>
<li><a href="#Canonical%20Projections%3B%20%24V%24%20and%20%24W%24%20as%20subspaces%20of%20subspaces%20of%20%24V%20%5Coplus%20W%24">2.3.1. Canonical Projections; \(V\) and \(W\) as subspaces of subspaces of \(V \oplus W\)</a></li>
</ul>
</li>
<li><a href="#Dual%20%28conjugate%29%20vector%20space">2.4. Dual (conjugate) vector space</a>
<ul>
<li><a href="#Example%202">2.4.1. Example 2</a></li>
<li><a href="#Dual%20basis">2.4.2. Dual basis</a></li>
<li><a href="#Example%20%28Important%29">2.4.3. Example (Important)</a></li>
</ul>
</li>
<li><a href="#Hyperplane">2.5. Hyperplane</a>
<ul>
<li><a href="#Affine%20Hyperplane">2.5.1. Affine Hyperplane</a></li>
</ul>
</li>
<li><a href="#Tensor%20Product%20of%20vector%20spaces">2.6. Tensor Product of vector spaces</a>
<ul>
<li><a href="#Example%20%24%5Cmathbb%20R%5Em%20%5Cotimes%20%5Cmathbb%20R%5En%24">2.6.1. Example \(\mathbb R^m \otimes \mathbb R^n\)</a></li>
<li><a href="#Dimension%20of%20tensor%20product%20is%20the%20product%20of%20dimensions">2.6.2. Dimension of tensor product is the product of dimensions</a></li>
</ul>
</li>
<li><a href="#Higher-rank%20tensor%20products">2.7. Higher-rank tensor products</a>
<ul>
<li><a href="#Canonical%20Isomorphisms">2.7.1. Canonical Isomorphisms</a></li>
</ul>
</li>
<li><a href="#Linear%20Maps%20and%20Tensors">2.8. Linear Maps and Tensors</a>
<ul>
<li><a href="#Tensors%20as%20linear%20operators">2.8.1. Tensors as linear operators</a></li>
<li><a href="#Linear%20operator%20as%20Tensors">2.8.2. Linear operator as Tensors</a></li>
<li><a href="#Revision">2.8.3. Revision</a></li>
<li><a href="#Examples">2.8.4. Examples</a></li>
</ul>
</li>
<li><a href="#Index%20Notation%20for%20Tensors">2.9. Index Notation for Tensors</a></li>
<li><a href="#Dirac%20notation%20for%20Tensors">2.10. Dirac notation for Tensors</a></li>
</ul>
</li>
<li><a href="#--Exterior%20Product">3. Exterior Product</a>
<ul>
<li><a href="#Motivation">3.1. Motivation</a></li>
<li><a href="#Computation">3.2. Computation</a></li>
<li><a href="#Exterior%20Product">3.3. Exterior Product</a>
<ul>
<li><a href="#Prallelograms%20in%20%24R%5E3%24%20and%20in%20%24R%5En%24">3.3.1. Prallelograms in \(R^3\) and in \(R^n\)</a></li>
<li><a href="#Definition">3.3.2. Definition</a></li>
<li><a href="#Questions">3.3.3. Questions</a></li>
<li><a href="#Definition%203">3.3.4. Definition 3</a></li>
</ul>
</li>
<li><a href="#Symmetric%20tensor%20product">3.4. Symmetric tensor product</a></li>
<li><a href="#Properties%20of%20spaces%20%24%5Cwedge%5Ek%20V%24">3.5. Properties of spaces \(\wedge^k V\)</a></li>
<li><a href="#Linear%20maps%20between%20spaces%20%24%5Cwedge%5Ek%20V%24">3.6. Linear maps between spaces \(\wedge^k V\)</a></li>
<li><a href="#Exterior%20product%20and%20linear%20dependence">3.7. Exterior product and linear dependence</a>
<ul>
<li><a href="#%2ATheorem%201%2A">3.7.1. <b>Theorem 1</b></a></li>
<li><a href="#%2ALemma%201%2A">3.7.2. <b>Lemma 1</b></a></li>
<li><a href="#%2ALemma%202%2A">3.7.3. <b>Lemma 2</b></a></li>
<li><a href="#%2ALemma%203%2A">3.7.4. <b>Lemma 3</b></a></li>
<li><a href="#%2ATheorem%202%2A">3.7.5. <b>Theorem 2</b></a></li>
<li><a href="#Remark%3A">3.7.6. Remark:</a></li>
</ul>
</li>
<li><a href="#Computing%20the%20dual%20basis">3.8. Computing the dual basis</a></li>
<li><a href="#Gaussian%20elimination">3.9. Gaussian elimination</a></li>
<li><a href="#Rank%20of%20a%20set%20of%20vectors">3.10. Rank of a set of vectors</a></li>
<li><a href="#Exterior%20product%20in%20index%20notation">3.11. Exterior product in index notation</a></li>
<li><a href="#Exterior%20Algebra%20%28Grassmann%20Algebra%29">3.12. Exterior Algebra (Grassmann Algebra)</a></li>
</ul>
</li>
<li><a href="#Basic%20Applications">4. Basic Applications</a>
<ul>
<li><a href="#Determinants%20through%20permuatations%3A%20The%20hard%20way">4.1. Determinants through permuatations: The hard way</a>
<ul>
<li><a href="#Question">4.1.1. Question</a></li>
<li><a href="#Derivation%20of%20the%20Leibniz%20Formula%20for%20Determinants">4.1.2. Derivation of the Leibniz Formula for Determinants</a></li>
</ul>
</li>
<li><a href="#The%20space%20%24%5Cwedge%5EN%20V%24%20and%20oriented%20volume">4.2. The space \(\wedge^N V\) and oriented volume</a></li>
<li><a href="#Determinants%20of%20operators">4.3. Determinants of operators</a>
<ul>
<li><a href="#Leibnitz%27s%20rule">4.3.1. Leibnitz's rule</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#Advanced%20Applications">5. Advanced Applications</a></li>
<li><a href="#Scalar%20Product">6. Scalar Product</a></li>
</ul>
</div>
</div>
\(\newcommand{\astrk}[1]{#1^\ast}\)
\(\newcommand{\astk}[1]{#1^\ast}\)
\(\newcommand{\dim}{\text{dim}}\)
\(\newcommand{\astar}{{a^\ast}}\)


<p>
Notes prepared by Bibek Panthi while reading `Linear Algebra via Exterior Products' by Sergei Winitzki, Ph.D.
</p>

<div id="outline-container-Homomorphism" class="outline-2">
<h2 id="Homomorphism"><span class="section-number-2">1.</span> Homomorphism</h2>
<div class="outline-text-2" id="text-1">
<p>
A homomorphism is a map between two algebraic structures of the same type, that preserves the operations of the structures. This means a map \(f: A \to B\) between two sets \(A, B\) equipped with the same structure such that if \((.)\) is an operation of the structure (supposed here, for simplification, to be a binary operation) then 
</p>

\begin{equation*}
f(x . y) = f(x) . f(y)
\end{equation*}

<p>
The operations that must be preserved by a homomorphism include 0-ary operations, that is the constants. In particular, when an identity element is required by the type of structure, the identity element of the first structure must be mapped to the corresponding identity element of the second structure.
</p>

<p>
The notation for the operations does not need to be the same in the source and the target of a homomorphism. For example, the real numbers form a group for addition, and the positive real numbers form a group for multiplication. The exponential function
</p>

<p>
\(x\mapsto e^{x}\)
</p>

<p>
satisfies
</p>

<p>
\(e^{x+y}=e^{x}e^{y}\)
</p>

<p>
and is thus a homomorphism between these two groups.
(<a href="https://en.wikipedia.org/wiki/Homomorphism">Source</a>)
</p>
</div>
</div>

<div id="outline-container-Vectors" class="outline-2">
<h2 id="Vectors"><span class="section-number-2">2.</span> Vectors</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="outline-container-Projectors%20and%20Subspaces" class="outline-3">
<h3 id="Projectors%20and%20Subspaces"><span class="section-number-3">2.1.</span> Projectors and Subspaces</h3>
<div class="outline-text-3" id="text-2-1">
<dl class="org-dl">
<dt>Subspace</dt><dd>A subspace of a vector space \(V\) is a subset</dd>
</dl>
<p>
\(S \subset V\)
  such that \(S\) itself is a vector space. (i.e. it must be closed
  under addition, and scaling and thus also include \(0\))
</p>
<dl class="org-dl">
<dt>Projector</dt><dd>A linear operator \(\hat{P} : V â†’ V\) is called a projector if  \(\hat{P} \hat{P} = \hat{P}\)</dd>
</dl>
</div>
<div id="outline-container-Properties" class="outline-4">
<h4 id="Properties"><span class="section-number-4">2.1.1.</span> Properties</h4>
<div class="outline-text-4" id="text-2-1-1">
<ul class="org-ul">
<li>Eigenvalues of projector can be only the numbers 0 and 1.</li>
<li>If \(\hat{P}\) is a projector then im \(\hat{P}\)  is the eigenspace of \(\hat{P}\) with eignenvalue 1. dual</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-Isomorphism" class="outline-3">
<h3 id="Isomorphism"><span class="section-number-3">2.2.</span> Isomorphism</h3>
<div class="outline-text-3" id="text-2-2">
<p>
Two vector spaces are isomorphic if there exists a one-to-one linear map between them. The linear map is called the isomorphism.
</p>
<ul class="org-ul">
<li>Any vector space \(V\) of dimension \(n\) is isomorphic to the space  \(K^n\) of \(n\) -tuples.</li>
<li>Vector spaces \(K^m\)  and \(K^n\)  are isomorphic only if they have equal dimension, \(m = n\) . The reason they are not isomorphic for \(m \neq n\)  is that they have different numbers of vectors in a basis, while one-to-one linear maps must preserve linear independence and map a basis to a basis.</li>
</ul>
</div>

<div id="outline-container-Canonical%20Isomorphism" class="outline-4">
<h4 id="Canonical%20Isomorphism"><span class="section-number-4">2.2.1.</span> Canonical Isomorphism</h4>
<div class="outline-text-4" id="text-2-2-1">
<p>
A linear map between two vector spaces V and W is canonically
defined or canonical if it is defined independently of a choice of bases.
</p>

<ul class="org-ul">
<li>Vector spaces \(V\)  and \(W\) are canonically isomorphic if there exist a canonically defined isomorphism between them. The notation is  \(V \cong W\)</li>

<li>IF \(V\) is a one-dimensional vector space then End\(V \cong \mathbb K\) . Any linear operator (i.e. operator \(\subset\) End\(V\)) in \(V\) is a multiplication by a number; this number is the element of \(\mathbb K\) corresponding to the given operator. But \(V \ncong \mathbb K\) unless there is  a "preferred" vector \(e \in V, e \neq 0\) which would be mapped into the number \(1 \in \mathbb K\) . Usually vector spaces do not have any special vectors, so there is no canonical isomorphism. (However, End\(V\)  does have a special element - the identity \(\hat{1}_V\)).</li>
<li>My intuitive picture is that canonically isomorphic spaces have a fundamental structural similarity. An isomorphism that depends on the choice of basis,  is unsatisfactory if we are interested in properties that can be formulated geometrically (independently of any basis).</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-Direct%20Sum%20of%20Vector%20Space" class="outline-3">
<h3 id="Direct%20Sum%20of%20Vector%20Space"><span class="section-number-3">2.3.</span> Direct Sum of Vector Space</h3>
<div class="outline-text-3" id="text-2-3">
<p>
If \(V\) and \(W\) are two given vector spaces over a field \(K\), we define a new vector space \(V \oplus W\) as the space of pairs \((v,w)\) , where \(v \in V\) \(w \in W\). 
</p>
</div>

<div id="outline-container-Canonical%20Projections%3B%20%24V%24%20and%20%24W%24%20as%20subspaces%20of%20subspaces%20of%20%24V%20%5Coplus%20W%24" class="outline-4">
<h4 id="Canonical%20Projections%3B%20%24V%24%20and%20%24W%24%20as%20subspaces%20of%20subspaces%20of%20%24V%20%5Coplus%20W%24"><span class="section-number-4">2.3.1.</span> Canonical Projections; \(V\) and \(W\) as subspaces of subspaces of \(V \oplus W\)</h4>
<div class="outline-text-4" id="text-2-3-1">
<p>
The isomorphism \(\hat{P}_V\) is the canonical projection defined by 
\[\hat{P}_V (v ,w) \equiv v\]
</p>

<p>
It is usually convenient o denote vectors from\(V \oplus W\) by formal linear combinations, e.g. \(v +w\) instead of the pair notation \((v, w)\) . A pair \((v, 0)\) is denoted simply by \(v \in V \oplus W\) . 
</p>
</div>

<ol class="org-ol">
<li><a id="Exercise"></a>Exercise<br />
<div class="outline-text-5" id="text-2-3-1-1">
<p>
Show that the space \(\mathbb R^n \oplus \mathbb R^m\) is isomorphic to \(\mathbb R^{n+m}\) , but not canonically.
</p>
</div>
</li>
</ol>
</div>
</div>
<div id="outline-container-Dual%20%28conjugate%29%20vector%20space" class="outline-3">
<h3 id="Dual%20%28conjugate%29%20vector%20space"><span class="section-number-3">2.4.</span> Dual (conjugate) vector space</h3>
<div class="outline-text-3" id="text-2-4">
<p>
Dual vectors = co-vectors = linear forms 
</p>

<p>
Definition: A co-vector is a linear map \(V \to \mathbb K\) . The set of all co-vectors is the dual space to the vector space \(V\) . The zero covector is the linear function that maps all vectors into zero. This "space of all linear functions" is the space we denote by \(V^\ast\) . In our earlier notation, this space is the same as Hom\((V,\mathbb K)\) .
</p>
</div>

<div id="outline-container-Example%202" class="outline-4">
<h4 id="Example%202"><span class="section-number-4">2.4.1.</span> Example 2</h4>
<div class="outline-text-4" id="text-2-4-1">
<p>
Let \(V\) be the space of polynomials of degree not more than 2 in the variable \(x\) with real coefficients. This space \(V\) is three-dimensional and contains elements such as \(p \equiv p(x) = a + bx +cx^2\) . A linear function \(f^\ast\)  on \(V\) could be defined in a way that might appear nontrivial, such as
 \[f^\ast (p) = \int_0^\infty e^x p(x) dx\]
</p>

<p>
Nevertheless, it is clear that this is a linear function mapping \(V\)  int \(\mathbb R\) . Similarly, 
\[g^\ast (p) = \frac d {dx} \bigg|_{x=1} p(x) \] 
</p>

<p>
is a linear function. Hence, \(f^\ast\) and \(g^\ast\)  belong to \(V^\ast\) . 
</p>
</div>
</div>

<div id="outline-container-Dual%20basis" class="outline-4">
<h4 id="Dual%20basis"><span class="section-number-4">2.4.2.</span> Dual basis</h4>
<div class="outline-text-4" id="text-2-4-2">
<p>
Any vector \(v \in V\) is uniquely expressed as :
</p>

<p>
\[v = \sum_{j=1}^n v_j e_j\] 
</p>

<p>
The coefficients \(v_j\) , understood as a function of the vector \(v\) is a linear function of \(v\). Therefor they are covectors, i.e. elements of \(V^\ast\) . Let us denote these covector by \(e_1^\ast, ..., e_n^\ast\) . Note that these covectors depend on the entire basis \({e_j}\)  and not only on \(e_1\) , as it might appear from the notation. The covector \(e^\ast_1\) will change if we change \(e_2\) or any other basis vector. This is so because the component \(v_1\)  of a fixed vector \(v\) depends not only on \(e_1\) but also on every other basis vector \(e_j\) . 
</p>
</div>

<ol class="org-ol">
<li><a id="Theorem"></a>Theorem<br />
<div class="outline-text-5" id="text-2-4-2-1">
<p>
The set of \(n\) covectors \(e^\ast_1,..., e^\ast_n\) is a basis in \(V^\ast\) . Thus the dimension of the dual space \(V^\ast\)  is equal to that of \(V\) .
</p>

<p>
<b>Remark</b>: The theorem hold only for finite-dimensional spaces! For infinite-dimensional spaces \(V\), the dual space \(V^\ast\) may be "larger" or "smaller" than \(V\). 
</p>
</div>
</li>
</ol>
</div>


<div id="outline-container-Example%20%28Important%29" class="outline-4">
<h4 id="Example%20%28Important%29"><span class="section-number-4">2.4.3.</span> Example (Important)</h4>
<div class="outline-text-4" id="text-2-4-3">
<p>
\[f^\ast(v) = \sum_{k=1}^N f_k^\ast e_k^\ast \big( \sum_{l=1}^N v_l e_l \big) = \sum_{k=1}^N f^\ast_k v_k\]
This formula looks like the scalar product. But it doesn't describe a scalar product because for one thing, \(f^\ast\) and \(v\)  are from `different' vector spaces. Rather lets say that the scalar product resembles above eqn, and this happens only for a special choice of basis (an orthonormal basis) in \(V\).
</p>
</div>
</div>
</div>

<div id="outline-container-Hyperplane" class="outline-3">
<h3 id="Hyperplane"><span class="section-number-3">2.5.</span> Hyperplane</h3>
<div class="outline-text-3" id="text-2-5">
<p>
The hyperplane (i.e. subspace of codimension 1) annihilated by a covector \(f^\ast \in V^\ast\) is the set of all vector \(x \in V\) such that \(f^\ast(x) = 0\) . 
</p>

<p>
The hyperplane annihilated by a nonzero covector \(\astrk f\) is a subspace of \(V\) of dimension \(N-1\) (where \(N \equiv \text{dim} V\)) 
</p>
</div>

<div id="outline-container-Affine%20Hyperplane" class="outline-4">
<h4 id="Affine%20Hyperplane"><span class="section-number-4">2.5.1.</span> Affine Hyperplane</h4>
<div class="outline-text-4" id="text-2-5-1">
<p>
is the set of all vectors \(x \in V\) such that \(\astrk f (x) = \alpha\), where \(\astrk f \in \astrk V\) is nonzero, an \(\alpha\) is a number. 
</p>

<p>
Remark: An affine hyperplane with \(\alpha \neq 0\) is not a subspace of \(V\). 
</p>
</div>
</div>
</div>

<div id="outline-container-Tensor%20Product%20of%20vector%20spaces" class="outline-3">
<h3 id="Tensor%20Product%20of%20vector%20spaces"><span class="section-number-3">2.6.</span> Tensor Product of vector spaces</h3>
<div class="outline-text-3" id="text-2-6">
<p>
The motivation is that we would like to define a product of vectors, \(u \otimes v\), which behaves as we expect a product to behave, e.g. 
\[(a + \lambda b) \otimes c = a \otimes c + \lambda b \otimes c, \forall \lambda \in \mathbb K, \forall a, b, c \in V\]
and the same with respect to the second vector. This property is called bilinearity. It turns out to be impossible to define a nontrivial ( a "trivial" product would be \(a \otimes b = 0\) for all \(a,b\)) product of vectors in a general vector space, such that the result is again a vector in the same space. The solution is to define a product of vectors so that the resulting object is not a vector from \(V\) but an element of another space.
</p>

<p>
Definition: Suppose \(V\) and \(W\) are two vector spaces over a field \(\mathbb K\) ; then one defines a new vector space, which is called the tensor product of \(V\) and \(W\) and denoted by \(V \otimes W\). This is the space of <span class="underline">expressions</span> of the form 
</p>

\begin{equation} 
\label{orga54f192}
v_1 \otimes w_1 + ... + v_n \otimes w_n
\tag{1.16}
\end{equation}

<p>
where \(v_i \in V, w_i \in W\). The plus sign behaves as usual (commutative and associative). The symbol \(\otimes\) is a special separator symbol Further, we postulate that the following combinations are equal, 
</p>

\begin{equation}
\label{org1fd0105}
\lambda ( v \otimes w) = (\lambda v) \otimes w  = v \otimes (\lambda w) \tag{1.17}
\end{equation}

\begin{equation}
\label{orgd341b48}
(v_1 + v_2) \otimes w = v_1 \otimes w + v_2 \otimes w
\tag{1.18}
\end{equation}

\begin{equation} 
\label{org0261944}
v \otimes ( w_1 =w_2) = v \otimes w_1 + v \otimes w_2
\tag{1.19}
\end{equation}

<p>
for any vectors \(v,w, v_{1,2}, w_{1,2}\)  and for any constant \(\lambda\). 
Once could say that the symbol \(\otimes\)  behaves as a noncommutative product sign. This expression \(v \otimes w\) which is by definition an element of \(V \otimes W\), is called the tensor product of vectors \(v\) and \(w\). 
</p>

<p>
The representation of a tensor \(A \in V \otimes W\) in the form \eqref{orga54f192} is not unique, i.e there may be many possible choice of the vectors \(v_j\) and \(w_j\) that give the same tensor \(A\) . For example, 
</p>
\begin{equation}
v_1 \otimes w_1+ v_2 \otimes w_2 = (v_1 - v_2) \otimes w_1 + v_2 \otimes (w_1 + w_2)
\end{equation}
</div>

<div id="outline-container-Example%20%24%5Cmathbb%20R%5Em%20%5Cotimes%20%5Cmathbb%20R%5En%24" class="outline-4">
<h4 id="Example%20%24%5Cmathbb%20R%5Em%20%5Cotimes%20%5Cmathbb%20R%5En%24"><span class="section-number-4">2.6.1.</span> Example \(\mathbb R^m \otimes \mathbb R^n\)</h4>
<div class="outline-text-4" id="text-2-6-1">
<p>
Let \({e_1,...,e_m}\) and \({f_1, ..., f_n}\)  be the standard bases in \(\mathbb R^m\) and \(\mathbb R^n\) respectively. The vector space \(\mathbb R^m \otimes \mathbb R^n\) consists, by definition, of expressions of the form 
</p>
\begin{equation} 
\sum_{i=1}^k v_i \otimes w_i, \ \ \  v_i \in \mathbb R^m, w_i \in \mathbb R^n
\end{equation}
<p>
The vectors \(v_i, w_i\) can be decomposed in their basis. Then 
</p>
\begin{equation}
\sum_{i=1}^k v_i \otimes w_i \\
= \sum_{i=1}^k \bigg(\sum_{j=1}^m \lambda_{ij} e_j \bigg) \otimes \bigg( \sum_{l=1}^n \mu_{il} f_l \bigg) \\
= \sum_{j=1}^m \sum_{l=1}^n C_{jl} e_j \otimes f_l
\end{equation}

<p>
where \(C_{jl} = \sum_{i=1}^k \lambda_{ij} \mu_{il}\) is a certain set of numbers. In other words, and arbitrary element of  \(\mathbb R^m \otimes \mathbb R^n\) can be expressed as a linear combination of \(e_j \otimes f_l\) . And \(e_j \otimes f_l\)  are independent too so they form a basis in the space of  \(\mathbb R^m \otimes \mathbb R^n\). It follows that this space has dimension \(mn\) and that the element of  \(\mathbb R^m \otimes \mathbb R^n\) can be represented by rectangular tables of components \(C_{jl}\) . In other words, the space  \(\mathbb R^m \otimes \mathbb R^n\) is isomorphic to the linear space of rectangular \(m \times n\) matrices with coefficients from \(\mathbb K\) . This isomorphism is not canonical because the components \(C_{jl}\)  depend on the choice of the basis \({e_j}\) and \({f_j}\).
</p>
</div>
</div>

<div id="outline-container-Dimension%20of%20tensor%20product%20is%20the%20product%20of%20dimensions" class="outline-4">
<h4 id="Dimension%20of%20tensor%20product%20is%20the%20product%20of%20dimensions"><span class="section-number-4">2.6.2.</span> Dimension of tensor product is the product of dimensions</h4>
<div class="outline-text-4" id="text-2-6-2">
<p>
To prove this statement, we will explicitly construct a basis in \(V \otimes W\) out of two given bases in \(V\) and in \(W\) . We consider finite dimensional vector spaces \(V\) and \(W\) and vectors \(v_j \in V, w_j \in W\).
</p>
</div>

<ol class="org-ol">
<li><a id="Lemma%201%3A"></a>Lemma 1:<br />
<div class="outline-text-5" id="text-2-6-2-1">
<p>
a) If \(\{v_1, ..., v_m\}\) and \(\{w_1, ..., w_n\}\)  are two bases in their respective spaces then any element \(A \in V \otimes W\) can be expressed as a linear combination of the form
</p>

\begin{equation} 
A = \sum_{j=1}^m \sum_{k=1}^n \lambda_{jk} v_j \otimes w_k\end{equation}

<p>
with some coefficients \(\lambda_{jk}\) 
Proof: Trivial. (Give in <a href="#Example%20%24%5Cmathbb%20R%5Em%20%5Cotimes%20%5Cmathbb%20R%5En%24">Example \(\mathbb R^m \otimes \mathbb R^n\)</a> )
</p>

<p>
b) Any tensor \(A \in V \otimes W\) can be written as linear combination \(A = \sum_k a_k \otimes b_k\) where \(a_k \in V\) and \(b_k \in W\) , with at most \(min(m,n)\) terms in the sum. 
</p>

<p>
Proof: 
</p>

<p>
We can group the \(n\)  terms \(\lambda_{jk} w_k\) into new vectors \(b_j\) and obtain the required formula with \(m\) terms. 
</p>



<p>
\begin{equation} 
A = &sum;<sub>j=1</sub><sup>m</sup> v<sub>j</sub> &otimes; b<sub>j</sub>, <br />
 b<sub>j</sub> &equiv; &sum;<sub>k=1</sub><sup>n</sup> &lambda;<sub>jk</sub> w<sub>k</sub>
\end{equa
</p>

<p>
I will call this formula the decomposition of the tensor \(A\) in the basis \(\{v_j\}\). Since a similar decomposition with \(n\) terms exists for the basis $\{w<sub>k</sub>\}, it follows that \(A\) has a decomposition with at most \(min(m,n)\) terms. Now we need to show that the linear combination in Lemma 1 (a) is linearly independent, to prove that the set \({v_j \otimes w_k}\) is a basis in \(V \otimes W\). 
</p>
</div>
</li>

<li><a id="Lemma%202%3A"></a>Lemma 2:<br />
<div class="outline-text-5" id="text-2-6-2-2">
<p>
If \(f^\ast \in V^\ast\) is any covector, we define the map \(f^\ast : V\otimes W \to W\) (tensor into vectors) by the formula 
</p>

\begin{equation} 
\label{org9693c2b}
f^\ast \bigg( \sum_k v_k \otimes w_k \bigg) \equiv \sum_k f^\ast (v_k) w_k
\tag{1.21}
\end{equation}
<p>
Then this map is a linear map \(V \otimes W \to W\) 
</p>

<p>
Proof: 
The formula \eqref{org9693c2b} define the map explicitly (and canonically!). The linearity is evident from the definition \eqref{org9693c2b}, any linear combination of tensors are mapped into the corresponding linear combinations of vectors. 
</p>

<p>
However, there is one potential problem: there exists many representations of an element \(A \in V\otimes W\)  as an expression of the form \(\sum_k v_k \otimes w_k\) with different choice of \(v_k, w_k\) . Thus we need to show that the map \(f^\ast\) is well-defined by \eqref{org9693c2b}. i.e. that \(f^\ast(A)\) is always the same vector regardless of the choice of the vectors \(v_k\) and \(w_k\) used to represent \(A\).  Recall that different expressions for \(A\) can be equal as a consequence of the axioms \eqref{org1fd0105}-\eqref{org0261944}. Thus it is sufficient to show that \(f^\ast\) transforms both sides of those identities into the same vector. i.e. 
</p>

\begin{equation} 
f^\ast (\lambda v \otimes w) = \lambda f^\ast (v\otimes w), \end{equation}

\begin{equation} 
f^\ast [(v_1 + v_2)  \otimes w] = f^\ast (v_1 \otimes w) + f^\ast (v_2 \otimes w)
\end{equation}

\begin{equation} 
f^\ast [v \otimes (w_1 + w_2)] = f^\ast (v \otimes w_1) + f^\ast (v \otimes w_2). \end{equation}
<p>
these simple calculations are trivial to prove.
</p>
</div>
</li>

<li><a id="Lemma%203%3A"></a>Lemma 3:<br />
<div class="outline-text-5" id="text-2-6-2-3">
<p>
If \(\{ v_1, ... , v_m\}\) and \(\{u_1, ..., u_n\}\) are two linearly independent sets in their respective spaces then the set 
</p>

\begin{equation} 
\{v_j \otimes w_k\} \equiv \{v_j \otimes w_k | 1 \leq j \leq m , 1 \leq k \leq n\}
\end{equation}
<p>
is linearly independent in the space \({V \otimes W}\) 
</p>

<p>
Proof: 
We need to prove that a vanishing linear combination 
</p>

\begin{equation}
\label{orga5a61b2}
\sum_{j=1}^m \sum_{k=1}^n \lambda_{jk} v_j \otimes w_k = 0
\tag{1.23}
\end{equation}

<p>
is possible only if all \(\lambda_{jk} = 0\). Let us choose some fixed value \(j_1\); we will now prove that \(\lambda_{j_1k} = 0\) for all \(k\). Then there exists a covector \(\astk f \in \astk  V\) such that \(\astk f (v_j) = \delta_{j_1j}\). Then we apply the map \(\astk f : V \otimes W \to W\) to \eqref{orga5a61b2} 
</p>

\begin{equation*}
\astk f [ \sum_j \sum_k \lambda_{jk} v_j \otimes w_k \\
= \sum_j \sum_k \lambda_{jk} \astk f (v_j) w_k \\
= \sum_k \lambda_{j_1k} w_k \\ 
= \astk f (0) \\ 
= 0
\end{equation*}
<p>
Therefore \(\sum_k \lambda_{j_1k} w_k = 0\) and since the set \(\{w_k\}\) is linearly independent we must have \(\lambda_{j_1k} = 0\) for all \(k = 1,...,n\) 
</p>
</div>
</li>



<li><a id="Theorem%3A"></a>Theorem:<br />
<div class="outline-text-5" id="text-2-6-2-4">
<p>
If \(V\) and \(W\) are finite dimensional vector spaces then \(\dim (V\otimes W) = \dim V . \dim W\) 
</p>

<p>
Proof: By definition of dimension, there exists linearly independent sets of \(m \equiv \dim V\) vectors in V and of \(n \equiv \dim W\) vectors in \(W\) , and by the basis theorem, these sets are bases in \(V\) and \(W\) respectively. By Lemma 1 the set of \(mn\) elements \(\{v_j \otimes w_k\}\)  spans the space \(V \otimes W\), and by Lemma 3 this set is linearly independent. Therefore this set is a basis. Hence, there are no linearly independent sets of \(mn + 1\) elements in \(V \otimes W\) , so \(\dim (V \otimes W ) = mn\) . 
</p>
</div>
</li>
</ol>
</div>
</div>
<div id="outline-container-Higher-rank%20tensor%20products" class="outline-3">
<h3 id="Higher-rank%20tensor%20products"><span class="section-number-3">2.7.</span> Higher-rank tensor products</h3>
<div class="outline-text-3" id="text-2-7">
<p>
The tensor product of several spaces is defined similarly, e.g. \(U \otimes V \otimes W\) is the space of expressions of the form 
</p>

<p>
\[u_1 \otimes v_1 \otimes w_1 + ... + u_n \otimes v_n \otimes w_n\]
</p>

<p>
Alternatively (and equivalently) one can define the space \(U \otimes V \otimes W\) as the tensor product of the spaces \(U \otimes V\) and \(W\) 
</p>

<p>
(Also \((U \otimes V) \otimes W \cong U \otimes (V \otimes W)\) )
</p>

<p>
Definition: If we only work with one space \(V\) and if all other spaces are constructed out of \(V\) and \(V^\ast\) using the tensor product, then we only need the spaces of the form 
</p>

<p>
\[ V\otimes ... \otimes V \otimes V^\ast \otimes .. \otimes V^\ast\]
</p>

<p>
Elements of such space are called <b>tensor of rank</b> \((m,n)\). For example, vectors \(v \in V\) have rank \((1,0)\) , covector \(f^\ast \in V^\ast\)  have rank \((0,1)\). Scalars from \(\mathbb K\) have ran \((0,0)\) 
</p>
</div>

<div id="outline-container-Canonical%20Isomorphisms" class="outline-4">
<h4 id="Canonical%20Isomorphisms"><span class="section-number-4">2.7.1.</span> Canonical Isomorphisms</h4>
<div class="outline-text-4" id="text-2-7-1">
<ul class="org-ul">
<li>\((U \oplus W) \otimes W \cong (U \otimes W) \oplus (V \otimes W)\)</li>
<li>\((U \oplus V)^\ast \cong U^\ast \oplus V^\ast\)</li>
<li>\((U \otimes V)^\ast \cong U^\ast \otimes V^\ast\)</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-Linear%20Maps%20and%20Tensors" class="outline-3">
<h3 id="Linear%20Maps%20and%20Tensors"><span class="section-number-3">2.8.</span> Linear Maps and Tensors</h3>
<div class="outline-text-3" id="text-2-8">
<p>
Linear operators can be thought of as elements of the space \(V \otimes V^\ast\). This gives a convenient way to represent a linear operator by a coordinate-free formula. We will see that the space Hom\((V,W)\) of linear maps \(V \to W\) is canonically isomorphic to \(W \otimes V^\ast\).
</p>

<ul class="org-ul">
<li>\(\text{End}V \cong V \otimes V^\ast\)</li>
<li>In other words, linear operators are canonically (without choosing a basis) and uniquely mapped into tensors \(\in V \otimes V^\ast\)</li>
<li>From now on, I will not use the map explicitly. Rather, I will simply not distinguish between the spaces \(\text{End}V\) and \(V \otimes V^\ast\). I will write things like \(v \otimes w^\ast \in \text{End}V\) or \(\hat{A} = x \otimes y^\ast\). The space implied in each case will be clear from the context.</li>
</ul>
</div>

<div id="outline-container-Tensors%20as%20linear%20operators" class="outline-4">
<h4 id="Tensors%20as%20linear%20operators"><span class="section-number-4">2.8.1.</span> Tensors as linear operators</h4>
<div class="outline-text-4" id="text-2-8-1">
<p>
Any tensor from the space \(V \otimes \astk V\) acts as a linear map \(V \to V\) 
</p>

<p>
Lemma: 
A tensor \(A \in V \otimes \astk V\) expressed as 
\(A \equiv \sum_{j=1}^k v_j \otimes \astk f_j\) 
defines a linear operator \(\hat{A} : V \to V\) according to the formula
</p>
\begin{equation}
\label{orgda1d706}
\hat{A}x \equiv \sum_j \astk f_j(x) v_j
\tag{1.24}
\end{equation}
<p>
Proof: 
We need to prove two statements: 
</p>
<ul class="org-ul">
<li>The transformation is linear (this follows from the linearity of map \(\astk f_j\) )</li>
<li>The operator \(\hat{A}\) does not depend on the decomposition of the tensor \(A\) using particular vectors \(v_j\) and covectors \(\astk f_j\) (this is easily proved by checking that the operator \(\hat{A}\)  remains unchanged when the axioms \eqref{org1fd0105}-\eqref{org0261944}  are used. e.g.</li>
</ul>

<p>
The second axiom is (for \(u,v \in V\)): 
</p>
\begin{equation}
(u + v) \otimes w = u \otimes w + v \otimes w
\tag{1.18}
\end{equation}
<p>
Then 
</p>
\begin{equation*}
\hat{A}x = [ \sum_j (u_j + v_j) \otimes  \astk f_j ] x \\ = \sum_j \astk f_j (x) (u_j + v_j) \\ = [\sum_j u_j \otimes \astk f_j ] x + [ \sum_j v_j \otimes \astk f_j ] x
\end{equation*}

<p>
The action of \(\hat{A}\) on \(x\) remains unchanged for every x, which means that the operator \(\hat{A}\)  itself is unchanged. i.e. the axiom that works for tensor \(A\) also works for \(\hat{A}\) and the operator \(\hat{A}\) is independent of the choice of decomposition of tensor \(A\) because equivalent decompositions are those which follow axioms \eqref{org1fd0105}-\eqref{org0261944} and our operator also follows those axioms. 
</p>
</div>
</div>

<div id="outline-container-Linear%20operator%20as%20Tensors" class="outline-4">
<h4 id="Linear%20operator%20as%20Tensors"><span class="section-number-4">2.8.2.</span> Linear operator as Tensors</h4>
<div class="outline-text-4" id="text-2-8-2">
<p>
We have seen that any tensor \(A \in V\otimes \astk V\) has a corresponding linear map in \(\text{End}V\). Now conversely ,let \(\hat{A} \in \text{End}V\) the linear operator and let \(\{v_1, ... , v_n \}\) be a basis in \(V\). We will now find such covectors \(\astk f_k \in \astk V\) that the tensor \(\sum_k v_k \otimes \astk f_k\) corresponds to \(\hat{A}\). The required covectors cane be defined by the formula 
</p>
\begin{equation*}
\astk f_k(x) \equiv \astk v_k (\hat{A}x), \forall x \in V
\end{equation*}
<p>
where \({\astk v_k}\) is the dual basis. With this definition, we have, 
</p>
\begin{equation*}
[\sum_k v_k \otimes \astk f_k] x = \sum \astk f_k(x) v = \sum \astk v_k (\hat{A} x) v_k = \hat{A} x
\end{equation*}
<p>
The last equality is based on the formula 
</p>
\begin{equation*}
\sum \astk v_k(y) v_k = y
\end{equation*}
<p>
which holds because the components of a vector \(y\) in the basis \(\{v_k\}\) are \(\astk v_k (y)\). 
</p>
</div>

<ol class="org-ol">
<li><a id="Linear%20operator%20as%20Tensors--Theorem"></a>Theorem<br />
<div class="outline-text-5" id="text-2-8-2-1">
<p>
There is a canonical isomorphism \(A \to \hat{A}\) between the spaces \(V \otimes \astk V\) and \(\text{End} V\). 
</p>

<p>
Proof: 
To prove that a map is an isomorphism of vector spaces, we need to show that this map is linear and bijective (on-to-one). 
</p>
<ul class="org-ul">
<li>Linearity easily follows from the definition of map \(\hat{A}\).</li>
<li>To prove the bijectivity, we need to show that 
<ul class="org-ul">
<li>for any operator \(\hat{A}\) there exists a corresponding tensor \(A\) (this we have show <a href="#Linear%20operator%20as%20Tensors">above</a>), and</li>
<li>that two different tensors \(A \neq B\) cannot be mapped into the same operator \(\hat{A} = \hat{B}\). If two different tensors were mapped int other same operator, it would follow from the linearity that \(\widehat{A-B} = \hat{A} - \hat{B} = 0\), in other words, that a nonzero tensor \(C \equiv A-B \neq 0\) is mapped into the zero operator. But from the definition of <a href="#Tensors%20as%20linear%20operators">Tensors as linear operators</a>o a non zero tensor is not mapped into a zero operator. This means, two different tensors map into different operators.</li>
</ul></li>
</ul>

<p>
Also note that we did use a basis \(\{v_k\}\) in the construction of this isomorphism when we defined the covectors \(\astk f_k\) . However this map is canonical because it is the same map for all choices of the basis. Indeed, if we choose another basis \(\{v_k'\}\) then of course the covectors \(\astk {f_k'}\) will be different from \(\astk f_k\), but the tensor \(A\) will remain same, because (as we just proved) different tensors are always mapped into different operators. 
</p>
</div>
</li>
</ol>
</div>

<div id="outline-container-Revision" class="outline-4">
<h4 id="Revision"><span class="section-number-4">2.8.3.</span> Revision</h4>
<div class="outline-text-4" id="text-2-8-3">
<ul class="org-ul">
<li>We will see that the space Hom\((V,W)\) of linear maps \(V \to W\) is canonically isomorphic to \(W \otimes V^\ast\).</li>
<li>\(\text{End}V \cong V \otimes V^\ast\)</li>
<li>In other words, linear operators are canonically (without choosing a basis) and uniquely mapped into tensors \(\in V \otimes V^\ast\)</li>
<li>From now on, I will not use the map explicitly. Rather, I will simply not distinguish between the spaces \(\text{End}V\) and \(V \otimes V^\ast\). I will write things like \(v \otimes w^\ast \in \text{End}V\) or \(\hat{A} = x \otimes y^\ast\). The space implied in each case will be clear from the context.</li>
</ul>
</div>
</div>

<div id="outline-container-Examples" class="outline-4">
<h4 id="Examples"><span class="section-number-4">2.8.4.</span> Examples</h4>
<div class="outline-text-4" id="text-2-8-4">
<ul class="org-ul">
<li><b>Matrices as tensors</b>: Suppose that we have a matrix \(A_{jk}\) that specifies the linear operator \(\hat{A}\) in a basis \(\{e_k\}\) . Which tensor \(A \in V \otimes \astk V\) corresponds to this operator? <i>Answer</i>: \(A = \sum_{j,k=1}^n A_{jk} e_j \otimes \astk e_k\)</li>
<li><b>Product of linear operators</b>: Suppose \(\hat{A} = \sum_k v_k \otimes \astk f_k\) and \(\hat{B} = \sum_l w_l \otimes \astk g_l\) are two operators, then \(\hat{A}\hat{B} = \sum_k \sum_l \astk f_k (w_l) v_k \otimes \astk g_l\) . This appears too natural in Einstein's summation notation.</li>
<li>The space \(V \otimes \astk V\) can be interpreted as \(\text{End }V\) as \(\text{End }\astk V\) , or as \(\text{Hom}(V\otimes \astk V, \mathbb K)\) . This means that one tensor \(A \in V \otimes \astk V\) represents an operator in \(V\) , an operator in \(\astk V\) , or a mp from operators into numbers. What is the relation between all these different interpretations of the tensor \(A?\)  <i>Answer</i>: Lets take the identity tensor \(\hat{1}_V\)  represents the identity operator in \(V\) and in \(\astk V\). It also represents the following map \(V \otimes \astk V \to \mathbb K\) ,</li>
</ul>
\begin{equation*}
\hat{1}_V : v \otimes \astk f \mapsto \astk f (v)
\end{equation*}
<p>
This map applied to an operator \(\hat{A} \in V \otimes \astk V\) yields the trace of that operator.  
</p>
</div>
</div>
</div>

<div id="outline-container-Index%20Notation%20for%20Tensors" class="outline-3">
<h3 id="Index%20Notation%20for%20Tensors"><span class="section-number-3">2.9.</span> Index Notation for Tensors</h3>
</div>
<div id="outline-container-Dirac%20notation%20for%20Tensors" class="outline-3">
<h3 id="Dirac%20notation%20for%20Tensors"><span class="section-number-3">2.10.</span> Dirac notation for Tensors</h3>
</div>
</div>

<div id="outline-container---Exterior%20Product" class="outline-2">
<h2 id="--Exterior%20Product"><span class="section-number-2">3.</span> Exterior Product</h2>
<div class="outline-text-2" id="text-3">
<p>
The basic idea is that we would like to define an <i>antisymmetric</i> and bilinear product of vectors. i.e \(a \wedge b = -b \wedge a\)
</p>
</div>

<div id="outline-container-Motivation" class="outline-3">
<h3 id="Motivation"><span class="section-number-3">3.1.</span> Motivation</h3>
<div class="outline-text-3" id="text-3-1">
<p>
Comes from considering the proeprties of areas and voumens in the framework of elementatry Euclidean geometry.
</p>

<p>
2D Oriented Area: The oriented area \(A(a,b)\)  of a parallelogram spanned by the vectors \(a\) and \(b\) in the two-dimensional Euclidean space is an antisymmetric and bilinear function of the vectors:
</p>

\begin{align*}
A(a,b) = -A(b,a) \\
A(\lambda a,b) = \lambda A(a,b) \\ A(a, b+c) = A(a,b) + A(a,c)
\end{align*}

<p>
The oriented area is defined as 
</p>

<p>
\(A(a,b) = \pm |a| . |b|. \sin\alpha\) 
</p>

<p>
where the sign i positive when angle \(\alpha\) is measured from the vector \(a\) to \(b\) in the counterclockwise direction, and negative otherwise. 
</p>

<p>
The <code>antisymmetric</code> property comes from the definition of \(A(a,b)\) while the bilinearity property can be proved from geometrical constructions.
</p>
</div>
</div>

<div id="outline-container-Computation" class="outline-3">
<h3 id="Computation"><span class="section-number-3">3.2.</span> Computation</h3>
<div class="outline-text-3" id="text-3-2">
<p>
   Computation of oriented area is straighforward due to its algebraic properties. Say \(a\) and \(b\) are given through their components in standard basis \(\{e_1, e_2\}\). We assume, of course, that the vectors \(e_1\) adn \(e_2\) are orthogonal to each other and have unit length. Also, assume the right angle is measured from \(e_1\) to \(e_2\) in counterclockwise direction, so that \(A(e_1,e_2) = +1\). Then, 
(by using \(A(e_1,e_1) = 0\) and  \(A(e_2,e_2) = 0\))
</p>

\begin{equation*}
A(a,b) = A(\alpha_1 e_1 + \alpha_2 e_2, \beta_1 e_1 + \beta_2 e_2) = \alpha_1 \beta_2 - \alpha_2 \beta_1
\end{equation*}
</div>
</div>

<div id="outline-container-Exterior%20Product" class="outline-3">
<h3 id="Exterior%20Product"><span class="section-number-3">3.3.</span> Exterior Product</h3>
<div class="outline-text-3" id="text-3-3">
</div>
<div id="outline-container-Prallelograms%20in%20%24R%5E3%24%20and%20in%20%24R%5En%24" class="outline-4">
<h4 id="Prallelograms%20in%20%24R%5E3%24%20and%20in%20%24R%5En%24"><span class="section-number-4">3.3.1.</span> Prallelograms in \(R^3\) and in \(R^n\)</h4>
<div class="outline-text-4" id="text-3-3-1">
<p>
Its not possible to characterize the orientation of the area simply by a sign. 
</p>

<p>
Statement: Let \(a,b\) be two vectors in \(R^3\) and let \(P(a,b)\) be the parallelogram spanned by these vectors. Denote by \(P(a,b)_{e_1,e_2}\) the projection of \(P(a,b)\) onto the coordinate plane \(Span \{e_1,e_2\}\) . Denote by \(A(a,b)_{e_1,e_2}\) the oriented area of \(P(a,b)_{e_1,e_2}\) . Then \(A(a,b)_{e_1,e_2}\) is a bilinear, antisymmetric function of \(a\) and \(b\). 
</p>

<p>
Proof: Since the oriented area of projected parallelogram is bilinear, antisymmetric function of the projections of the vectors. And the projection of vectors onto the coordinate plane is a linear transformation. Hence, it can be shown that \(A(a,b)_{e_1,e_2}\) is indeeda bilinear, and antisymmetric function of \(a\) and \(b\). 
</p>

<p>
It is therefore convenient to consider the oriented areas of the three projections &#x2013; \(A(a,b)_{e_1,e_2}\) , \(A(a,b)_{e_2,e_3}\) , \(A(a,b)_{e_3,e_1}\) &#x2013; as three components of <b>vector-valued area</b> \(A(a,b)\) of prallelogram spanned by \(a,b\). Indeed these three projected areas coincide with the three Euclidean components of vector product \(a \times b\). However, the vector product cannot be generalized to all higher-dimensional spaces. So, we will generalize the idea of <b>projecting the prallelogram onto coordinate planes</b>. 
</p>

<p>
Consider a prallelogram in n-dimensional Euclidean space \(V\) with the standard basis \({e_1,...,e_n}\). Now we have \(n\) choose 2  = \(1/2*n*(n-1)\) projections. Each of the projection has an oriented area; which is bilinear, antisymmetric number valued function of \(a\) and \(b\) .  We may then regard these \(1/2*n*(n-1)\) numbers as the components of a vector representing the oriented area of the parallelogram. 
</p>

<p>
The antisymmetric, bilinear function \(A(a,b)\) whose value is a vector with \(1/2*n*(n-1)\) components, is a vector in <code>a new space</code> - the 'space of oriented areas',.  It si teh space of <b>bivectors</b>, to be denoted by \(\wedge^2 V\).
</p>

<p>
We will see that the unoriented area of the prallelogram is computed as teh length of the vector \(A(a,b)\).  This is generalization of the Pythagoras theorem to areas in higher-dimensional spaces.
</p>

<p>
The generalization of the Pythagoras theorem holds not only for areas but also for higher-dimensional volumes. 
</p>
</div>

<ol class="org-ol">
<li><a id="Analogy%20for%20higher%20dimensional%20Pythagoras%20theorem"></a>Analogy for higher dimensional Pythagoras theorem<br />
<div class="outline-text-5" id="text-3-3-1-1">
<p>
The analogy between ordinary vectors and vector-valued areas can be understood visually as follows. A straight line segment in an n-dimensional space is represented by a vector whose n components (in an orthonormal basis) are the signed lengths of the n projections of the line segment onto the coordinate axes. (The components are signed, or oriented, i.e. taken with a negative sign if the orientation of the vector is opposite to the orientation of the axis.) The length of a straight line segment, i.e. the length of the vector of \(v\), is then computed as \(\sqrt{<v,v>}\). A parallelogram in space is represented by a vector \(\psi\) whose \({n \choose k}\) components are the oriented in areas  of \({n \choose k}\)  projections of the prallelogram onto the coordinate planes. (The vector \(\psi\) belogns to the space of oriented areas, not to the original $n$-dimensional space.) The numerical value of the area of teh prallelogram is then computed as \(\sqrt{<\psi,\psi>}\).
</p>
</div>
</li>
</ol>
</div>

<div id="outline-container-Definition" class="outline-4">
<h4 id="Definition"><span class="section-number-4">3.3.2.</span> Definition</h4>
<div class="outline-text-4" id="text-3-3-2">
<p>
Like the tensor product space, the space of exterior products can be defined solely by its algebraic properties. 
</p>

<p>
<b>Definition 1:</b> Given a vector space \(V\), we define a new vector space \(V \wedge V\) called teh <code>exterior product</code> (or antisymmetric tensor product, or alternating product, or <code>wedge product</code>) of two copies of \(V\) . The space \(V \wedge V\) is the subspace in \(V \otimes V\) consisting of all <b>antisymmetric</b> tensors. i.e. tensors of the form 
</p>
\begin{equation*}
v_1 \otimes v_2 - v_2 \otimes v_1
\end{equation*}

<p>
and all linear combinations of such tensors. 
</p>

<p>
The exterior product of two vectors \(v_1\) and \(v_2\) is the expression shown above; it is obviously and antisymmetric and bilinear function of \(v_1\) and \(v_2\). 
</p>

<p>
<b>Defintion 2</b>: The exterior product of \(k\) copies of \(V\) (also called the $k$-the exterior power of \(V\) ) is denoted by \(\wedge^k V\) and is defined as the subspace of totally antisymmetric tensors within \(V \otimes ... \otimes V\). In the concise notation, this is the space spanned by expressions of the form 
</p>
\begin{equation*}
v_1 \wedge v_2 \wedge ... \wedge v_k ; v_j \in V
\end{equation*}
<p>
assuming that the properties of the wedge product hold. 
</p>

<p>
The previously defined space of bivectors is in this notation \(V \wedge V \equiv \wedge^2 V\). A natural extension of this notation is \(\wedge^0 V = \mathrm{K}\) and \(\wedge^1 V \equiv V\) 
Tensors from the space \(\wedge^n V\) are also called $n$-vectors or antisymmetric tensors of rank \(n\). 
</p>
</div>
</div>

<div id="outline-container-Questions" class="outline-4">
<h4 id="Questions"><span class="section-number-4">3.3.3.</span> Questions</h4>
<div class="outline-text-4" id="text-3-3-3">
</div>
<ol class="org-ol">
<li><a id="example"></a>example<br />
<div class="outline-text-5" id="text-3-3-3-1">
<p>
Lets compute the 3-vectors \(a \wedge b \wedge c\) where the vectors are in \(\mathrm{R}^3\) 
Then, if we expand each vector \((a,b,c)\) in terms of basis vectors \(\{e_1,e_2,e_3\}\) then the terms such as \(e_1 \wedge e_2 \wedge e_1\) will vanish because 
</p>
\begin{equation*}
e_1 \wedge e_2 \wedge e_1 = - e_2 \wedge e_1 \wedge e_1 = 0
\end{equation*}
<p>
so, only terms containing different vectors need to be kept and we find 
\(a \wedge b \wedge c = (sth) e_1 \wedge e_2 \wedge e_3\)
</p>
</div>
</li>

<li><a id="question%201"></a>question 1<br />
<div class="outline-text-5" id="text-3-3-3-2">
<p>
Our original goal was to introduce a product of vectors in order to obtain a geometric representation of oriented areas. Instead, \(a \wedge b\) was defined algebraically, through tensor products. It is clear that \(a \wedge b\) is antisymmetric and bilinear, but why does it represent an oriented area? 
</p>

<p>
Right now we have constructed the space \(V \wedge V\) simply as the space of antisymmetric products. By constructing that space merely out of the axioms of the antisymmetric products, we already covered every possible bilinear antisymmetric product. This means that any antisymmetric bilinear function of tow vectors \(x\) and \(y\) is proportional to \(x \wedge y\), or more generally, is a linear function of \(x \wedge y\) (perhaps with values in a different space). Therefore the space of oriented areas (i.e. the space of linear combinations of \(A(x,y)\) for various \(x\) and \(y\)) is in any case mapped to a subspace of \(V \wedge V\). We have also seen that oriented areas in \(N\) dimensions can be represented through \({N \choose 2}\) projections, which indicates that they are vectors in some \({N \choose 2}\) -dimensional space. We will see that the space \(V \wedge V\) has exactly this dimension. 
</p>
</div>
</li>
<li><a id="remark%20on%20the%20origin%20of%20the%20name%20%22exterior%22"></a>remark on the origin of the name "exterior"<br />
<div class="outline-text-5" id="text-3-3-3-3">
<p>
&#x2026; We can interpret this geometrically by saying that the `product' of two volumes is zero if these volumes have a vector in common. This motivated Grassmann to call his anitsymmetric product `exterior'. In his reasoning, the product of two `extensive quanitities' (such as lines, areas, or volumes) is nonzero only when each of two quanitities is geometrically `to the exterior' (outside) of the other. 
</p>

<p>
(I don't get this really)
</p>
</div>
</li>

<li><a id="question%202"></a>question 2<br />
<div class="outline-text-5" id="text-3-3-3-4">
<p>
Is the space \(\wedge^n (V^\ast)\) dinfferent from \((\wedge^n V)^\ast\) ?
</p>

<p>
These spaces are canonically isomorphic, but ther is a subtle technical issue worth mentioning. Consider an example: 
\(a^\ast \wedge b^\ast \in \wedge^2 (V^\ast)\) can act upon \(u \wedge v \in \wedge^2 V\) bu the standar tensor product rule, namely \(a^\ast \otimes b^\ast\) acts on \(u \otimes v\) as 
</p>
\begin{equation*}
(a^\ast \otimes b^\ast) (u \otimes v) = a^\ast (u) b^\ast (v)
\end{equation*}
<p>
so by using the definition of \(a^\ast \wedge b^\ast\) and \(u \wedge v\) through tensor product we find, 
</p>
\begin{equation*}
(a^\ast \wedge b^\ast) (u \wedge v) = 2 \{ a^\ast(u) b^\ast(v) - b^\ast(u) a^\ast(v) \}
\end{equation*}
<p>
we get  combinatorial factor 2. With \(\wedge^n (V^\ast)\) and \((\wedge^n V)^\ast\) we get a factor of \(n!\). It is not always convenient to have this combinatorial factor. For example in a finite numebr field the number \(n!\) might be equal to zero for large enough \(n\) . In these cases we could <i>redefine</i> the action of \(a^\ast \wedge b^\ast\) on \(u \wedge v\) as 
</p>
\begin{equation*}
(a^\ast \wedge b^\ast) (u\wedge v) \equiv a^\ast (u) b^\ast (v) - b^\ast(u) a^\ast (v)
\end{equation*}
<p>
If we are not working in a finite number field, we are able to divide by any integer, so we may keep combinatorial factors in the denominators of expressions where such factors appear. 
</p>
</div>
</li>
</ol>
</div>

<div id="outline-container-Definition%203" class="outline-4">
<h4 id="Definition%203"><span class="section-number-4">3.3.4.</span> Definition 3</h4>
<div class="outline-text-4" id="text-3-3-4">
<p>
The action of a \(k\) form \(f_1^\ast \wedge ... \wedge f_k^\ast\)  on a \(k\) -vector \(v_1 \wedge ... \wedge v_k\) is defined by 
</p>
\begin{equation}
\sum_\sigma (-1)^{|\sigma|} f_1^\ast(v_{\sigma(1)}) ... f_k^\ast (v_{\sigma(k)})
\end{equation}
<p>
where the summation is performed over all permutations \(\sigma\) of the ordered set \((1,...,k)\) 
</p>
</div>
</div>
</div>

<div id="outline-container-Symmetric%20tensor%20product" class="outline-3">
<h3 id="Symmetric%20tensor%20product"><span class="section-number-3">3.4.</span> Symmetric tensor product</h3>
<div class="outline-text-3" id="text-3-4">
<p>
At this point, could we also define something else, say a symmetric product instead of the exterior product? We could try to define a product say \(a \odot b\) such that \(a \odot b = 2 b \odot a\). But this won't work because, we would have 
</p>
\begin{equation*}
b \odot a = 2 a \odot b = 4 b \odot a
\end{equation*}
<p>
so all the `\(\odot\)' products would have to vanish. 
</p>

<p>
We can define a symmetric tensor product, \(\otimes_S\) with the property 
</p>
\begin{equation*}
a \otimes_S b = b \otimes_S a
\end{equation*}

<p>
but it is impossible to define anything else in a similar fashion. (Grassmann has proved it)
</p>


<p>
The antisymmetric tensor product is the eignespace (within \(V \otimes V\)) of the exchange operator \(\hat{T}\) with the eigenvalue \(-1\). This operator has only eigenvalues \(\pm 1\) , so the only other possibility is to consider the eignespace with eignevalue \(+1\). This eignespace is spanned by symmetric tensors of the form \(u \otimes v+ v \otimes u\), and can be considered as the space of symmetric tensor products. We could write 
</p>
\begin{equation*}
a \otimes_s b  = a \otimes b + b \otimes a
\end{equation*}
<p>
and develop the properties of this product. However it turns out that the symmetric tensor product is much less useful for the purposes of linear algebra that the antisymmetric subspace. 
</p>
</div>
</div>
<div id="outline-container-Properties%20of%20spaces%20%24%5Cwedge%5Ek%20V%24" class="outline-3">
<h3 id="Properties%20of%20spaces%20%24%5Cwedge%5Ek%20V%24"><span class="section-number-3">3.5.</span> Properties of spaces \(\wedge^k V\)</h3>
<div class="outline-text-3" id="text-3-5">
<p>
Similar to tensors, n-vectors are not uniquely respresentable by linear combinations of exterior products. There can be alternative representation for the same $n$-vector. 
</p>

<p>
Statement: Any $(N-1)$-vector in an $N$-dimensional space can be written as a single-term exterior product of the form \(a_1 \wedge ... \wedge a_{N-1}\)
</p>

<p>
This can be proved by induction. 
</p>
</div>
</div>

<div id="outline-container-Linear%20maps%20between%20spaces%20%24%5Cwedge%5Ek%20V%24" class="outline-3">
<h3 id="Linear%20maps%20between%20spaces%20%24%5Cwedge%5Ek%20V%24"><span class="section-number-3">3.6.</span> Linear maps between spaces \(\wedge^k V\)</h3>
<div class="outline-text-3" id="text-3-6">
<p>
A simplest example is a map 
</p>
\begin{equation*}
L_a : \omega \to a \wedge \omega
\end{equation*}
<p>
mapping \(\wedge^k V \to \wedge^{k+1}V\); here the vector \(a\) is fixed. It is important to chect that \(L_a\) is a linear map between these spaces. 
</p>

\begin{align}
L_a (\omega + \lambda \omega') 
&= a \wedge (\omega + \lambda \omega')  \\
&= a \wedge \omega + \lambda a \wedge \omega' \\
&= L_a(\omega) + \lambda L_a(\omega')
\end{align}

<p>
Let us now fix a covector \(a^\ast\) .  A covector maps from vector space to field of scalar. Now we will use covector to construct a map \(V \wedge V \to V\). Lets denote this map by \(l_{a^\ast}\). 
</p>

<p>
It would be incorrect to define the map \(l_{a^\ast}\) by the formula \(l_{a^\ast}(v\wedge w) = a^\ast (v)w\) because such a definition does not respect the antisymmetry of the wedge product and thus violates the linearity condition (How? I get that it violates antisymmetric, but it doesn't violate linearity. No! it does, because the linearity must also hold when the representation of the exterior product changes i.e \(\omega = v \wedge w\) may also be expressed as \(\omega = (v - w) \wedge w\) but since \(l_\astar\) violates antisymmetry, and the different representations of \(\omega\) are obtained by using the anitsymmetry and linearity properties, it also violates linearity. (See proof of page 88))
</p>

\begin{align}
l_{a^\ast} (w \wedge v) 
&\overset{!}{=}  l_{a^\ast} ((-1) v \wedge w) \\
&= -l_{a^\ast}(v \wedge w) \neq a^\ast(v) w
\end{align}

<p>
So we need to act with \(a^\ast\)  on each of the vectors in a  wedge product and make sure that the correct minus sign comes outside. 
</p>

\begin{equation*}
l_a^\ast (v \wedge w) \equiv a^\ast(v) w - a^\ast(w) v
\end{equation*}

<p>
is an acceptable formula. 
</p>

<p>
Let us now extent \(l_{a^\ast}\) to a map 
</p>
\begin{equation*}
l_{a^\ast} : \wedge^k V \to \wedge^{k-1} V
\end{equation*}

<p>
defined as follows: 
</p>
\begin{align}
l_\astar v &\equiv \astar (v)\\ 
l_\astar (v \wedge \omega) &\equiv \astar(v) \omega - v \wedge l_\astar (\omega)
\end{align}
<p>
This definition is inductive. The action of \(l_{a^\ast}\) on a sum of terns is defined by requiring linearity, 
</p>
\begin{align}
l_\astar (A + \lambda B) 
\equiv l_\astar(A) + \lambda l_\astar(B)
\end{align}

<p>
We can convert this inductive definition to a more explicit formula. 
</p>
\begin{equation*}
l_\astar(v_1 \wedge ... \wedge v_k) \equiv \astar (v_1) v_2 \wedge ... \wedge v_k - \astar(v_2) v_1 \wedge v_3 \wedge ... \wedge v_k + ... + (-1)^{k-1} \astar(v_k) v_1 \wedge ... \wedge v_{k-1}
\end{equation*}
<p>
This map is called the <b>interior product</b> or the <b>insertion</b> mapcar. The insertion map \(l_\astar \psi\) `inserts' the covector \(\astar\) into the tensor \(\psi \in \wedge^k V\) by acting with \(\astar\) on each of the vectors in the exterior product that makes up \(\psi\). 
</p>
</div>
</div>

<div id="outline-container-Exterior%20product%20and%20linear%20dependence" class="outline-3">
<h3 id="Exterior%20product%20and%20linear%20dependence"><span class="section-number-3">3.7.</span> Exterior product and linear dependence</h3>
<div class="outline-text-3" id="text-3-7">
<p>
One powerful property of the exterior product is its close realtion to linear independence of sets of vectors. E.g. if \(u = \lambda v\) then \(u \wedge v = 0\). 
</p>
</div>

<div id="outline-container-%2ATheorem%201%2A" class="outline-4">
<h4 id="%2ATheorem%201%2A"><span class="section-number-4">3.7.1.</span> <b>Theorem 1</b></h4>
<div class="outline-text-4" id="text-3-7-1">
<p>
A set \(\{v_1, ..., v_k\}\) of vectors from \(V\) is linearly independent if and only if \((v_1 \wedge v_2 \wedge ... \wedge v_k) \neq 0\).
</p>

<p>
Proof: 
First prove that if \(\{v_j\}\) is linearly dependent then the exterior product is zero. This can be easly proved. 
</p>

<p>
Then we need to prove that if  \(\{v_j\}\) is linearly independent then exterior product is non-zero. The proof is by induction in \(k\) . 
</p>

<p>
For \(k=1\), if \(\{v_1\}\) is linearly independent then clearly, \(v_1 \neq 0\). 
</p>

<p>
Assume that the statement is proved for \(k-1\) and that \(\{v_1, ..., v_k\}\) is a linearly independent set. There exists a covector \(f^\ast \in V^\ast\) such that \(f^\ast (v_1) =1\) and \(f^\ast(v_i) = 0, i = 2,..,k\). Now we apply the interior product map \(l_{f^\ast} : \wedge^k V \to \wedge^{k-1} V\) to the tensor \(v_1 \wedge ... \wedge v_k\) and find that 
</p>

\begin{equation*}
l_{f^\ast} ( v_1 \wedge ... \wedge v_k) = v_2 \wedge ... \wedge v_k
\end{equation*}
<p>
By induction step, the linear independence of \(k-1\) vectors entails \(v_2 \wedge ... \wedge v_k \neq 0\). And since the map \(l_{f^\ast}\) is linear this entails, \((v_1 \wedge v_2 \wedge ... \wedge v_k) \neq 0\). 
</p>

<p>
Also note that any tensor from the highest exterior power \(\wedge^NV\) can be represented as just a single-term exterior product of \(N\) vectors.
</p>
</div>
</div>

<div id="outline-container-%2ALemma%201%2A" class="outline-4">
<h4 id="%2ALemma%201%2A"><span class="section-number-4">3.7.2.</span> <b>Lemma 1</b></h4>
<div class="outline-text-4" id="text-3-7-2">
<p>
For any tensor \(\omega \in \wedge^N V\) there exists vectors \(\{v_1, ... , v_N\}\) such that
 \(\omega = v_1 \wedge ... \wedge v_N\)
</p>
</div>
</div>

<div id="outline-container-%2ALemma%202%2A" class="outline-4">
<h4 id="%2ALemma%202%2A"><span class="section-number-4">3.7.3.</span> <b>Lemma 2</b></h4>
<div class="outline-text-4" id="text-3-7-3">
<p>
If \(\{e_1, ..., e_N\}\) is a basis in \(V\) then any tensor \(A \in \wedge^m V\) can be decomposed as a linear combination fo the tensors \(e_{k_1} \wedge e_{k_2} \wedge ... \wedge e_{k_m}\) with some indices \(k_j, 1 \leq j \leq m\)
</p>
</div>
</div>

<div id="outline-container-%2ALemma%203%2A" class="outline-4">
<h4 id="%2ALemma%203%2A"><span class="section-number-4">3.7.4.</span> <b>Lemma 3</b></h4>
<div class="outline-text-4" id="text-3-7-4">
<p>
If \(\{v_1, ... , v_n \}\) is a linearly independent set of vectors (not necessarily a basis in \(V\) since \(n \leq N\) ), then:
</p>

<ol class="org-ol">
<li>The set of \({n \choose 2}\) tensors</li>
</ol>
<p>
\(\{ v_j \wedge v_k, 1 \leq j < k \leq n\}\) 
is linearly independent in the space \(\wedge^2 V\) 
</p>

<ol class="org-ol">
<li>The set of \({ n \choose m }\) tensors</li>
</ol>
<p>
\(\{v_{k_1} \wedge v_{k_2} \wedge ... \wedge v_{k_m}, 1 \leq k_1 < k_2 < ... < k_m \leq n \}\) 
is linearly independent in the space \(\wedge^m V\) for \(2 \leq m \leq n\) 
</p>

<p>
Proof of 3.1) 
Suppose the set \(\{v_j\}\) is linearly independent but the set \(\{v_j \wedge v_k \}\) is linearly dependent so taht there exists a linear combination
\(\sum_{1\leq j < k \leq n} \lambda_{jk}v_j \wedge v_k = 0\) 
with at least some \(\lambda_{jk} \neq 0\). Without loss of generality, \(\lambda_{12} \neq 0\). There exists a covector \(f^\ast \in V^\ast\) such tat $f^&lowast;(v<sub>1</sub>) = 1 $ and \(f^\ast(v_i) = 0\) for \(i \neq 1\). Applying the interior product with this covector to the above tensor, 
\(0 = l_{f^\ast} \big[ ... \big] = \sum_{k=2}^n \lambda_{1k} v_k\) 
therefore by linear independence of \(\{v_k\}\) all \(0 = l_{f^\ast} \big[ ... \big] = \sum_{k=2}^n \lambda_{1k} v_k\) contradicting the assumptino \(\lambda_{12} \neq 0\).
</p>
</div>
</div>

<div id="outline-container-%2ATheorem%202%2A" class="outline-4">
<h4 id="%2ATheorem%202%2A"><span class="section-number-4">3.7.5.</span> <b>Theorem 2</b></h4>
<div class="outline-text-4" id="text-3-7-5">
<p>
The dimension of space \(\wedge^m V\) is
</p>
\begin{equation*}
\dim \wedge^m V = { N \choose m } = \frac{N!}{m! (N-m)!}
\end{equation*}
<p>
where \(N \equiv \text{dim} V\). For \(m>N\) we have \(\dim \wedge^m V = 0\)
</p>

<p>
Proof: We explicitly create a basis in \(\wedge^m V\). Choose basis \(\{e_1, ..., e_N\}\) in \(V\). Then use Lemma 3 and Lemma 2 to reach the conclusion that the \(\dim \wedge^m N = { N \choose m }\) .
</p>

<p>
For \(m>N\), Theorem 1 would imply that only possible tensor in space \(\wedge^m V\) would be the zero tensor.  
</p>
</div>
</div>

<div id="outline-container-Remark%3A" class="outline-4">
<h4 id="Remark%3A"><span class="section-number-4">3.7.6.</span> Remark:</h4>
<div class="outline-text-4" id="text-3-7-6">
<p>
While \(a \wedge b\) is interpreted geometrically as the oriented area of a parallelogram spanned by \(a\) and \(b\), a general linear combination such as \(a \wedge b + c \wedge d + e \wedge f\) oes not have this interpretation (unless it can be reducted to a single-term product \(x \wedge y\) ). If not reducible to a single-term product, it can be interpreted only as a formal linear combination of two areas.
</p>
</div>
</div>
</div>

<div id="outline-container-Computing%20the%20dual%20basis" class="outline-3">
<h3 id="Computing%20the%20dual%20basis"><span class="section-number-3">3.8.</span> Computing the dual basis</h3>
<div class="outline-text-3" id="text-3-8">
<p>
Suppose \(\{v_1, ... , v_N\}\) is a given basis; we would like to compute its dual basis. For instance the covector \(v_1^\ast\) of the dual basis is a linear function such that 
</p>
\begin{equation*}
x = \sum_{i=1}^N x_i v_i; v_i^\ast(x) = x_i
\end{equation*}
<p>
We start from the observation that tensor \(\omega \equiv v_1 \wedge ... \wedge v_N\) is nonzero. The exterior product \(x \wedge v_2 \wedge ... \wedge v_N\) is qual to zero if \(x_1 = 0\). This suggest that the exterior product of \(x\) with the $(N-1)$-vector is quite similar to the covector \(v_1^\ast\). 
</p>

<p>
Lets introduce a special <b>complement</b> operation denoted by a star: 
\(*(v_1) \equiv v_2 \wedge ... \wedge v_N\) 
Then \(v_1^\ast(x) \omega = x \wedge *(v_1)\). This equation can be used to computing \(v_1^\ast\). 
</p>

<p>
For \(v_2\), if we would like to have \(x_2 \omega = x \wedge *(v_2)\) we need to add an extra minus sign and define 
</p>
\begin{equation*}
*(v_2) \equiv -v_1 \wedge v_3 \wedge ... \wedge v_N
\end{equation*}
<p>
It is clear that we can define the tensors \(*(v_i)\) for \(i=1,...,N\) in this way. The complement map \(*: V \to \wedge^{N-1} V\) satisfies \(v_j \wedge *(v_j) = \omega\) for each <i>basis</i> vector \(v_j\).
</p>

<p>
<b>Remark</b>: \(*(v_i)\) suggest that the complement operator is a some operation applied to \(v_i\) but it is not so. The complement of \(v_i\) depends on the entire basis and not merely on the single vector! Also, the property \(v_1 \wedge (v_1) = \omega\) is not sufficient to define the tensor \(*(v_1)\). 
</p>

<p>
<b>Remark</b>: When the vector space is equipped with a scalar product, on usually chooses an <i>orthonormal</i> basis to define the complement map; then the complement map is call <b>Hodge Star</b>. It turns outh that the Hodge star is indpendent of the choice of the basis as long as the basis is orthonormal with respect to the given scalar product, and as long as the orientation of the basis is unchanged (i.e. as long as the tensor \(\omega\) doesn't change sign). In other words, Hodge star operation is invariant under orthogonal and orientation-preserving transformations of the basis; 
</p>
</div>
</div>

<div id="outline-container-Gaussian%20elimination" class="outline-3">
<h3 id="Gaussian%20elimination"><span class="section-number-3">3.9.</span> Gaussian elimination</h3>
<div class="outline-text-3" id="text-3-9">
<p>
is a procedure for computing the exterior product of $n$-vectors.
</p>
</div>
</div>
<div id="outline-container-Rank%20of%20a%20set%20of%20vectors" class="outline-3">
<h3 id="Rank%20of%20a%20set%20of%20vectors"><span class="section-number-3">3.10.</span> Rank of a set of vectors</h3>
<div class="outline-text-3" id="text-3-10">
<p>
We have defined the rank of a map as the dimension of the image of the map, and we have seen that the rank is equal to the minimum number of tensor product terms needed to represent the map as a tensor. An analogous concept can be introduced for sets of vectors.
</p>

<p>
<b>Definition</b>: if \(S = \{v_1, ... ,v_n\}\) is a set of vectors (where \(n\) is not necessarily smaller than the dimension \(N\) of space), the rank of the set \(S\) is the dimension of the subspace spanned by the vectors in \(S\). 
</p>

\begin{equation*}
rank(S) = \dim Span S
\end{equation*}

<p>
We will now show how to use the exterior product for computing the rank of a given (finite) set \(S\). 
</p>

<p>
First, compute the tensor \(v_1 \wedge ... \wedge v_n\). If this tensor is non zero then the set S is linearly independent and the rank of \(S\) is equal to \(n\). 
</p>

<p>
If rank is less than \(n\). First assusem that all \(v_j \neq 0\) (i.e. any zero vectors can be omitted without changing the rank of \(S\)). Then we compute \(v_1 \wedge v_2\) ; if the result is zero, we may omit \(v_2\) and try \(v_1 \wedge v_3\). If it is not 0, we try \(v_1 \wedge v_2 \wedge v_3\) and so on. Eventually we arrive at a subset \(\{v_{i_1}, ..., v_{i_k}\} \subset S\) such that \(v_{i_1} \wedge ... \wedge v_{i_k} \neq 0\) but \(v_{i_1} \wedge ... \wedge v_{i_k} \wedge v_j = 0\) for any other \(v_j\). Then the rank of \(S\) is equal to \(k\). 
</p>
</div>
</div>

<div id="outline-container-Exterior%20product%20in%20index%20notation" class="outline-3">
<h3 id="Exterior%20product%20in%20index%20notation"><span class="section-number-3">3.11.</span> Exterior product in index notation</h3>
<div class="outline-text-3" id="text-3-11">
<p>
Let us choose a basis \(\{e_j\}\) in \(V\); then the dual basis \(\{e^\ast_j\}\) in \(V\) and the basis \(\{e_{k_1} \wedge ... \wedge e_{k_m} \}\)  in \(\wedge^m V\)  are fixed. 
</p>


<p>
Then exterior product \(A \equiv u \wedge v\) is written in index notation as \(A^{ij} = u^iv^j - u^jv^i\). Not that the matrix \(A^{ij} = - A^{ji}\) is antisymmetric. 
</p>

<p>
Another example: The 3-vector \(u\wedge v \wedge w\) can be expanded in the basis as 
v\begin{equation}
u&and; v &and; w = &sum;<sub>i,j,k=1</sub><sup>N</sup> B<sup>ijk</sup>e<sub>i</sub> &and; e<sub>j</sub> &and; e<sub>k</sub>
\end{equation}
</p>

<p>
And the components \(B^{ijk}\) would be given by
</p>
\begin{equation}
B^{ijk} = u^i v^j w^k - u^i v^k w^j + ... 
\end{equation}
<p>
where every premutation of the set \((i,j,k)\) of indices enters with the sign corresponding to the parity of that premutation. 
</p>

<p>
<b>Remark</b>: The `three-dimensional array' \(B^{ijk}\) is antisymmetici to any pair of indices. Such arrays are called totally antisymmetric. 
</p>

<p>
The formula for the components \(B^{ijk}\) of \(u\wedge v \wedge w\) is not particularly convenient, so lets rewrite it in suitable form for generalization. 
</p>

<p>
Let us first consider the exterior product of three vectors as a map \(\hat{E} : V \otimes V \otimes V \to \wedge^3 V\)
This map is linear and can be represented, in the index notation as 
</p>
\begin{equation*}
(u \wedge v \wedge w)^{ijk} = E^{ijk}_{lmn} u^l v^m w^n
\end{equation*}
<p>
where the array \(E^{ijk}_{lmn}\) is the component representation of the map \(E\).
</p>

<p>
By analogy, the map \(\hat{E} : V \otimes ... \otimes V \to \wedge^n V\) can be represent in the index notation by the array of components \(E^{i_1...i_n}_{j_1...j_n}\). This array is totally antisymmetric with respect to all the indices \(\{i_s\}\) and separately with respect to all \(\{j_s\}\).  
</p>

<p>
Using this array, the exterior product of two general antisymmetric tensors, say \(\phi \in \wedge^m V\) and \(\psi \in \wedge^n V\) such that \(m+n \leq N\) can be represented in the index notation by 
</p>
\begin{equation}
(\phi \wedge \psi)^{i_1...i_{m+n}} = \frac 1 {m!n!} \sum_{(j_s,k_s)} E^{i_1...i_{m+n}}_{j_1..j_m k_1...k_n} \phi^{j_1...j_m}\psi^{k_1...k_n}
\end{equation}
<p>
The combinatorial factor \(m!n!\) is needed to compensate from the \(m!\) equal terms arising from the summation over \((j_1,...,j_m)\) due to the fact that \(\phi^{j_1...j_m}\) is totally antisymmetric and similarly form the \(n!\) equal terms.
</p>

<p>
The general formula for \(E^{i_1...i_n}_{j_1...j_n}\) is that it is \((-1)^{|\sigma|}\) when \((i_1,...,i_n)\) is a permutation \(\sigma\) of \((j_1,...,j_n)\) and is \(0\) otherwise. 
</p>

<p>
The <b>Levi-Civita symbol</b> is defined as a totally antisymmetric array with \(N\) indices, whose values are \(0\) or \(\pm 1\) according to the formula 
</p>

\begin{equation}
\epsilon^{i_1...i_N} = (-1)^{|\sigma|} \text{if $(i_1,...,i_n)$ is a permutation $\sigma$ of $(1,...,N)$; 0 otherwise}
\end{equation}

<p>
Depending on convenicence, we may write \(\epsilon\) with upper or lower indices since \(\epsilon\) is just an array of numbers. 
</p>

<p>
Notice that 
</p>
\begin{equation*}
\epsilon^{i_1...i_N} = E^{i_1...i_N}_{1...N}
\end{equation*}
<p>
Lets consider the expression 
</p>
\begin{equation}
\label{orgf2ab3de}
\tilde{E}^{i_1...i_n}_{j1...j_n} \equiv \sum_{k1,...,k_{N-n}} \epsilon^{i_1...i_nk_1...k_{N-n}} \epsilon_{j_1...j_nk_1...k_{N-n}}
\tag{eqn:2.11}
\end{equation}

<p>
This expression has \(2n\) free indices and is totally antisymmetric in these free indices (since \(\epsilon\) is totally antisymmetic in all indices).
</p>

<p>
<b>Statement</b>: The exterior product operator \(E^{i_1...i_n}_{j_1...j_n}\) is expressed through Levi-Civita symbol as 
</p>


\begin{equation}
E^{i_1...i_n}_{j_1...j_n} = \frac 1 {(N-n)!} \tilde{E}^{i_1...i_n}_{j_1...j_n}
\end{equation}

<p>
<b>Proof</b>: Let us compare the values of \(E^{i_1...i_n}_{j_1...j_n}\) and \(\tilde{E}^{i_1...i_n}_{j_1...j_n}\) where the indices have some fixed values. There are two cases either the set \((i_1,...,i_n)\) is a permutation of the set \((j_1,...,j_n)\); in that case we may denote this peruation by \(\sigma\); or it is not a permutation. 
</p>

<p>
Considering the case when a permuation \(\sigma\) brings \((j_1,...,j_n)\) into \((i_1,...,i_n)\), we find that the symbols \(\epsilon\) in \eqref{orgf2ab3de} will be nonzero only if the indices \((k_1,...,k_{N-n})\) are a permuatation of the complemnt of the set \((i_1,...,i_n)\). There are \((N-n)!\) such permutations, each contributing the same value to the sum in \eqref{orgf2ab3de}. Hence we may write the sum as 
</p>

\begin{align}
\tilde{E}^{i_1...i_n}_{j1...j_n} &\equiv (N-n)! \epsilon^{i_1...i_nk_1...k_{N-n}} \epsilon_{j_1...j_nk_1...k_{N-n}}
\text{no implicit sums!} \\ 
&= (N-n)! (-1)^{|\sigma|}
\end{align}
<p>
where the indices \({k_s}\) are chosen such that the values of \(\epsilon\) are nonzero. 
Thus the required formula is valid for first case. 
In case the permuatation \(\sigma\) doesn't exist, we note that 
</p>
\begin{equation}
\tilde{E}^{i_1...i_n}_{j_1...j_n} = 0
\end{equation}
<p>
And therefore \(E\) and \(\tilde{E}\) both will be equal to zero.
</p>

<p>
Note that the formula for the top exterior power \((n=N)\) is simple and involves no summation 
</p>
\begin{equation*}
E^{i_1...i_N}_{j_1...j_N} = \epsilon^{i_1...i_N}\epsilon_{j_1...j_N}
\end{equation*}

<p>
<b>Remark</b>: As a rule, a summation of the Levi-Civita symbol \(\epsilon\) with any anitsymmetric tensor (e.g. another \(\epsilon\)) gives rise to a combinatorial factor \(n!\) when the summation goes over \(n\) indices. 
</p>
</div>
</div>

<div id="outline-container-Exterior%20Algebra%20%28Grassmann%20Algebra%29" class="outline-3">
<h3 id="Exterior%20Algebra%20%28Grassmann%20Algebra%29"><span class="section-number-3">3.12.</span> Exterior Algebra (Grassmann Algebra)</h3>
<div class="outline-text-3" id="text-3-12">
<p>
The formalism of exterior algebra is used e.g. in physical theories of quantum fermionic fields and supersymmetry. 
</p>

<p>
<b>Definition</b>: An <b>algebra</b> is a vector space with a <i>distributive multiplication</i>. 
</p>

<p>
In other words, \(A\) is an algebra if its a vector space over a field \(\mathbb K\) and if for any \(a,b \in A\) their product \(ab \in A\) is defined, such taht the product is distributive over addition and \(\lambda (ab) = (\lambda a)b = a (\lambda b)\) for \(\lambda \in \mathbb K\). 
</p>

<p>
<b>Examples of Algerbas</b>:
</p>
<ol class="org-ol">
<li>All \(N \times N\) matrices with coefficients from \(mathbb K\) are $N<sup>2</sup>$-dimensional algebra.</li>
<li>The filed \(\mathbb K\) is a one-dimensional algerba over itself. This algebra is commutative.</li>
</ol>

<p>
<b>Statement</b>: If \(\omega \in \wedge^m V\) then we can define the map \(L_{\omega}: \wedge^k V \to \wedge^{k+m} V\) by the formula 
</p>
\begin{equation*}
L_{\omega} (v_1 \wedge ... \wedge v_k) = \omega \wedge v_1 \wedge ... \wedge v_k
\end{equation*}

<p>
For elements of \(\wedge^0V \equiv \mathbb K\) we define \(L_\lambda \omega \equiv \lambda \omega\) and also, \(L_\omega \lambda \equiv \lambda \omega\) for \(\lambda \in \mathbb K\). Then the map \(L_\omega\) is linear for any \(\omega \in \wedge^m V\), \(0 \leq m \leq N\) 
</p>

<p>
<b>Definition</b>: The <b>exterior algebra</b> (also called the <b>Grassmann algebra</b>) based on a vector space \(V\) is the space \(\wedge V\) defind as the direct sum, 
</p>

\begin{equation*}
\wedge V \equiv \mathbb K \oplus V \oplus \wedge^2 V \oplus ... \oplus \wedge^N V
\end{equation*}

<p>
with the multiplication defined by the map \(L\), which is exteded to the whole of \(\wedge V\) by linearity. 
</p>
</div>
</div>
</div>
<div id="outline-container-Basic%20Applications" class="outline-2">
<h2 id="Basic%20Applications"><span class="section-number-2">4.</span> Basic Applications</h2>
<div class="outline-text-2" id="text-4">
</div>
<div id="outline-container-Determinants%20through%20permuatations%3A%20The%20hard%20way" class="outline-3">
<h3 id="Determinants%20through%20permuatations%3A%20The%20hard%20way"><span class="section-number-3">4.1.</span> Determinants through permuatations: The hard way</h3>
<div class="outline-text-3" id="text-4-1">
<p>
<b>Definition D0</b>: The <i>determinant</i> of a square \(N \times N\) matrix \(A_{ij}\) is the number 
</p>
\begin{equation*}
\label{org2a00dda}
det(A_{ij}) \equiv \sum_{\sigma} (-1)^{|\sigma|} A_{\sigma(1)1} ... A_{\sigma(N)N} \tag{eqn:3.1}
\end{equation*}

<p>
where the summation goes over all permutations \(\sigma : (1, ..., N) \to (k_1, ..., k_N)\), and the parity function \(|\sigma|\) is equal to \(0\) if the permutation \(\sigma\) is even and to \(1\) if it is odd. 
</p>

<p>
If a \(2 \times 2\) matrix, there are only two permutations of the set \((1,2)\), namely \((1,2)\) and \((2,1)\) so, the explicit formula for determinant is:
</p>

\begin{equation*}
a_{11}a_{22} - a_{21}a_{12}
\end{equation*}

<p>
Note that an \(N \times N\) matrix has \(N!\) terms in this type of formula. 
</p>
</div>

<div id="outline-container-Question" class="outline-4">
<h4 id="Question"><span class="section-number-4">4.1.1.</span> Question</h4>
<div class="outline-text-4" id="text-4-1-1">
<p>
To me, definition D0 seems unmotivated and strange. It is not clear why this complicated combination of matrix elements has any useful properties at all. Even if so then maybe there exists another complicated combination of matrix elements that is even more useful?  
</p>

<p>
Answer: Yes, indeed: There exist other complicated combinations that are also useful. All this is best understood if we do not begin by studying the definition (3.1). Instead, we will proceed in a coordinate-free manner and build upon geometric intuition.  We will interpret the matrix \(A_{ij}\) not as a â€œtable of numbersâ€ but as a coordinate representation of a linear transformation \(\hat{A}\)  in some vector space \(V\)  with respect to some given basis. We will define an action of the operator \(\hat{A}\)  on the exterior product space \(\wedge^N V\) in a certain way. That action will allow us to understand the properties and the uses of determinants without long calculations.
</p>
</div>
</div>

<div id="outline-container-Derivation%20of%20the%20Leibniz%20Formula%20for%20Determinants" class="outline-4">
<h4 id="Derivation%20of%20the%20Leibniz%20Formula%20for%20Determinants"><span class="section-number-4">4.1.2.</span> Derivation of the Leibniz Formula for Determinants</h4>
<div class="outline-text-4" id="text-4-1-2">
<p>
See this video on the derivation of the derivation of the \eqref{org2a00dda} <a href="https://www.youtube.com/watch?v=Sv7VseMsOQc">https://www.youtube.com/watch?v=Sv7VseMsOQc</a>
</p>

<p>
Basic sketch of the proof is as follows:
Thinkinging the matrix representation of the transformation, determinants have some properties:
</p>
<ol class="org-ol">
<li>They are linear on the column vectors</li>
<li>If two column vectors are same/colinear, then the determinant is zero</li>
</ol>

<p>
Use these property, we can breakdown the determinant of a matrix interms of sum of determinants of matrices with only basis vectors in the columns using the distributive property (1). This results in a sum over all possible combinations of column vectors, but by property (2) if the column vectors are same, the determinant is zero. Thus only the permutations of the column vectors remain (i.e. no column vector is repeated).
</p>
</div>
</div>
</div>

<div id="outline-container-The%20space%20%24%5Cwedge%5EN%20V%24%20and%20oriented%20volume" class="outline-3">
<h3 id="The%20space%20%24%5Cwedge%5EN%20V%24%20and%20oriented%20volume"><span class="section-number-3">4.2.</span> The space \(\wedge^N V\) and oriented volume</h3>
<div class="outline-text-3" id="text-4-2">
<p>
The space \(\wedge^N V\) is one dimensional. Therefore, all nonzero tensors from \(\wedge^N V\) are proportional to each other. 
</p>

<p>
<b>Example</b>: In two dimensional space \(V\) lets choose a basis \({e_1, e_2}\) and cosider two arbitrary vectors \(v_1\) and \(v_2\). \(v_1 = a_{11} e_1 + a_{12} e_2\), Then 
</p>

\begin{equation*}
v_1 \wedge v_2 = (a_{11} a_{22} - a_{12} a_{21}) e_1 \wedge e_2
\end{equation*}

<p>
We may observe that firstly, the 2-vector \(v_1 \wedge v_2\) is proportional to \(e_1 \wedge e_2\) and secondly, the proportionality coefficient is equal to the determinant of matrix \(A_{ij}\) 
</p>

<p>
<b>Lemma</b>: In an $N$-dimensional space
</p>
<ol class="org-ol">
<li>The volume of parallelepiped spanned by \(\{\lambda v_1, v_2, ..., v_N\}\) is \(\lambda\) times greater than that of \(\{v_1,v_2,...,v_N\}\) .</li>

<li>Two parallelepipeds spanned by the sets of vectors \(\{v_1,v_2,...,v_N\}\) and \(\{v_1+\lambda v_2, v_2, ..., v_N\}\) have equal volume.</li>
</ol>

<p>
<b>Statement</b>: Consider an $N$-dimensional space \(V\), Then: 
</p>
<ol class="org-ol">
<li>Two parallelepipeds spanned by the sets of vectors \(\{u_1, ..., u_n\}\) and \(\{v_1,v_2,...,v_N\}\) have equal volumes if and only if the corresponding tensors from \(\wedge^N V\) are equal upto a sign,</li>
</ol>
\begin{equation*}
u_1 \wedge ... \wedge u_N = \pm v_1 \wedge ... \wedge v_N
\end{equation*}

<ol class="org-ol">
<li>If \(u_1 \wedge ... \wedge u_N = \lambda v_1 \wedge ... \wedge v_N\) , where \(\lambda \in \mathbb K\) is a number, \(\lambda \neq 0\), then the volumes of the two parallelepipeds differ by a factor of \(|\lambda|\).</li>
</ol>

<p>
<b>Proof of Statement 1</b>: We can transform from the first basis \(\{u_1, ..., u_n\}\) into the second basis \(\{v_1, ... , v_N\}\) (upto ordering of vectors) by a sequence of transformations of two types: either we multiply one of the vectors \(u_j\) by a number \(\lambda\) or we add \(\lambda u_j\) to another vector \(u_k\). Using above lemmas and properties of exterior product we can show that these transformation change the volume in the same way as the exterior product \(u_1 \wedge ... \wedge u_N\) changes. 
</p>


<p>
A tensor such as \(v_1 \wedge ... \wedge v_n\) can be used to determine the numerical value of the volume only if we can compare it to another given tensor, \(u_1 \wedge ... \wedge u_N\), which by <i>assumption</i> corresponds to a prallelepiped of unit volume. A choice of a <i>reference</i> tensor \(u_1 \wedge ... \wedge u_N\) can be made, for instance, if we are given a basis in \(V\). Thus there is no natural map from \(\wedge^N V\) to \(\mathbb K\). In other words the sapce \(\wedge^N V\)  is <i>not canonically isomorphic</i> to the space \(\mathbb K\). 
</p>

<p>
<b>Remark</b>: When a scalar product is defined in \(V\), there is a preffered choice of basis, namely an orthonormal basis \(\{e_j\}\) such that \(<e_i,e_j> = \delta_{ij}\) 
</p>
</div>
</div>

<div id="outline-container-Determinants%20of%20operators" class="outline-3">
<h3 id="Determinants%20of%20operators"><span class="section-number-3">4.3.</span> Determinants of operators</h3>
<div class="outline-text-3" id="text-4-3">
<p>
Let \(\hat{A} \in \text{End}V\) be a linear operator. Consider this action on tesnors from the sapce \(\wedge^N V\) defiend in the following way, \(v_1 \wedge ... \wedge v_N \to \hat{A} v_1 \wedge ... \wedge \hat{A} v_N\). I denote this operation by \(\wedge^N \hat{A}^N\). 
</p>

<p>
We just defined \(\wedge^N \hat{A}^N\) on single term products; the action of \(\wedge^N \hat{A}^N\) on linear combinations of such products is obtained by requiring linearity. 
</p>

<p>
<b>Homework</b>: Verify \(\wedge^N \hat{A}^N\) is a linear map. 
</p>

<p>
Since, \(\wedge^N V\) is one-dimensional. So, \(\wedge^N \hat{A}^N\) being a linear operator in a one-dimensional space, must act simply as multiplication by a number. Thus we can write 
</p>

\begin{equation*}
\wedge^N \hat{A}^N = \alpha \hat{1}_{\wedge^N V}
\end{equation*}

<p>
where \(\alpha \in \mathbb K\) is a number which is somehow associated with the oprator \(\hat{A}\). This number is actually equal to the <i>determinant</i> of the operator \(\hat A\) as given by <i>Definition D0</i>. 
</p>

<p>
<b>Definition D2</b>: The determinant \(det{\hat A}\) of a linear transformation \(\hat A\) is the nuber by with the <i>oriented</i> volume of any parallelepiped grows after the transformation. 
</p>

<p>
<b>Statement 3</b>: If \(\{e_j\}\) is any basis in \(V\), \(\{e_j^\ast\}\) is the dual basis, and a linear operator \(\hat A\) is represented by a tensor, 
</p>

\begin{equation*}
\hat A = \sum_{j,k=1}^N A_{jk} e_j \otimes e_k^\ast
\end{equation*}

<p>
then the determinant of \(\hat A\) is given by the formula \eqref{org2a00dda}
</p>

<p>
<b>Proof</b>: The action of \(\wedge^N \hat{A}^N\) on the basis element \(e_1 \wedge ... \wedge e_N \in \wedge^NV\) is 
</p>

\begin{align}
&\wedge^N \hat{A}^N (e_1 \wedge ... \wedge e_N)  \\ 
&= \hat A e_1 \wedge ... \wedge \hat A e_N  \\ 
&= \big(\sum_{j_1 = 1}^N A_{j_1 1} e_{j_1}\big)  \wedge ... \wedge \big( \sum_{j_N = 1}^N A_{j_N N}e_{j_N} \big) \\ 
&= \sum_{j_1} ... \sum_{j_N}(A_{j_1 1} ... A_{j_N N}) e_{j_1} \wedge ... \wedge e_{j_N}
\end{align}

<p>
In the last sum, the only nonzero terms are those in which the indices \(j_1, ... , j_N\) do not repeat; in other words, \((j_1,...,j_N)\) is a <i>permutation</i> of the set \((1,...,N)\). From the antisymmetry of the exterior product and the definition of the parity of the permutation, we can express above expression as in \eqref{org2a00dda}
</p>
</div>

<div id="outline-container-Leibnitz%27s%20rule" class="outline-4">
<h4 id="Leibnitz%27s%20rule"><span class="section-number-4">4.3.1.</span> Leibnitz's rule</h4>
</div>
</div>
</div>
<div id="outline-container-Advanced%20Applications" class="outline-2">
<h2 id="Advanced%20Applications"><span class="section-number-2">5.</span> Advanced Applications</h2>
</div>
<div id="outline-container-Scalar%20Product" class="outline-2">
<h2 id="Scalar%20Product"><span class="section-number-2">6.</span> Scalar Product</h2>
<div class="outline-text-2" id="text-6">
<hr />
<h3>References</h3>

<ul class="org-ul">
<li><a href="https://www.youtube.com/watch?v=Sv7VseMsOQc">https://www.youtube.com/watch?v=Sv7VseMsOQc</a></li>
<li><a href="https://en.wikipedia.org/wiki/Homomorphism">https://en.wikipedia.org/wiki/Homomorphism</a></li>
</ul>
<hr />
<h3>Backlinks</h3>

<ul class="org-ul">
<li><a href="geometric_algebra.html#ID-A6FFDB08-85C0-450A-950D-CDC89B763974">Geometric Algebra</a></li>
</ul>
</div>
</div>
</div>
<div id="postamble" class="status">
<hr/>You can send your feedback, queries <a href="mailto:bpanthi977@gmail.com?subject=Feedback: Linear Algebra via Exterior Products">here</a><span id="visits"></span><span id="claps"></span><div id="claps-message"></div><a href="https://bpanthi977.github.io/braindump/data/rss.xml"><img src="https://bpanthi977.github.io/braindump/data/rss.png" /></a>
</div>
</body>
</html>
