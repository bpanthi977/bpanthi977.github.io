<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Grokking</title>
<meta name="author" content="Bibek Panthi" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="../css/stylesheet.css" />
<link rel="stylesheet" type="text/css" href="../css/braindump.css" />
<script src="../js/counters.js" type="text/javascript"></script>
<script src="../js/URI.js" type="text/javascript"></script>
<script src="../js/pages.js" type="text/javascript"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="./index.html"> UP </a>
 |
 <a accesskey="H" href="../index.html"> HOME </a>
</div><div id="preamble" class="status">
<p class="date">Date: <span class="timestamp-wrapper"><span class="timestamp">&lt;2024-10-02 Wed&gt;</span></span></p>
</div>
<div id="content" class="content">
<h1 class="title">Grokking
<br />
<span class="subtitle">Generalization beyond Overfitting</span>
</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#Definition%20of%20Grokking">1. Definition of Grokking</a>
<ul>
<li><a href="#Jargon%20file">1.1. Jargon file</a></li>
<li><a href="#Grokking">1.2. Grokking</a></li>
</ul>
</li>
<li><a href="#Setup">2. Setup</a>
<ul>
<li><a href="#Dataset%20of%20Binary%20operation%20tables">2.1. Dataset of Binary operation tables</a></li>
<li><a href="#Model%20%26%20Training">2.2. Model &amp; Training</a></li>
</ul>
</li>
<li><a href="#Why%20algorithmic%20dataset%3F">3. Why algorithmic dataset?</a></li>
<li><a href="#Observations">4. Observations</a>
<ul>
<li><a href="#Smaller%20Dataset%20%3D%3E%20More%20steps%20required%20%5B%5B%5Bpdf%3Apapers%2FGrokking%20-%20Generalization%20Beyond%20Overfitting%20on%20Small%20Algorithmic%20Datasets%20-%202201.02177v1.pdf%3A%3A1%2B%2B0.38%3B%3Bannot-1-5%5D%5Bpg.%201%5D%5D%5D">4.1. Smaller Dataset =&gt; More steps required [pg. 1]</a></li>
<li><a href="#Weight%20Decay%20helps">4.2. Weight Decay helps</a></li>
<li><a href="#Adding%20some%20noise%20helps">4.3. Adding some noise helps</a></li>
<li><a href="#Authors%20believe%20grokking%20to%20be%20Architecture%20Agnostic">4.4. Authors believe grokking to be Architecture Agnostic</a></li>
</ul>
</li>
<li><a href="#Why%20does%20this%20happen%3F">5. Why does this happen?</a>
<ul>
<li><a href="#ID-94D39FB1-6C13-494C-A953-7DC0BA07755A">5.1. Noise drives model to flatter/simpler solutions</a></li>
<li><a href="#Some%20evidence">5.2. Some evidence</a></li>
</ul>
</li>
</ul>
</div>
</div>
<p>
source: Grokking - Generalization Beyond Overfitting on Small Algorithmic Datasets - 2201.02177v1.pdf
[<a href="papers/Grokking - Generalization Beyond Overfitting on Small Algorithmic Datasets - 2201.02177v1.pdf">file:</a>][<a href="papers/Grokking - Generalization Beyond Overfitting on Small Algorithmic Datasets - 2201.02177v1.pdf#page=nil">pdf:</a>][<a href="https://arxiv.org/abs/2201.02177">ArXiv</a>]
</p>
<div id="outline-container-Definition%20of%20Grokking" class="outline-2">
<h2 id="Definition%20of%20Grokking"><span class="section-number-2">1.</span> Definition of Grokking</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-Jargon%20file" class="outline-3">
<h3 id="Jargon%20file"><span class="section-number-3">1.1.</span> Jargon file</h3>
<div class="outline-text-3" id="text-1-1">
<p>
Jargon file (aka The Hacker's Dictionary) <a href="http://catb.org/jargon/html/G/grok.html">says</a>:
</p>

<blockquote>
<p>
When you claim to "grok" some knowledge or technique, you are asserting that you have not merely learned it in a detached instrumental way but that it has become part of you, part of your identity.
</p>
</blockquote>
</div>
</div>
<div id="outline-container-Grokking" class="outline-3">
<h3 id="Grokking"><span class="section-number-3">1.2.</span> Grokking</h3>
<div class="outline-text-3" id="text-1-2">
<blockquote>
<p>
We show that, long after severely overfitting, validation accuracy sometimes suddenly begins to increase from chance level toward perfect generalization. We call this phenomenon ‘grokking’ [<a href="papers/Grokking - Generalization Beyond Overfitting on Small Algorithmic Datasets - 2201.02177v1.pdf#page=2">pg. 2</a>]
</p>
</blockquote>


<div id="figure-1" class="figure">
<p><img src="data/grokking/grokking-20241002020118.png" alt="grokking-20241002020118.png" />
</p>
<p><span class="figure-number">Figure 1: </span>Grokking</p>
</div>
</div>
</div>
</div>
<div id="outline-container-Setup" class="outline-2">
<h2 id="Setup"><span class="section-number-2">2.</span> Setup</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="outline-container-Dataset%20of%20Binary%20operation%20tables" class="outline-3">
<h3 id="Dataset%20of%20Binary%20operation%20tables"><span class="section-number-3">2.1.</span> Dataset of Binary operation tables</h3>
<div class="outline-text-3" id="text-2-1">
<p>
The datasets we consider are binary operation tables of the form a ◦ b = c where a, b, c are discrete symbols with no internal structure, and ◦ is a binary operation. [<a href="papers/Grokking - Generalization Beyond Overfitting on Small Algorithmic Datasets - 2201.02177v1.pdf#page=2">pg. 2</a>]
</p>

<p>
(See <a href="papers/Grokking - Generalization Beyond Overfitting on Small Algorithmic Datasets - 2201.02177v1.pdf#page=6">Appendix A.1.1</a> for full list)
</p>


<div id="figure-2" class="figure">
<p><img src="data/grokking/dataset-20241002063256.png" alt="dataset-20241002063256.png" />
</p>
</div>
</div>
</div>
<div id="outline-container-Model%20%26%20Training" class="outline-3">
<h3 id="Model%20%26%20Training"><span class="section-number-3">2.2.</span> Model &amp; Training</h3>
<div class="outline-text-3" id="text-2-2">
<ul class="org-ul">
<li>A standard decoder-only transformer with 2 layers, (about \(4\times10^5\) parameters) [<a href="papers/Grokking - Generalization Beyond Overfitting on Small Algorithmic Datasets - 2201.02177v1.pdf#page=6">pg. 6</a>]</li>
<li>AdamW optimizer</li>
<li>Optimization budget of 10<sup>5</sup> gradient updates</li>
<li>Simple Hyperparmaters based on performance in S5 [<a href="papers/Grokking - Generalization Beyond Overfitting on Small Algorithmic Datasets - 2201.02177v1.pdf#page=6">pg. 6</a>]
<ul class="org-ul">
<li>Weight decays (good perf. on S5)</li>
<li>No annealing of learning rate (for simplicity)</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-Why%20algorithmic%20dataset%3F" class="outline-2">
<h2 id="Why%20algorithmic%20dataset%3F"><span class="section-number-2">3.</span> Why algorithmic dataset?</h2>
<div class="outline-text-2" id="text-3">
<ul class="org-ul">
<li>As difficulty of problem increases the generalization phenomenon is seen after very long optimization</li>
<li>Easy to use. Can be trained on single GPU</li>
</ul>

<blockquote>
<p>
We argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the finite training dataset. [<a href="papers/Grokking - Generalization Beyond Overfitting on Small Algorithmic Datasets - 2201.02177v1.pdf#page=1">pg. 1</a>]
</p>
</blockquote>
</div>
</div>
<div id="outline-container-Observations" class="outline-2">
<h2 id="Observations"><span class="section-number-2">4.</span> Observations</h2>
<div class="outline-text-2" id="text-4">
</div>
<div id="outline-container-Smaller%20Dataset%20%3D%3E%20More%20steps%20required%20%5B%5B%5Bpdf%3Apapers%2FGrokking%20-%20Generalization%20Beyond%20Overfitting%20on%20Small%20Algorithmic%20Datasets%20-%202201.02177v1.pdf%3A%3A1%2B%2B0.38%3B%3Bannot-1-5%5D%5Bpg.%201%5D%5D%5D" class="outline-3">
<h3 id="Smaller%20Dataset%20%3D%3E%20More%20steps%20required%20%5B%5B%5Bpdf%3Apapers%2FGrokking%20-%20Generalization%20Beyond%20Overfitting%20on%20Small%20Algorithmic%20Datasets%20-%202201.02177v1.pdf%3A%3A1%2B%2B0.38%3B%3Bannot-1-5%5D%5Bpg.%201%5D%5D%5D"><span class="section-number-3">4.1.</span> Smaller Dataset =&gt; More steps required [<a href="papers/Grokking - Generalization Beyond Overfitting on Small Algorithmic Datasets - 2201.02177v1.pdf#page=1">pg. 1</a>]</h3>
<div class="outline-text-3" id="text-4-1">
<ul class="org-ul">
<li>For larger dataset sizes, the training and validation curves tend to track each other more closely. [<a href="papers/Grokking - Generalization Beyond Overfitting on Small Algorithmic Datasets - 2201.02177v1.pdf#page=2">pg. 2</a>]</li>
</ul>



<div id="figure-3" class="figure">
<p><img src="data/grokking/smaller_dataset_require_higher_steps_for_generalization-20241002064500.png" alt="smaller_dataset_require_higher_steps_for_generalization-20241002064500.png" />
</p>
<p><span class="figure-number">Figure 2: </span>Smaller dataset require higher steps for generalization</p>
</div>
</div>
</div>
<div id="outline-container-Weight%20Decay%20helps" class="outline-3">
<h3 id="Weight%20Decay%20helps"><span class="section-number-3">4.2.</span> Weight Decay helps</h3>
<div class="outline-text-3" id="text-4-2">
<blockquote>
<p>
We compare various optimization details to measure their impact on data efficiency. We find that weight decay is particularly effective at improving generalization on the tasks we study. [<a href="papers/Grokking - Generalization Beyond Overfitting on Small Algorithmic Datasets - 2201.02177v1.pdf#page=2">pg. 2</a>]
</p>
</blockquote>

<blockquote>
<p>
We found that weight decay towards the initialization of the network is also effective, but not quite as effective as weight decay towards the origin [<a href="papers/Grokking - Generalization Beyond Overfitting on Small Algorithmic Datasets - 2201.02177v1.pdf#page=4">pg. 4</a>]
</p>
</blockquote>
</div>
</div>
<div id="outline-container-Adding%20some%20noise%20helps" class="outline-3">
<h3 id="Adding%20some%20noise%20helps"><span class="section-number-3">4.3.</span> Adding some noise helps</h3>
<div class="outline-text-3" id="text-4-3">
<blockquote>
<p>
Adding some noise to the optimization process, e.g.
</p>
<ul class="org-ul">
<li>gradient noise from using minibatches,</li>
<li>Gaussian noise applied to weights before or after computing the gradients</li>
</ul>
<p>
is beneficial for generalization, consistent with the idea that such noise might induce the optimization to find flatter minima that generalize better [<a href="papers/Grokking - Generalization Beyond Overfitting on Small Algorithmic Datasets - 2201.02177v1.pdf#page=4">pg. 4</a>]
</p>
</blockquote>
</div>
</div>
<div id="outline-container-Authors%20believe%20grokking%20to%20be%20Architecture%20Agnostic" class="outline-3">
<h3 id="Authors%20believe%20grokking%20to%20be%20Architecture%20Agnostic"><span class="section-number-3">4.4.</span> Authors believe grokking to be Architecture Agnostic</h3>
<div class="outline-text-3" id="text-4-4">
<blockquote>
<p>
In contrast we study the phenomenon of generalization in data-limited regime, with an emphasis on phenomena that we believe to be architecture-agnostic. [<a href="papers/Grokking - Generalization Beyond Overfitting on Small Algorithmic Datasets - 2201.02177v1.pdf#page=7">pg. 7</a>]
</p>
</blockquote>

<ul class="org-ul">
<li>Later other papers show it to be architecture agnostic and dataset agnostic</li>
</ul>

<blockquote>
<p>
Liu et al. [2022] have shown that similar artifacts are observed for various model architectures trained with a variety of datasets, including images, languages, and graphs. [<a href="papers/Grokfast - Accelerated Grokking by Ampilfying Slow Gradients - 2405.20233v2.pdf#page=1">Grokfast - pg. 1</a>]
</p>
</blockquote>
</div>
</div>
</div>
<div id="outline-container-Why%20does%20this%20happen%3F" class="outline-2">
<h2 id="Why%20does%20this%20happen%3F"><span class="section-number-2">5.</span> Why does this happen?</h2>
<div class="outline-text-2" id="text-5">
</div>
<div id="outline-container-ID-94D39FB1-6C13-494C-A953-7DC0BA07755A" class="outline-3">
<h3 id="ID-94D39FB1-6C13-494C-A953-7DC0BA07755A"><span class="section-number-3">5.1.</span> Noise drives model to flatter/simpler solutions</h3>
<div class="outline-text-3" id="text-5-1">
<blockquote>
<p>
We conjectured that the grokking phenomena we report in this work may be due to the noise from SGD driving the optimization to flatter/simpler solutions that generalize better and hope to investigate in future work whether any of these measures are predictive of grokking. [<a href="papers/Grokking - Generalization Beyond Overfitting on Small Algorithmic Datasets - 2201.02177v1.pdf#page=9">pg. 9</a>]
</p>
</blockquote>



<div id="figure-4" class="figure">
<p><img src="data/grokking/flatness_of_loss-20241002070926.png" alt="flatness_of_loss-20241002070926.png" />
</p>
<p><span class="figure-number">Figure 4: </span>Flatness of Loss [Source: <a href="https://arxiv.org/abs/1609.04836">On Large-Batch Training For Deep Learning: Generalization Gap And Sharp Minima</a>]</p>
</div>
</div>
</div>
<div id="outline-container-Some%20evidence" class="outline-3">
<h3 id="Some%20evidence"><span class="section-number-3">5.2.</span> Some evidence</h3>
<div class="outline-text-3" id="text-5-2">
<ul class="org-ul">
<li>Sharpness of minimum seems to predict generalization on \(S_5\) (they are correlated) [<a href="papers/Grokking - Generalization Beyond Overfitting on Small Algorithmic Datasets - 2201.02177v1.pdf#page=10">pg. 10</a>]</li>

<li>So, Grokking may only happen after the network’s parameters are in flatter regions of the loss landscape. [<a href="papers/Grokking - Generalization Beyond Overfitting on Small Algorithmic Datasets - 2201.02177v1.pdf#page=10">pg. 10</a>]</li>
</ul>


<hr />
<h3>Backlinks</h3>

<ul class="org-ul">
<li><a href="batching.html#ID-3C21992B-BFF7-4D5A-BDC5-64C1B8812478">Batching</a></li>
</ul>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<hr/>You can send your feedback, queries <a href="mailto:bpanthi977@gmail.com?subject=Feedback: Grokking">here</a><span id="visits"></span><span id="claps"></span><div id="claps-message"></div>
</div>
</body>
</html>
