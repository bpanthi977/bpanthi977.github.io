<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Eliezer Yudkowsky</title>
<meta name="author" content="Bibek Panthi" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="../css/stylesheet.css" />
<link rel="stylesheet" type="text/css" href="../css/braindump.css" />
<script src="../js/counters.js" type="text/javascript"></script>
<script src="../js/URI.js" type="text/javascript"></script>
<script src="../js/pages.js" type="text/javascript"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="./index.html"> UP </a>
 |
 <a accesskey="H" href="../index.html"> HOME </a>
</div><div id="preamble" class="status">
<p class="date">Date: <span class="timestamp-wrapper"><span class="timestamp">[2020-11-24 Tue]</span></span></p>
</div>
<div id="content" class="content">
<h1 class="title">Eliezer Yudkowsky</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#ID-03920FBE-0E65-46FA-8A94-03298F112941">1. MIRI - Machine Intelligence Research Institute</a></li>
</ul>
</div>
</div>
<p>
Eliezer Shlomo Yudkowsky (born September 11, 1979) is an American <a href="artificial_intelligence.html#ID-2674491f-a645-4499-996e-af04db2be74d">Artificial intelligence</a> (AI) researcher and writer <b>best known for popularizing the idea of friendly artificial intelligence</b>.
</p>

<p>
An autodidact, Yudkowsky did not attend high school or college.
</p>

<p>
Noting the difficulty of formally specifying general-purpose goals by hand, Russell and Norvig cite Yudkowsky's proposal that autonomous and adaptive systems be designed to learn correct behavior over time:
</p>

<blockquote>
<p>
Yudkowsky (2008) goes into more detail about how to design a Friendly AI. He asserts that friendliness (a desire not to harm humans) should be designed in from the start, but that the designers should recognize both that their own designs may be flawed, and that the robot will learn and evolve over time. Thus the challenge is one of mechanism designâ€”to design a mechanism for evolving AI under a system of checks and balances, and to give the systems utility functions that will remain friendly in the face of such changes.
</p>
</blockquote>

<p>
In response to the instrumental convergence concern, where autonomous decision-making systems with poorly designed goals would have default incentives to mistreat humans, <b>Yudkowsky and other MIRI researchers have recommended that work be done to specify software agents that converge on safe default behaviors even when their goals are misspecified</b>.
</p>

<p>
In the intelligence explosion scenario hypothesized by I. J. Good, recursively self-improving AI systems quickly transition from subhuman general intelligence to superintelligent.
</p>

<ul class="org-ul">
<li><a href="ai_singularity.html#ID-A741EDC0-E6F9-43C4-BDF9-9674CA256124">Apparent sharp jump in AI due to anthromorphism of intelligence</a></li>
<li><a href="ai_singularity.html#ID-D9622CE1-7806-437C-91BE-82673E8470C1">Theoritical limits on computational complexity may prevent intelligence explosion</a></li>
</ul>
<div id="outline-container-ID-03920FBE-0E65-46FA-8A94-03298F112941" class="outline-2">
<h2 id="ID-03920FBE-0E65-46FA-8A94-03298F112941"><span class="section-number-2">1.</span> MIRI - Machine Intelligence Research Institute</h2>
<div class="outline-text-2" id="text-1">
<p>
Over 300 blogposts by Yudkowsky on philosophy and science (originally written on <a href="lesswrong.html#ID-73C85982-32E3-46A9-9F40-C317D9A7442B">LessWrong</a> and <span class="underline">Overcoming Bias</span>) were released as an ebook entitled Rationality: From AI to Zombies by the Machine Intelligence Research Institute (MIRI) in 2015. MIRI has also published Inadequate Equilibria, Yudkowsky's 2017 ebook on the subject of societal inefficiencies.
</p>

<hr />
<h3>References</h3>

<ul class="org-ul">
<li><a href="https://en.wikipedia.org/wiki/Eliezer_Yudkowsky">https://en.wikipedia.org/wiki/Eliezer_Yudkowsky</a></li>
</ul>
<hr />
<h3>Backlinks</h3>

<ul class="org-ul">
<li><a href="lesswrong.html#ID-73C85982-32E3-46A9-9F40-C317D9A7442B">LessWrong</a></li>
</ul>
</div>
</div>
</div>
<div id="postamble" class="status">
<hr/>You can send your feedback, queries <a href="mailto:bpanthi977@gmail.com?subject=Feedback: Eliezer Yudkowsky">here</a><span id="visits"></span><span id="claps"></span><div id="claps-message"></div>
</div>
</body>
</html>
