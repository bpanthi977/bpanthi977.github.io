<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>ICVF</title>
<meta name="author" content="Bibek Panthi" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="../css/stylesheet.css" />
<link rel="stylesheet" type="text/css" href="../css/braindump.css" />
<script src="../js/counters.js" type="text/javascript"></script>
<script src="../js/URI.js" type="text/javascript"></script>
<script src="../js/pages.js" type="text/javascript"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="H" href="./index.html"> HOME </a>
</div><div id="preamble" class="status">
<p class="date">Date: <span class="timestamp-wrapper"><span class="timestamp">[2023-08-28 Mon]</span></span></p>
</div>
<div id="content" class="content">
<h1 class="title">ICVF</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#Reinforcement%20Learning">1. Reinforcement Learning</a></li>
<li><a href="#DQN%3A%20Deep%20Q-Network">2. DQN: Deep Q-Network</a></li>
<li><a href="#Value%20Function">3. Value Function</a></li>
<li><a href="#Problem%20Statement">4. Problem Statement</a></li>
<li><a href="#ICVF%20-%20Intention%20Conditioned%20Value%20Function">5. ICVF - Intention Conditioned Value Function</a>
<ul>
<li><a href="#How%20would%20this%20function%20be%20useful%3F">5.1. How would this function be useful?</a></li>
<li><a href="#How%20to%20represent%20%24V%24%2C%20%24s%24%20and%20%24z%24%20%3F">5.2. How to represent \(V\), \(s\) and \(z\) ?</a>
<ul>
<li><a href="#Example%20of%20Feature%20Extraction">5.2.1. Example of Feature Extraction</a></li>
<li><a href="#ID-5CF3DFF3-922B-4EEE-A6DB-33764B1F1DAE">5.2.2. Successor Representation</a></li>
</ul>
</li>
<li><a href="#Piecing%20it%20together">5.3. Piecing it together</a></li>
</ul>
</li>
<li><a href="#Training">6. Training</a></li>
</ul>
</div>
</div>
<div id="outline-container-Reinforcement%20Learning" class="outline-2">
<h2 id="Reinforcement%20Learning"><span class="section-number-2">1.</span> Reinforcement Learning</h2>
<div class="outline-text-2" id="text-1">

<div id="figure-1" class="figure">
<p><img src="data/icvf/agent_environement_action-20230828082819.png" alt="agent_environement_action-20230828082819.png" />
</p>
<p><span class="figure-number">Figure 1: </span>Agent-Environement-Action</p>
</div>



<div id="figure-2" class="figure">
<p><img src="data/icvf/rl-20230828082843.png" alt="rl-20230828082843.png" />
</p>
<p><span class="figure-number">Figure 2: </span>RL</p>
</div>
</div>
</div>
<div id="outline-container-DQN%3A%20Deep%20Q-Network" class="outline-2">
<h2 id="DQN%3A%20Deep%20Q-Network"><span class="section-number-2">2.</span> DQN: Deep Q-Network</h2>
<div class="outline-text-2" id="text-2">
<p>
Q Function \(Q: (s, a) \to r\)
</p>

<p>
At a state \(s\),
</p>

<ul class="org-ul">
<li>\(a_1\) : \(Q(s, a_1)\)</li>
<li>\(a_2\) : \(Q(s, a_2)\)</li>
<li>\(a_3\) : \(Q(s, a_3)\)</li>
</ul>

<p>
Pick the action with highest return
</p>

<p>
\(a = argmax_{a} Q(s, a)\)
</p>

<p>
But we don't know the \(Q\) function, so use a network to represent the function (\(\hat{Q}\)) and learn that function.
</p>

<p>
\(Q(s, a) \gets r + max_a Q(s', a)\)
</p>
</div>
</div>
<div id="outline-container-Value%20Function" class="outline-2">
<h2 id="Value%20Function"><span class="section-number-2">3.</span> Value Function</h2>
<div class="outline-text-2" id="text-3">
<p>
Value Function: \(V: s \to r\)
</p>

<p>
At a state \(s\),
</p>

<ul class="org-ul">
<li>\(a_1 \Rightarrow s_1\) : \(V(s_1)\)</li>
<li>\(a_2 \Rightarrow s_2\) : \(V(s_2)\)</li>
<li>\(a_3 \Rightarrow s_3\) : \(V(s_3)\)</li>
</ul>

<p>
Pick the action with highest return
</p>
</div>
</div>
<div id="outline-container-Problem%20Statement" class="outline-2">
<h2 id="Problem%20Statement"><span class="section-number-2">4.</span> Problem Statement</h2>
<div class="outline-text-2" id="text-4">
<p>
<img src="data/icvf/breakout_youtube_video-20230828134846.png" alt="breakout_youtube_video-20230828134846.png" />
<a href="file:///Users/bpanthi977/Documents/synced/Notes/papers/RL from Passive Data via Latent Intentions - 2304.04782; Intention Conditioned Value Function (ICVF).pdf#page=18">pg. 18</a>
</p>

<ol class="org-ol">
<li>Algorithm is given game play video (It is told neither about actions nor reward)</li>
<li>Pre-Training</li>
<li>Give action and reward information</li>
<li>Fine-Tune</li>
<li>Play the game</li>
</ol>

<p>
The alogrithm has to try to understand the reason/intent behind whatever's happening on the screen.
</p>
</div>
</div>
<div id="outline-container-ICVF%20-%20Intention%20Conditioned%20Value%20Function" class="outline-2">
<h2 id="ICVF%20-%20Intention%20Conditioned%20Value%20Function"><span class="section-number-2">5.</span> ICVF - Intention Conditioned Value Function</h2>
<div class="outline-text-2" id="text-5">
<p>
\(V(s, s_+, z) \in [0,1]\)
</p>

<p>
Let's learn a function that says, what's the probability that we transition from state \(s\) to state \(s_+\) if acting with intent \(z\)
</p>
</div>
<div id="outline-container-How%20would%20this%20function%20be%20useful%3F" class="outline-3">
<h3 id="How%20would%20this%20function%20be%20useful%3F"><span class="section-number-3">5.1.</span> How would this function be useful?</h3>
<div class="outline-text-3" id="text-5-1">
<p>
After pre-training we know the reward function say \(r^\#\) thus we also know the intent with which we have to play \(z^\#\) then with,
</p>

<p>
\(V_r(s) = \sum_{s_+ \in S}  r^\#(s_+) V(s, s_+, z^\#)\)
</p>

<p>
At state \(s\)
</p>
<ul class="org-ul">
<li>\(a_1 \Rightarrow s_1\) : \(V_r(s_1)\)</li>
<li>\(a_2 \Rightarrow s_2\) : \(V_r(s_2)\)</li>
<li>\(a_3 \Rightarrow s_3\) : \(V_r(s_3)\)</li>
</ul>

<p>
Choose the action with max return.
</p>
</div>
</div>
<div id="outline-container-How%20to%20represent%20%24V%24%2C%20%24s%24%20and%20%24z%24%20%3F" class="outline-3">
<h3 id="How%20to%20represent%20%24V%24%2C%20%24s%24%20and%20%24z%24%20%3F"><span class="section-number-3">5.2.</span> How to represent \(V\), \(s\) and \(z\) ?</h3>
<div class="outline-text-3" id="text-5-2">
<dl class="org-dl">
<dt><b>State</b> \(s\)</dt><dd>We could represent the state as a matrix of pixels colors.</dd>
</dl>




<dl class="org-dl">
<dt><b>Intent</b> \(z\)</dt><dd><p>
Intent is a representation of reward \(r\)
</p>

<p>
\(r: s \to \mathrm{R}\)
</p>

<p>
and  \(r\) is a function from state to a real number.
</p></dd>
</dl>


<p>
How do we represent functions?
</p>




<dl class="org-dl">
<dt><b>Value function</b>   \(V\)</dt><dd><p>
is a function of two states and a intent.
</p>

<p>
How do we represent that?
</p></dd>
</dl>



<p>
<b>Solution</b>:
</p>

<ol class="org-ol">
<li>Use (ofcourse) neural networks to represent functions,</li>

<li>Use linear representation for easier analysis and convergence gurantees,</li>

<li>Use feature extractors to extract linear feature representation</li>
</ol>
</div>
<div id="outline-container-Example%20of%20Feature%20Extraction" class="outline-4">
<h4 id="Example%20of%20Feature%20Extraction"><span class="section-number-4">5.2.1.</span> Example of Feature Extraction</h4>
<div class="outline-text-4" id="text-5-2-1">

<div id="figure-3" class="figure">
<p><img src="data/icvf/feature_extraction-20230828192436.png" alt="feature_extraction-20230828192436.png" />
</p>
<p><span class="figure-number">Figure 3: </span>Example of Feature Extraction</p>
</div>
</div>
</div>
<div id="outline-container-ID-5CF3DFF3-922B-4EEE-A6DB-33764B1F1DAE" class="outline-4">
<h4 id="ID-5CF3DFF3-922B-4EEE-A6DB-33764B1F1DAE"><span class="section-number-4">5.2.2.</span> Successor Representation</h4>
<div class="outline-text-4" id="text-5-2-2">
<p>
\(V(s, s_+, z) = e_s^T (I - P_z)^{-1} e_{s_+}\)
</p>

<ul class="org-ul">
<li>\(e_s\)             : Vector representing \(s\)</li>
<li>\(e_{s_+}\)           : Vector representing \(s_+\)</li>
<li>\((I - P_z)^{-1}\) : Matrix representing Intention \(z\), \(P_z\) is transition probabilities under a policy with intent \(z\)</li>
</ul>


<p>
This form derives from <a href="file:///Users/bpanthi977/Documents/synced/Notes/papers/The Successor Representation - Peter Dayan.pdf#page=4">The Successor Representation - Peter Dayan.pdf: Page 4</a> 
</p>


<p>
But since using punctate (1-hot) state encoding is not feasible, we convert state to linear features:
</p>


<p>
\(V(s, s_+, z) = \phi(s) T(z) \psi(z)\)
</p>


<p>
And this feature extraction is done using neural network parameterized by \(\theta\) :
</p>


<p>
\(\hat{V}(s, s_+, z) = \phi_\theta(s) T_\theta (z) \psi_\theta (s_+)\)
</p>


<p>
where, \(\phi(s)\) and \(\psi(s_+)\) are \(d\) -dimensional and \(T(z)\) is \(d \times d\) dimensional
</p>
</div>
</div>
</div>
<div id="outline-container-Piecing%20it%20together" class="outline-3">
<h3 id="Piecing%20it%20together"><span class="section-number-3">5.3.</span> Piecing it together</h3>
<div class="outline-text-3" id="text-5-3">
<p>
\(\hat{V}(s, s_+, z) = \phi_\theta(s) T_\theta (z) \psi_\theta (s_+)\)
</p>

<p>
If we can learn the above ICVF, then for a given reward function \(r^\#\), (<a href="file:///Users/bpanthi977/Documents/synced/Notes/papers/RL from Passive Data via Latent Intentions - 2304.04782; Intention Conditioned Value Function (ICVF).pdf#page=4">pg. 4 eqn. 4</a>)
</p>

<p>
\(V_r(s) = \sum_{s_+ \in S} r^\#(s_+) \hat{V}(s, s_+, z^\#)\)
</p>

<p>
\(V_r(s) = \phi(s) T(z) \sum_{s_+} r^\#(s_+) \psi(s_+)\)
</p>

<p>
\(V_r(s) = \phi(s) T(z) \psi(r)\)
</p>

<p>
Now, we also have an representation for \(r\) i.e. \(\psi(r)\),
Since, Intent is a representation of reward \(r\), we have a representation for \(z\)
</p>

<p>
\(z = \psi(r)\)
</p>

<p>
This completes the representation part.
</p>
</div>
</div>
</div>
<div id="outline-container-Training" class="outline-2">
<h2 id="Training"><span class="section-number-2">6.</span> Training</h2>
<div class="outline-text-2" id="text-6">
<p>
<a href="file:///Users/bpanthi977/Documents/synced/Notes/papers/RL from Passive Data via Latent Intentions - 2304.04782; Intention Conditioned Value Function (ICVF).pdf#page=5">pg. 5: Algorithm</a>
</p>

<p>
Advantage function \(A_z(s, a)\) is the extra reward from taking action \(a\) at state \(s\) instead of acting according to intent \(z\)
</p>

<p>
\(A_z(s,a) = (r + V(s')) - V(s)\)
</p>


<p>
If \(A > 0\) : action is taken with intention \(z\)
</p>

<p>
In that case,
</p>

<p>
\(V(s, s_+, z) \gets 1(s=s_+) + V(s', s_+, z)\)
</p>

<p>
If \(A<0\) : don't update
</p>

<p>
This is the update equation.
</p>

<p>
Details:
</p>
<ol class="org-ol">
<li>Use <a href="file:///Users/bpanthi977/Documents/synced/Notes/papers/RL from Passive Data via Latent Intentions - 2304.04782; Intention Conditioned Value Function (ICVF).pdf#page=4">Expectile (pg. 4)</a> (\(\alpha = 0.9\) )</li>
<li>Use single sample estimate for Advantage (pg. 5)</li>
</ol>

<hr />
<h3>Backlinks</h3>

<ul class="org-ul">
<li><a href="reinforcement_learning.html#ID-B010228E-5555-4D07-8E63-B54E476A249E">Reinforcement Learning</a></li>
<li><a href="representation_learning.html#ID-5472BE86-3F85-4968-85F7-BF29128B8319">Representation Learning</a></li>
</ul>
</div>
</div>
</div>
<div id="postamble" class="status">
<hr/>You can send your feedback, queries <a href="mailto:bpanthi977@gmail.com?subject=Feedback: ICVF">here</a><span id="visits"></span><span id="claps"></span><div id="claps-message"></div>
</div>
</body>
</html>
