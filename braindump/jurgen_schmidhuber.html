<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Jürgen Schmidhuber</title>
<meta name="author" content="Bibek Panthi" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="../css/stylesheet.css" />
<link rel="stylesheet" type="text/css" href="../css/braindump.css" />
<script src="../js/counters.js" type="text/javascript"></script>
<script src="../js/URI.js" type="text/javascript"></script>
<script src="../js/pages.js" type="text/javascript"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="H" href="./index.html"> HOME </a>
</div><div id="preamble" class="status">
<p class="date">Date: <span class="timestamp-wrapper"><span class="timestamp">[2023-04-01 Sat]</span></span> April  1, 2023</p>
</div>
<div id="content" class="content">
<h1 class="title">Jürgen Schmidhuber</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#ID-BAAFCF8C-961E-4E5E-AEE8-ABEB645BEA5F">1. Artificial Curiosity Since 1990-91</a></li>
</ul>
</div>
</div>
<p>
He <a href="https://openreview.net/forum?id=BZ5a1r-kVsf&amp;noteId=GsxarV_Jyeb">claimed</a> he already had the main ideas of  <a href="yann_lecun.html#ID-5C6B54DE-6567-4A80-B3E5-6B6F839CE515">Yann LeCun</a>'s position paper <a href="yann_lecun.html#ID-A5C68C98-9B44-4871-9972-4FFB08DE48CF">A Path Towards Autonomous Machine Intelligence</a>.
</p>
<div id="outline-container-ID-BAAFCF8C-961E-4E5E-AEE8-ABEB645BEA5F" class="outline-2">
<h2 id="ID-BAAFCF8C-961E-4E5E-AEE8-ABEB645BEA5F"><span class="section-number-2">1.</span> Artificial Curiosity Since 1990-91</h2>
<div class="outline-text-2" id="text-1">
<p>
<b>Problem of Coming up with good question:</b>
</p>

<p>
There are two important things in science: 
</p>
<ol class="org-ol">
<li>Finding answers to given questions,</li>
<li>Coming up with good questions</li>
</ol>

<p>
But how to implement the creative part (2) in artificial systems?
</p>
<ul class="org-ul">
<li>through reinforcement learning (RL)</li>
<li>gradient-based artificial neural networks (NNs),</li>
<li>or other machine learning methods ?</li>

<li><p>
<b>Through Generative Adversarial Networks:*</b>
</p>

<p>
The first NN is the controller C. C probabilistically generates outputs that may influence an environment. The second NN is the world model M. It predicts the environmental reactions to C's outputs. Using gradient descent, M minimizes its error, thus becoming a better predictor. But in a zero sum game, the reward-maximizing C tries to find sequences of output actions that maximize the error of M. Thus M's loss is C's gain.
</p>

<p>
The term generative adversarial networks (GANs) is actually a new name for an instance of the principle published in 1990.
</p></li>

<li><p>
<b>Curiosity Through Maximizing Learning Progress:</b>
</p>

<p>
An agent controlled by C might get stuck in front of a TV screen showing highly unpredictable white noise. The refore, in stochastic environments, <span class="underline">C's reward should not be the errors of M, but an approximation of the first derivative of M's errors across subsequent training iterations.</span>
</p>

<p>
C's reward should be M's learning progress or improvements. As a consequence, despite M's high errors in front of  the noisy TV above, C won't get rewarded for getting stuck there,
</p></li>
</ul>
<p>
simply because M's errors won't improve: <span class="underline">both the totally predictable and the fundamentally unpredictable will get boring</span>. 
</p>

<ol class="org-ol">
<li><b>RL for maximizing information Gain or Baysian Surpise:</b>
<ul class="org-ul">
<li><a href="papers/Intrinsically Motivated Reinforcement Learning - NIPS 2004.pdf">papers/Intrinsically Motivated Reinforcement Learning - NIPS 2004.pdf</a></li>
</ul></li>

<li><b>Adversarial agents design surprising computational experiments:</b></li>

<li><p>
<b>Maximizing compression progress like scientists and artists do:</b>
</p>

<p>
I have frequently pointed out that the history of science is a history of data compression progress through incremental discovery of simple laws that govern seemingly complex observation sequences.
</p>

<p>
Read here how my Formal Theory of Fun uses the concept of compression progress to explain not only science but also  art, music, and humor. Take humor for example.
</p>

<p>
Consider the following statement: Biological organisms are driven by the "Four Big F's":  Feeding, Fighting, Fleeing, Mating.Some subjective observers who read this for the first time think it is funny. Why? The punch line after the last comma is unexpected for those who expected another "F." Initially this failed expectation results in sub-optimal
</p></li>
</ol>
<p>
data compression—storage of expected events does not cost anything,
but deviations from predictions require extra bits to encode them. The compressor, however, does not stay the same forever: within a short time interval,
its learning algorithm kicks in and improves it's performance on the data seen so far. The number of saved bits (or a similar measure of learning progress) becomes the observer's intrinsic reward, possibly strong enough to motivate her to read on in search for more 
reward through additional yet unknown patterns.
</p>

<p>
<span class="underline">While previous attempts at explaining humor also focused on the element of surprise, they lacked the essential concept of novel pattern detection measured by compression progress due to learning. This progress is zero whenever the unexpected is just random white noise, and thus no fun at all</span>
</p>

<ul class="org-ul">
<li><p>
<b>Does curiosity distort the basic RL problem?</b>
</p>

<p>
The controller/model systems above (aka CM systems) typically maximize the sum of standard external rewards (for achieving user-given goals) and intrinsic curiosity rewards. Does this distort the basic RL problem? It turns out <span class="underline">not so much</span>. In totally learnable environments, in the long run, the <span class="underline">intrinsic reward even vanishes next to the external  reward</span>.
</p></li>
</ul>

<hr />
<h3>References</h3>

<ul class="org-ul">
<li><a href="https://people.idsia.ch/~juergen/artificial-curiosity-since-1990.html">https://people.idsia.ch/~juergen/artificial-curiosity-since-1990.html</a></li>
</ul>
</div>
</div>
</div>
<div id="postamble" class="status">
<hr/>You can send your feedback, queries <a href="mailto:bpanthi977@gmail.com?subject=Feedback: Jürgen Schmidhuber">here</a><span id="visits"></span><span id="claps"></span><div id="claps-message"></div>
</div>
</body>
</html>
