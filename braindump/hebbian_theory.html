<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Hebbian Theory</title>
<meta name="author" content="Bibek Panthi" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="../css/stylesheet.css" />
<link rel="stylesheet" type="text/css" href="../css/braindump.css" />
<script src="../js/counters.js" type="text/javascript"></script>
<script src="../js/URI.js" type="text/javascript"></script>
<script src="../js/pages.js" type="text/javascript"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="./index.html"> UP </a>
 |
 <a accesskey="H" href="../index.html"> HOME </a>
</div><div id="preamble" class="status">
<p class="date">Date: <span class="timestamp-wrapper"><span class="timestamp">[2020-11-15 Sun]</span></span> November 15, 2020</p>
</div>
<div id="content" class="content">
<h1 class="title">Hebbian Theory</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#Hebbian%20theory">1. Hebbian theory</a>
<ul>
<li><a href="#Hebbian%20engrams%20and%20cell%20assembly%20theory">1.1. Hebbian engrams and cell assembly theory</a></li>
<li><a href="#Principles">1.2. Principles</a></li>
<li><a href="#Relationship%20to%20unsupervised%20learning%2C%20stability%2C%20and%20generalization">1.3. Relationship to unsupervised learning, stability, and generalization</a></li>
<li><a href="#Exceptions">1.4. Exceptions</a></li>
<li><a href="#Hebbian%20learning%20account%20of%20%5B%5Bid%3Aae814499-7862-4684-ba26-861c8e4a4561%5D%5BMirror%20Neurons%5D%5D">1.5. Hebbian learning account of Mirror Neurons</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-Hebbian%20theory" class="outline-2">
<h2 id="Hebbian%20theory"><span class="section-number-2">1.</span> Hebbian theory</h2>
<div class="outline-text-2" id="text-1">
<p>
Hebbian theory is a neuroscientific theory claiming that an increase in synaptic efficacy arises from a presynaptic cell's repeated and persistent stimulation of a postsynaptic cell.
</p>

<p>
The theory is also called Hebb's rule, Hebb's postulate, and cell assembly theory.
</p>

<p>
The theory is often summarized as "Cells that fire together wire together."[2] However, Hebb emphasized that cell A needs to "take part in firing" cell B, and such causality can occur only if cell A fires just before, not at the same time as, cell B. 
</p>

<p>
The theory attempts to explain associative or Hebbian learning, in which simultaneous activation of cells leads to pronounced increases in synaptic strength between those cells.
</p>
</div>
<div id="outline-container-Hebbian%20engrams%20and%20cell%20assembly%20theory" class="outline-3">
<h3 id="Hebbian%20engrams%20and%20cell%20assembly%20theory"><span class="section-number-3">1.1.</span> Hebbian engrams and cell assembly theory</h3>
</div>
<div id="outline-container-Principles" class="outline-3">
<h3 id="Principles"><span class="section-number-3">1.2.</span> Principles</h3>
<div class="outline-text-3" id="text-1-2">
<p>
From the point of view of artificial neurons and artificial neural networks, Hebb's principle can be described as a method of determining how to alter the weights between model neurons.
</p>

<p>
The weight between two neurons increases if the two neurons activate simultaneously, and reduces if they activate separately.
</p>

<p>
Hopfield network,
</p>
</div>
</div>
<div id="outline-container-Relationship%20to%20unsupervised%20learning%2C%20stability%2C%20and%20generalization" class="outline-3">
<h3 id="Relationship%20to%20unsupervised%20learning%2C%20stability%2C%20and%20generalization"><span class="section-number-3">1.3.</span> Relationship to unsupervised learning, stability, and generalization</h3>
<div class="outline-text-3" id="text-1-3">
<p>
This is an intrinsic problem due to this version of Hebb's rule being unstable, as in any network with a dominant signal the synaptic weights will increase or decrease exponentially.
</p>

<p>
Intuitively, this is because whenever the presynaptic neuron excites the postsynaptic neuron, the weight between them is reinforced, causing an even stronger excitation in the future, and so forth, in a self-reinforcing way. One may think a solution is to limit the firing rate of the postsynaptic neuron by adding a non-linear, saturating response function f {\displaystyle f} , but in fact, it can be shown that for any neuron model, Hebb's rule is unstable.[6]
</p>

<p>
Because, again, c âˆ— {\displaystyle \mathbf {c} ^{*}}  is the eigenvector corresponding to the largest eigenvalue of the correlation matrix between the x i {\displaystyle x<sub>i</sub>} s, this corresponds exactly to computing the first principal component of the input.
</p>


<p>
This mechanism can be extended to performing a full PCA (principal component analysis)
</p>

<p>
We have thus connected Hebbian learning to PCA, which is an elementary form of unsupervised learning, in the sense that the network can pick up useful statistical aspects of the input, and "describe" them in a distilled way in its output.[8]
</p>
</div>
</div>
<div id="outline-container-Exceptions" class="outline-3">
<h3 id="Exceptions"><span class="section-number-3">1.4.</span> Exceptions</h3>
<div class="outline-text-3" id="text-1-4">
<p>
One of the most well-documented of these exceptions pertains to how synaptic modification may not simply occur only between activated neurons A and B, but to neighboring neurons as well
</p>

<p>
The compound most commonly identified as fulfilling this retrograde transmitter role is nitric oxide, which, due to its high solubility and diffusibility, often exerts effects on nearby neurons.[11] This type of diffuse synaptic modification, known as volume learning, counters, or at least supplements, the traditional Hebbian model.[12]
</p>
</div>
</div>
<div id="outline-container-Hebbian%20learning%20account%20of%20%5B%5Bid%3Aae814499-7862-4684-ba26-861c8e4a4561%5D%5BMirror%20Neurons%5D%5D" class="outline-3">
<h3 id="Hebbian%20learning%20account%20of%20%5B%5Bid%3Aae814499-7862-4684-ba26-861c8e4a4561%5D%5BMirror%20Neurons%5D%5D"><span class="section-number-3">1.5.</span> Hebbian learning account of <a href="mirror_neurons.html#ID-ae814499-7862-4684-ba26-861c8e4a4561">Mirror Neurons</a></h3>
<div class="outline-text-3" id="text-1-5">
<p>
<a href="grandmother_cell.html#ID-8312593f-72b9-41d8-bf05-7a74e3d2cc1d">Grandmother Cell</a>
</p>

<hr />
<h3>References</h3>

<ul class="org-ul">
<li><a href="https://en.wikipedia.org/wiki/Hebbian_theory">https://en.wikipedia.org/wiki/Hebbian_theory</a></li>
</ul>
<hr />
<h3>Backlinks</h3>

<ul class="org-ul">
<li><a href="Hopfield network.html#ID-3d80a688-31e9-4b55-ae6e-86800a5a9ec3">Hopfield network</a></li>
<li><a href="mirror_neurons.html#ID-ae814499-7862-4684-ba26-861c8e4a4561">Mirror Neurons</a></li>
</ul>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<hr/>You can send your feedback, queries <a href="mailto:bpanthi977@gmail.com?subject=Feedback: Hebbian Theory">here</a><span id="visits"></span><span id="claps"></span><div id="claps-message"></div>
</div>
</body>
</html>
