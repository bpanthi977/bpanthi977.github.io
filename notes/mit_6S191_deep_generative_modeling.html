<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2023-11-13 Mon 20:07 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Deep Generative Modeling</title>
<meta name="author" content="Bibek Panthi" />
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<link rel="stylesheet" type="text/css" href="../css/stylesheet.css" />
<script type="text/javascript" src="/js/counters.js"></script>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="./index.html"> UP </a>
 |
 <a accesskey="H" href="../index.html"> HOME </a>
</div><div id="preamble" class="status">
<p class="date">Date: 2023-03-18</p>
</div>
<div id="content" class="content">
<h1 class="title">Deep Generative Modeling</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#Uses%20of%20Generative%20Models">1. Uses of Generative Models</a>
<ul>
<li><a href="#Debiasing">1.1. Debiasing</a></li>
<li><a href="#Outlier%20Detection">1.2. Outlier Detection</a></li>
</ul>
</li>
<li><a href="#Latent%20Variable">2. Latent Variable</a></li>
<li><a href="#Autoencoders">3. Autoencoders</a></li>
<li><a href="#Variational%20AutoEncoder">4. Variational AutoEncoder</a>
<ul>
<li><a href="#VAE%20Loss">4.1. VAE Loss</a></li>
<li><a href="#Regularization%20Loss">4.2. Regularization Loss</a></li>
<li><a href="#Optimization">4.3. Optimization</a></li>
<li><a href="#Disentanglement%20%5Cbeta-VAEs">4.4. Disentanglement &beta;-VAEs</a></li>
</ul>
</li>
<li><a href="#Generative%20Adversarial%20Networks%20%28GANs%29">5. Generative Adversarial Networks (GANs)</a>
<ul>
<li><a href="#Training%20GANs">5.1. Training GANs</a></li>
<li><a href="#Progressive%20Growing">5.2. Progressive Growing</a></li>
<li><a href="#StyleGAN%282%29%3A%20progressive%20growing%20%2B%20style%20transfer">5.3. StyleGAN(2): progressive growing + style transfer</a></li>
<li><a href="#Conditional%20GANs">5.4. Conditional GANs</a></li>
<li><a href="#CycleGAN%3A%20domain%20transformation">5.5. CycleGAN: domain transformation</a></li>
</ul>
</li>
<li><a href="#Distribution%20Transformer">6. Distribution Transformer</a></li>
</ul>
</div>
</div>
<p>
<a href="https://www.youtube.com/watch?v=QcLlc9lj2hk&amp;list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&amp;index=6">Lecture 4: Deep Generative Modeling</a>
</p>

<p>
Generative Modeling is Unsupervised Learning Problem where we have to find an underlying structure of the data. 
</p>

<p>
Goal: Take as input training samples from some distribution and learn a model that represents that distribution @ 0:03:24
</p>

<p>
There are two class of problems:
</p>
<ul class="org-ul">
<li>Density Estimation: Finding the underlying distribution of data (i.e. the probability density function)</li>
<li>Sample Generation: Sampling an sample from that distribution</li>
</ul>

<p>
Neural Networks are good for this task because @ 0:04:34
</p>
<ul class="org-ul">
<li>the data distribution is high dimensional</li>
<li>and the distribution is complex</li>
</ul>

<p>
Some classes of Generative models are (0:07:30): 
</p>
<ul class="org-ul">
<li>Autoencoders and Variational Autoencoders (VAEs)</li>
<li>Generative Adversarial Networks (GANs)</li>
</ul>
<p>
Both of these are latent variable models.
</p>

<div id="outline-container-Uses%20of%20Generative%20Models" class="outline-2">
<h2 id="Uses%20of%20Generative%20Models"><span class="section-number-2">1.</span> Uses of Generative Models</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-Debiasing" class="outline-3">
<h3 id="Debiasing"><span class="section-number-3">1.1.</span> Debiasing</h3>
<div class="outline-text-3" id="text-1-1">
<p>
0:05:44
</p>

<p>
We can uncover underlying features in a dataset and create more fair and representative dataset.
</p>


<div id="orge7b08e1" class="figure">
<p><img src="../img/mit_6s191/debiasing-20230316142121.png" alt="debiasing-20230316142121.png" />
</p>
<p><span class="figure-number">Figure 1: </span>Debiasing</p>
</div>
</div>
</div>
<div id="outline-container-Outlier%20Detection" class="outline-3">
<h3 id="Outlier%20Detection"><span class="section-number-3">1.2.</span> Outlier Detection</h3>
<div class="outline-text-3" id="text-1-2">
<p>
0:06:42
</p>

<p>
We can detect rare events in data which are nonetheless important for model to handle. E.g. for autonomous driving detect outliers like a deer walking and make model more capable of handling those scenarios is very important.
</p>


<div id="orgdcc3044" class="figure">
<p><img src="../img/mit_6s191/outlier_detection-20230316142354.png" alt="outlier_detection-20230316142354.png" />
</p>
<p><span class="figure-number">Figure 2: </span>Outlier Detection</p>
</div>
</div>
</div>
</div>
<div id="outline-container-Latent%20Variable" class="outline-2">
<h2 id="Latent%20Variable"><span class="section-number-2">2.</span> Latent Variable</h2>
<div class="outline-text-2" id="text-2">
<p>
0:07:50
</p>

<p>
Latent Variables are variables that we cannot observe but they influence our observation. 
As an analogy, take Plato's Allegory of Cave. The shadows are what we observe (the observed variables), but the actual object that cast the shadows are like the latent variables. 
</p>
</div>
</div>

<div id="outline-container-Autoencoders" class="outline-2">
<h2 id="Autoencoders"><span class="section-number-2">3.</span> Autoencoders</h2>
<div class="outline-text-2" id="text-3">
<p>
0:09:14
</p>

<ul class="org-ul">
<li>Autoencoders build some encoding of an input and try to reconstruct the input.</li>
<li>It is an unsupervised approach for learning a lower-dimensional feature representation from unlabeled training data</li>
</ul>


<div id="org1ddcc87" class="figure">
<p><img src="../img/mit_6s191/autoencoders-20230316145850.png" alt="autoencoders-20230316145850.png" />
</p>
<p><span class="figure-number">Figure 3: </span>AutoEncoders</p>
</div>

<p>
Reconstruction is an powerful technique in Unsupervised Learning. 0:12:40
</p>
</div>
</div>
<div id="outline-container-Variational%20AutoEncoder" class="outline-2">
<h2 id="Variational%20AutoEncoder"><span class="section-number-2">4.</span> Variational AutoEncoder</h2>
<div class="outline-text-2" id="text-4">
<p>
0:14:08
</p>

<p>
With autoencoder, the same latent variable \(z\) gives the same output \(\hat{x}\) because the decoding is deterministic. VAEs introduce stochasticity so that new samples can be generated. 0:15:09
</p>

<p>
This is done by breaking down the latent space \(z\) to a mean vector (\(\mu\)) and standard deviation vector (\(\sigma\)). The encoder outputs \(\mu\) and \(\sigma\) from which \(z\) can be sampled. 
</p>
</div>
<div id="outline-container-VAE%20Loss" class="outline-3">
<h3 id="VAE%20Loss"><span class="section-number-3">4.1.</span> VAE Loss</h3>
<div class="outline-text-3" id="text-4-1">
<p>
0:17:30
</p>

<div id="org59bd781" class="figure">
<p><img src="../img/mit_6s191/vae_optimization-20230316153957.png" alt="vae_optimization-20230316153957.png" />
</p>
<p><span class="figure-number">Figure 4: </span>VAE optimization</p>
</div>

<p>
Loss function \(L(\phi, \theta, x)\) is reconstruction loss + regularization term. 
</p>

<ul class="org-ul">
<li>Encoder computes: \(q_{\phi}(z|x)\) i.e. the distribution of latent representation given the input image</li>
<li>Decoder computes: \(p_{\theta}(x|z)\) i.e. the distribution of images given the latent representation</li>
<li>Reconstruction loss: log-likelihood (?), \(||x-\hat{x}||^2\) (0:18:24)</li>
</ul>
</div>
</div>

<div id="outline-container-Regularization%20Loss" class="outline-3">
<h3 id="Regularization%20Loss"><span class="section-number-3">4.2.</span> Regularization Loss</h3>
<div class="outline-text-3" id="text-4-2">
<p>
Regularization loss: \(D(q_{\phi}(z|x)\ ||\ p(z))\) is divergence in the two probability distribution. 
</p>
<ul class="org-ul">
<li>\(q_{\phi}(z|x)\) is inferred latent distribution</li>
<li>\(p(z)\) is a prior distribution on the latent space</li>
<li>A common choice for the prior is a <b>Normal Gaussain</b> distribution
<ul class="org-ul">
<li>Encourages encodings to distribute evenly around the center of the latent space</li>
<li>Penalize the network when it tries to "cheat" by clustering points in specific regions (i.e. by memorizing the data)</li>
</ul></li>
</ul>

<p>
We use Regularization function so that: 0:22:31
</p>
<ul class="org-ul">
<li>Latent space is <b>continuous</b></li>
<li>Latent space is <b>Completeness</b>: Sampling from latent space must give meaningful content</li>
</ul>

<p>
If regularization is not enforced: 0:24:45 
</p>
<ul class="org-ul">
<li>variance can be small and</li>
<li>means may be distributed far apart so that there is no meaningful content in between</li>
</ul>

<p>
However, greater Regularization can adversely effect the reconstruction. So, a balance is needed. 
</p>
</div>
</div>

<div id="outline-container-Optimization" class="outline-3">
<h3 id="Optimization"><span class="section-number-3">4.3.</span> Optimization</h3>
<div class="outline-text-3" id="text-4-3">
<p>
0:26:55
Backpropagation cannot be done through Sampling operation. So, we have to use a clever idea: <b>Reparametrize the sampling layer</b> \(z \sim N(\mu, \sigma^2)\) as \(z = \mu + \sigma \times \epsilon\) where \(\epsilon\) is sampled stochastically. 
</p>


<div id="orgbcf147a" class="figure">
<p><img src="../img/mit_6s191/reparametization_of_sampling_layer-20230316155303.png" alt="reparametization_of_sampling_layer-20230316155303.png" />
</p>
<p><span class="figure-number">Figure 5: </span>Reparametization of Sampling Layer</p>
</div>
</div>
</div>
<div id="outline-container-Disentanglement%20%5Cbeta-VAEs" class="outline-3">
<h3 id="Disentanglement%20%5Cbeta-VAEs"><span class="section-number-3">4.4.</span> Disentanglement &beta;-VAEs</h3>
<div class="outline-text-3" id="text-4-4">
<p>
0:30:44
</p>

<p>
We want latent variables that are uncorrelated with each other. &beta;-VAEs achieve this by enforcing diagonal prior on the latent variables to encourage independence. 
</p>


<div id="org952f47b" class="figure">
<p><img src="../img/mit_6s191/beta_vaes-20230316155613.png" alt="beta_vaes-20230316155613.png" />
</p>
<p><span class="figure-number">Figure 6: </span>&beta;-VAEs</p>
</div>
</div>
</div>
</div>
<div id="outline-container-Generative%20Adversarial%20Networks%20%28GANs%29" class="outline-2">
<h2 id="Generative%20Adversarial%20Networks%20%28GANs%29"><span class="section-number-2">5.</span> Generative Adversarial Networks (GANs)</h2>
<div class="outline-text-2" id="text-5">
<p>
0:35:07
</p>

<p>
Idea: We don't want to explicitly model density, and instead just sample to generate new instances.
</p>


<div id="org112928f" class="figure">
<p><img src="../img/mit_6s191/gans-20230316160043.png" alt="gans-20230316160043.png" />
</p>
<p><span class="figure-number">Figure 7: </span>GANs</p>
</div>

<ul class="org-ul">
<li>Generator generates data from noise</li>
<li>Discriminator tries to separate between real and fake data</li>
</ul>

<p>
GANs avoid the problem of latent space regularization in AutoEncoders, because in some way the random noise \(z\) is itself the latent space, and it being within our control is both continuous and complete. If it wasn't complete the generator won't perform good on some input. 
</p>
</div>

<div id="outline-container-Training%20GANs" class="outline-3">
<h3 id="Training%20GANs"><span class="section-number-3">5.1.</span> Training GANs</h3>
<div class="outline-text-3" id="text-5-1">
<p>
0:39:43
</p>
<ul class="org-ul">
<li>Loss function is Adversial Objective</li>
<li><p>
Discriminator (\(D\)) tries to maximize how well it can discriminate between fake \(G(z)\) data, and real data (\(x\))
</p>

<p>
\(\arg\max_D E_{z,x}[\log D(G(z)) + \log(1-D(x))]\)
</p></li>

<li>Generator (\(G\)) tries to fool the discriminator \(D\): 
\(\arg \min_G E_{z,x} [\log D(G(z))]\)</li>
</ul>

<p>
So, the overall objective is: 
</p>

<p>
\(\arg \min_G \max_D E_{z,x} [ \log D(G(z)) + \log(1 - D(x))]\)
</p>

<p>
0:43:06 GANs are distribution transformers. The generator maps data from gaussain noise to a target distribution. 
0:43:27 We can interpolate in noise distribution to interpolate in target distribution. 
</p>
</div>
</div>
<div id="outline-container-Progressive%20Growing" class="outline-3">
<h3 id="Progressive%20Growing"><span class="section-number-3">5.2.</span> Progressive Growing</h3>
<div class="outline-text-3" id="text-5-2">
<p>
0:44:51
</p>

<p>
<img src="../img/mit_6s191/progressive_growing-20230316161414.png" alt="progressive_growing-20230316161414.png" />
Add more layers as training progresses 
</p>
<ul class="org-ul">
<li>Speeds up training</li>
<li>More stable training</li>
</ul>
</div>
</div>
<div id="outline-container-StyleGAN%282%29%3A%20progressive%20growing%20%2B%20style%20transfer" class="outline-3">
<h3 id="StyleGAN%282%29%3A%20progressive%20growing%20%2B%20style%20transfer"><span class="section-number-3">5.3.</span> StyleGAN(2): progressive growing + style transfer</h3>
<div class="outline-text-3" id="text-5-3">
<p>
0:45:30
</p>
</div>
</div>
<div id="outline-container-Conditional%20GANs" class="outline-3">
<h3 id="Conditional%20GANs"><span class="section-number-3">5.4.</span> Conditional GANs</h3>
<div class="outline-text-3" id="text-5-4">
<p>
0:47:18
</p>


<div id="org529c462" class="figure">
<p><img src="../img/mit_6s191/paired_translation-20230316161742.png" alt="paired_translation-20230316161742.png" />
</p>
<p><span class="figure-number">Figure 8: </span>Paired Translation an example of Conditional GANs</p>
</div>
</div>
</div>
<div id="outline-container-CycleGAN%3A%20domain%20transformation" class="outline-3">
<h3 id="CycleGAN%3A%20domain%20transformation"><span class="section-number-3">5.5.</span> CycleGAN: domain transformation</h3>
<div class="outline-text-3" id="text-5-5">
<p>
0:50:21 
</p>

<p>
CycleGAN emphasize the idea of GANs being <a href="../../mit_6_s191_introduction_to_deep_learning.html#ID-777B6F6F-054F-434D-8357-7E9D2BBBEFFF">distribution transformers</a>. 
</p>

<div id="org7372fae" class="figure">
<p><img src="../img/mit_6s191/cyclegan-20230316161848.png" alt="cyclegan-20230316161848.png" />
</p>
<p><span class="figure-number">Figure 9: </span>CycleGAN: Transformation from one distribution to another distribution.</p>
</div>
</div>
</div>
</div>

<div id="outline-container-Distribution%20Transformer" class="outline-2">
<h2 id="Distribution%20Transformer"><span class="section-number-2">6.</span> Distribution Transformer</h2>
<div class="outline-text-2" id="text-6">

<div id="orgce3c101" class="figure">
<p><img src="../img/mit_6s191/distribution_transformers-20230316161917.png" alt="distribution_transformers-20230316161917.png" />
</p>
<p><span class="figure-number">Figure 10: </span>Distribution Transformers</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<hr/>You can send your feedback, queries <a href="mailto:bpanthi977@gmail.com?subject=Feedback: Deep Generative Modeling">here</a><span id="visits"></span><span id="claps"></span><div id="claps-message"></div>
</div>
</body>
</html>